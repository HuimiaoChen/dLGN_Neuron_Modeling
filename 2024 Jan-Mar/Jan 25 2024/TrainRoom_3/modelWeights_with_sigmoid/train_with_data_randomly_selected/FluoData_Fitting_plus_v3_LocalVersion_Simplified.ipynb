{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MSUnx8YS7-c"
      },
      "source": [
        "WHERE needs input parameters is labeled with \"=== xxxx ===\" in the headline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwji5fbG9dW0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "import copy\n",
        "import sys\n",
        "import random\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smIBw_NiiKd1"
      },
      "source": [
        "## Read and plot fluorescence structure data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UaGOO7A0oxq"
      },
      "outputs": [],
      "source": [
        "# # The code below is used to see whether a trial's post 2s is same as another trial's pre 2s.\n",
        "# # However, no finding that a trial's post 2s is same as another trial's pre 2s, because the\n",
        "# # post 2s periods are processed to make them more close to the baseline.\n",
        "\n",
        "# # Here, use dFF data. Changing mat_data['Master_dFF'] to mat_data['Master_f'] to check F data.\n",
        "\n",
        "# root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "\n",
        "# i = 1\n",
        "# path_ = os.path.join(root_path, cell_name, cell_name + 'red_dFFStructuresrun' + run_num[i] + '.mat')\n",
        "# mat_data = scipy.io.loadmat(path_)\n",
        "# fluo_runx = mat_data['Master_dFF']\n",
        "\n",
        "# conca_runx = np.empty((0,fluo_runx[0,0].shape[2]))\n",
        "# index_record = []\n",
        "# action = True\n",
        "# while action:\n",
        "#     action = False\n",
        "#     for index, value in np.ndenumerate(fluo_dFF_runx):\n",
        "#         value = np.squeeze(value)\n",
        "#         # print(value.shape)\n",
        "#         # print(value[-31:, :].shape)\n",
        "#         if conca_runx.shape[0] == 0:\n",
        "#             conca_runx = np.concatenate((conca_runx, value), axis=0)\n",
        "#             index_record = index_record + [index]\n",
        "#             action = True\n",
        "#             break\n",
        "#         if np.array_equal(conca_runx[:31, :], value[-31:, :]):\n",
        "#             conca_runx = np.concatenate((value[:-31, :], conca_runx), axis=0)\n",
        "#             index_record = [index] + index_record\n",
        "#             action = True\n",
        "#             break\n",
        "#         if np.array_equal(conca_runx[-31:, :], value[:31, :]):\n",
        "#             fluo_dFconca_dFF_runxF_runx = np.concatenate((conca_runx, value[31:, :]), axis=0)\n",
        "#             index_record = index_record + [index]\n",
        "#             action = True\n",
        "#             break\n",
        "# print(conca_runx.shape)\n",
        "# # a shape (93, 10) means no found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmYcXKtyCGrc"
      },
      "source": [
        "### Functions: read and plot structure data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZMtLb2riQGM"
      },
      "outputs": [],
      "source": [
        "def read_conca_fluo_data(cell_name = 'CL090_230515',\n",
        "             run_num = ['4', '5', '6'],\n",
        "             color = 'red',\n",
        "             datatype = 'F',\n",
        "             root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    This fuction is used to concatenate dFF or F data from different pieces of\n",
        "    red/green structure data.\n",
        "    color = 'red' or 'green'\n",
        "    type = 'F' or 'dFF'\n",
        "    if len(run_num) is 3, the output is <class 'numpy.ndarray'> with shape (3, 48),\n",
        "    and its elements are <class 'numpy.ndarray'> with shape (93, 1, 10) for red\n",
        "    and shape (93, 281, 10) for green (281 is the number of components, 10 is\n",
        "    the number of repeats and 93 is 6 seconds; the 2p imaging frequency is\n",
        "    15.63 Hz so roughly 93 frames for 6 seconds).\n",
        "    '''\n",
        "\n",
        "    conca_fluo_data = np.empty((0, 48))\n",
        "\n",
        "    for i in range(len(run_num)):\n",
        "        path_ = os.path.join(root_path, cell_name, cell_name + color + '_dFFStructuresrun' + run_num[i] + '.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "        if datatype == 'dFF':\n",
        "            fluo_data_runx = mat_data['Master_dFF']\n",
        "        elif datatype == 'F':\n",
        "            fluo_data_runx = mat_data['Master_f']\n",
        "        fluo_data_runx = fluo_data_runx[:, :-1] # delete the last column (49th)\n",
        "        # print(type(fluo_data_runx))\n",
        "        # print(fluo_data_runx.shape)\n",
        "        # print(fluo_data_runx[0,0].shape)\n",
        "\n",
        "        conca_fluo_data = np.concatenate((conca_fluo_data, fluo_data_runx), axis=0) # like np.vsatck\n",
        "\n",
        "    return conca_fluo_data\n",
        "\n",
        "def plot_all_trials(conca_fluo_data,\n",
        "           cell_name = 'CL090_230515',\n",
        "           run_num = ['4', '5', '6'],\n",
        "           color = 'red',\n",
        "           datatype = 'F',\n",
        "           component = 1):\n",
        "    '''\n",
        "    This function draws the figures based on the conca_fluo_data generated by\n",
        "    the function read_conca_fluo_data.\n",
        "    color = 'red' or 'green'\n",
        "    type = 'F' or 'dFF'\n",
        "    component is for 'green'; it is from 1 to the component number.\n",
        "    if color is 'red', it can be arbitrary value.\n",
        "    '''\n",
        "    runs_string = ' '.join(run_num)\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "\n",
        "    # Create some sample data\n",
        "    x_intervals = np.linspace(0, 6, 94) # Divide [0, 6] into 94 intervals, cuz data length of a trial is 93\n",
        "    x_middle_points = (x_intervals[:-1] + x_intervals[1:]) / 2  # Calculate middle points\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(nrows=48, ncols=len(run_num), figsize=(5 * len(run_num), 3 * 48), constrained_layout=True)\n",
        "\n",
        "    # Ensure ax is always 2-dimensional, otherwise if len(run_num) is 1, ax will be 1-dimensional (we need 2-dimensional ax).\n",
        "    if len(ax.shape) == 1:\n",
        "        ax = ax[:, np.newaxis]\n",
        "\n",
        "    # Create sf array\n",
        "    sf = np.tile([0.02, 0.08, 0.32], 16)\n",
        "    # Create tf array\n",
        "    tf = np.tile([1, 1, 1, 4, 4, 4], 8)\n",
        "    # Create orientation array\n",
        "    orientation = np.repeat(np.arange(0, 360, 45), 6)\n",
        "\n",
        "    for x in range(48):\n",
        "        for y in range(len(run_num)):\n",
        "            if color == 'green':\n",
        "                data_patch = conca_fluo_data[y, x][:,component-1,:]\n",
        "            if color == 'red':\n",
        "                data_patch = np.squeeze(conca_fluo_data[y, x])\n",
        "            for i in range(data_patch.shape[1]):\n",
        "                ax[x, y].plot(x_middle_points, data_patch[:, i])\n",
        "\n",
        "            # Set axis labels and title\n",
        "            ax[x, y].set_xlabel('Time (seconds)', fontsize=16)\n",
        "            ax[x, y].set_ylabel(datatype, fontsize=16)\n",
        "            # ax.set_title('dFF for a trial', fontsize=18)\n",
        "\n",
        "            # Set tick parameters\n",
        "            ax[x, y].tick_params(labelsize=16)  # Adjust tick size as needed\n",
        "\n",
        "            ax[x, y].axvspan(2, 4, facecolor='gray', alpha=0.2)\n",
        "\n",
        "            if y == 0:\n",
        "                x_data = ax[x, y].get_xticks()\n",
        "                y_data = ax[x, y].get_yticks()\n",
        "                x_data_min = np.min(x_data)\n",
        "                x_data_max = np.max(x_data)\n",
        "                y_data_min = np.min(y_data)\n",
        "                y_data_max = np.max(y_data)\n",
        "                x_text = x_data_min - 0.3 * (x_data_max - x_data_min)\n",
        "                y_text = 0.5 * (y_data_max + y_data_min)\n",
        "\n",
        "        # Write the group information to the right of each row\n",
        "        ax[x, 0].text(x_text, y_text, f'Ori: {orientation[x]}° \\nTF: {tf[x]} \\nSF: {sf[x]}', fontsize=14, va='center')\n",
        "    if color == 'red':\n",
        "        fig.suptitle(f'All Conditions All Rounds All Repests {color_name} {datatype} Data ({cell_name}) (Columns: run {runs_string})', fontsize=16)\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}.pdf'\n",
        "    if color == 'green':\n",
        "        fig.suptitle(f'All Conditions All Rounds All Repests {color_name} {datatype} Data ({cell_name} Component {component}) (Columns: run {runs_string})', fontsize=16)\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_{component:03d}.pdf'\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_trials_separately_in_one_run(conca_fluo_data,\n",
        "           cell_name = 'CL090_230515',\n",
        "           run_num = ['4', '5', '6'],\n",
        "           run_num_ = '4',\n",
        "           color = 'red',\n",
        "           datatype = 'F',\n",
        "           component = 1):\n",
        "    '''\n",
        "    This function draws the figures based on the conca_fluo_data generated by\n",
        "    the function read_conca_fluo_data.\n",
        "    color = 'red' or 'green'\n",
        "    type = 'F' or 'dFF'\n",
        "    component is for 'green'; it is from 1 to the component number.\n",
        "    if color is 'red', it can be arbitrary value.\n",
        "    '''\n",
        "    runs_string = ' '.join(run_num)\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "\n",
        "    # Create some sample data\n",
        "    x_intervals = np.linspace(0, 6, 94) # Divide [0, 6] into 94 intervals, cuz data length of a trial is 93\n",
        "    x_middle_points = (x_intervals[:-1] + x_intervals[1:]) / 2  # Calculate middle points\n",
        "    # x_intervals = np.linspace(0, 14, 218) # 31+93+93=217\n",
        "    # x_middle_points = (x_intervals[:-1] + x_intervals[1:]) / 2 \n",
        "    # !!! Note: choosing (0,6,93) or (0, 14, 218)\n",
        "    # depends on the intertrial duration. It is possible that even if the intertrial is 6s (93 frames;total 93+31+93)\n",
        "    # but the structure data only uses 2 s before  and after the stimultation, thereby being 31 fames for intertrials \n",
        "    # (total 31+31+31, keeping same as the original setting).\n",
        "\n",
        "    # Create sf array\n",
        "    sf = np.tile([0.02, 0.08, 0.32], 16)\n",
        "    # sf = np.tile([0.08], 8)\n",
        "    # Create tf array\n",
        "    tf = np.tile([1, 1, 1, 4, 4, 4], 8)\n",
        "    # tf = np.tile([2], 8)\n",
        "    # Create orientation array\n",
        "    orientation = np.repeat(np.arange(0, 360, 45), 6)\n",
        "    # orientation = np.repeat(np.arange(0, 360, 45), 1)\n",
        "\n",
        "    for x in range(48):\n",
        "    # for x in range(8):\n",
        "        if color == 'green':\n",
        "            data_patch = conca_fluo_data[run_num.index(run_num_), x][:,component-1,:]\n",
        "        if color == 'red':\n",
        "            data_patch = np.squeeze(conca_fluo_data[run_num.index(run_num_), x])\n",
        "        if x == 0: # only at first time, create the figure \n",
        "            # Create a figure and axis\n",
        "            # fig, ax = plt.subplots(nrows=8, ncols=data_patch.shape[1], figsize=(5 * data_patch.shape[1], 3 * 8), constrained_layout=True)\n",
        "            fig, ax = plt.subplots(nrows=48, ncols=data_patch.shape[1], figsize=(5 * data_patch.shape[1], 3 * 48), constrained_layout=True)\n",
        "            if len(ax.shape) == 1: ## ensure ax is 2-dimensional\n",
        "                ax = ax[:, np.newaxis]\n",
        "        for y in range(data_patch.shape[1]):\n",
        "            ax[x, y].plot(x_middle_points, data_patch[:, y])\n",
        "\n",
        "            # Set axis labels and title\n",
        "            ax[x, y].set_xlabel('Time (seconds)', fontsize=16)\n",
        "            ax[x, y].set_ylabel(datatype, fontsize=16)\n",
        "            # ax.set_title('dFF for a trial', fontsize=18)\n",
        "\n",
        "            # Set tick parameters\n",
        "            ax[x, y].tick_params(labelsize=16)  # Adjust tick size as needed\n",
        "\n",
        "            ax[x, y].axvspan(2, 4, facecolor='gray', alpha=0.2)\n",
        "            # ax[x, y].axvspan(6, 8, facecolor='gray', alpha=0.2)\n",
        "\n",
        "            ax[x, y].set_title(f'Repeat {y+1}')\n",
        "\n",
        "            if y == 0:\n",
        "                x_data = ax[x, y].get_xticks()\n",
        "                y_data = ax[x, y].get_yticks()\n",
        "                x_data_min = np.min(x_data)\n",
        "                x_data_max = np.max(x_data)\n",
        "                y_data_min = np.min(y_data)\n",
        "                y_data_max = np.max(y_data)\n",
        "                x_text = x_data_min - 0.3 * (x_data_max - x_data_min)\n",
        "                y_text = 0.5 * (y_data_max + y_data_min)\n",
        "\n",
        "        # Write the group information to the right of each row\n",
        "        ax[x, 0].text(x_text, y_text, f'Ori: {orientation[x]}° \\nTF: {tf[x]} \\nSF: {sf[x]}', fontsize=14, va='center')\n",
        "    if color == 'red':\n",
        "        fig.suptitle(f'All Conditions All Repests for run {run_num_} {color_name} {datatype} Data ({cell_name}) (Columns: run {runs_string})', fontsize=16)\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_run-{run_num_}.pdf'\n",
        "    if color == 'green':\n",
        "        fig.suptitle(f'All Conditions All Repests for run {run_num_} {color_name} {datatype} Data ({cell_name} Component {component}) (Columns: run {runs_string})', fontsize=16)\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}__run-{run_num_}_{component:03d}.pdf'\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktxOqxhZMTnD"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL096_231018'\n",
        "# run_num = ['4']\n",
        "# run_num_ = '4'\n",
        "# color = 'red'\n",
        "# datatype = 'F'\n",
        "# component = 1\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "# plot_all_trials(conca_fluo_data, cell_name, run_num, color, datatype, component)\n",
        "# plot_trials_separately_in_one_run(conca_fluo_data, cell_name, run_num, run_num_, color, datatype, component)\n",
        "\n",
        "# color = 'green'\n",
        "# datatype = 'F'\n",
        "# component = 1\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "# plot_all_trials(conca_fluo_data, cell_name, run_num, color, datatype, component)\n",
        "\n",
        "# print(type(conca_fluo_data))\n",
        "# print(conca_fluo_data.shape)\n",
        "# print(type(conca_fluo_data[1,1]))\n",
        "# print(conca_fluo_data[1,1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "453bKUZO3XGN"
      },
      "source": [
        "Read red and green data then plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF_ye-UJNzX7"
      },
      "outputs": [],
      "source": [
        "# cell_name_list = ['CL075_230228', 'CL079_230324' 'CL090_230515']\n",
        "# run_num_list = [['1', '2', '3'], ['1', '2', '3'], ['4', '5', '6']]\n",
        "\n",
        "# cell_name_list = ['CL090_230515']\n",
        "# run_num_list = [['4', '5', '6']]\n",
        "\n",
        "# for cell_name_, run_num_ in zip(cell_name_list, run_num_list):\n",
        "#     for color_ in ['red', 'green']:\n",
        "#         for datatype_ in ['F', 'dFF']:\n",
        "#             conca_fluo_data_ = read_conca_fluo_data(cell_name_, run_num_, color_, datatype_)\n",
        "#             if color_ == 'red':\n",
        "#                 plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_)\n",
        "#             if color_ == 'green':\n",
        "#                 for component_ in range(1, conca_fluo_data_[0,0].shape[1]+1):\n",
        "#                     plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_, component_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download files if running on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB5rBXAuQwrm"
      },
      "outputs": [],
      "source": [
        "# batch download the plotted figures\n",
        "# uncomment the code below to download figures if needed\n",
        "\n",
        "# import glob\n",
        "\n",
        "# folder_path = '.'\n",
        "# # file_prefix = 'All_Conditions_All_Rounds_All_Repeats_'\n",
        "# file_prefix = 'CL'\n",
        "\n",
        "# # Use glob to find all files with the given prefix in the folder\n",
        "# matching_files = glob.glob(f\"{folder_path}/{file_prefix}*\")\n",
        "# # print(matching_files)\n",
        "# # # Print the matching file names\n",
        "# # for file_path in matching_files:\n",
        "# #     print(file_path)\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# zip_filename = 'files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     # Add files to the zip file\n",
        "#     for file_path in matching_files:\n",
        "#         zipf.write(file_path)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDDpPN0GmUEf"
      },
      "source": [
        "## Extract chronological order of randomized conditions and generate sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxtjWYNA1YML"
      },
      "source": [
        "### Functions: extract chronological order and get condition order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA7kYXzomUY-"
      },
      "outputs": [],
      "source": [
        "def extract_chronological_order(cell_name = 'CL090_230515',\n",
        "                 run_num = ['4', '5', '6'],\n",
        "                 root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    This function uses the frame_2p_metadata_run data to extract the the chronological\n",
        "    order of randomized conditions.\n",
        "    Finally, it returns\n",
        "    ori_3d, tf_3d, sf_3d, all are with shape (3, 10, 48) if the length of rum_num is 3,\n",
        "    3 is the number of runs (the length of run_num); 10 is the number of repeats in each\n",
        "    run; 48 is the number of conditions.\n",
        "    The elements in \"48\" dimension show the ori/tf/sf parameters in chronological order\n",
        "    for a certain run and repeat.\n",
        "    '''\n",
        "\n",
        "    ori_3d = np.empty((len(run_num),10,48))\n",
        "    tf_3d = np.empty((len(run_num),10,48))\n",
        "    sf_3d = np.empty((len(run_num),10,48))\n",
        "\n",
        "    for i in range(len(run_num)):\n",
        "        path_ = os.path.join(root_path, cell_name, 'frame_2p_metadata_run' + run_num[i] + '.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "        stim = mat_data['stim']\n",
        "        # print(type(stim))\n",
        "        # print(stim.shape)\n",
        "        # print(type(stim[0,0]))\n",
        "        # print(stim[0,0].dtype.names)\n",
        "        # print(type(stim['frame']))\n",
        "        # print(stim['frame'].shape)\n",
        "        # print(type(stim['frame'][0,0]))\n",
        "        # print(stim['frame'][0,0].dtype.names)\n",
        "\n",
        "        ori = np.squeeze(stim['frame'][0,0]['orientation'][0,0])\n",
        "        tf = np.squeeze(stim['frame'][0,0]['temporal_frequencies_hz'][0,0])\n",
        "        sf = np.squeeze(stim['frame'][0,0]['sf'][0,0])\n",
        "        # print(type(ori))\n",
        "        # print(type(tf))\n",
        "        # print(type(sf))\n",
        "        # print(ori.shape)\n",
        "        # print(tf.shape)\n",
        "        # print(sf.shape)\n",
        "        # print(len(np.nonzero(ori)[0]))\n",
        "        # print(len(np.nonzero(tf)[0]))\n",
        "        # print(len(np.nonzero(sf)[0]))\n",
        "\n",
        "        indices = np.nonzero(tf)[0] # cannot use nonzero in ori, because it can be 0 degree\n",
        "\n",
        "        unique_indices = [indices[0]]\n",
        "        last_number = indices[0]\n",
        "        for num in indices:\n",
        "            if num - last_number > 1:\n",
        "                unique_indices.append(num)\n",
        "            last_number = num\n",
        "        # the resulting array where continuous numbers are deleted, and only the first one is retained\n",
        "        # i.e., [1,2,3,21,22,23,24,33,34,35,36,37,48,49] -> [1,21,33,48]\n",
        "\n",
        "        # print(len(unique_indices))\n",
        "        ori = ori[unique_indices].reshape(10,48)\n",
        "        tf = tf[unique_indices].reshape(10,48)\n",
        "        sf = sf[unique_indices].reshape(10,48)\n",
        "        # print(ori.shape)\n",
        "        # print(tf.shape)\n",
        "        # print(sf.shape)\n",
        "\n",
        "        ori_3d[i,:,:] = ori\n",
        "        tf_3d[i,:,:] = tf\n",
        "        sf_3d[i,:,:] = sf\n",
        "\n",
        "    # print(ori_3d.shape)\n",
        "    # print(tf_3d.shape)\n",
        "    # print(sf_3d.shape)\n",
        "\n",
        "    return ori_3d, tf_3d, sf_3d\n",
        "\n",
        "\n",
        "def get_condition_order(ori_3d, tf_3d, sf_3d):\n",
        "    '''\n",
        "    This function uses ori_3d, tf_3d, sf_3d, which are generated by the function\n",
        "    extract_chronological_order to calculate condition_order, where the column\n",
        "    number represents the condition and they are sorted in chronological order.\n",
        "\n",
        "    The conditions along the columns in structure data are given in a fixed order,\n",
        "    following the order of vectors: orientation, tf, sf (created as follows). But they\n",
        "    are temporally randomized. That is why we use this function to get the chronological\n",
        "    order of the columns (conditions).\n",
        "\n",
        "    The returned condition_order is with shape (3,10,48). For the dimension \"48\", 48 elements\n",
        "    are column indexes in structure data, like [2, 0, 23, 24, ...], meaning in structure data\n",
        "    column 2 occurs first, then column 0, ...\n",
        "    '''\n",
        "\n",
        "    # Create orientation array\n",
        "    orientation = np.repeat(np.arange(0, 360, 45), 6)\n",
        "    # Create tf array\n",
        "    tf = np.tile([1, 1, 1, 4, 4, 4], 8)\n",
        "    # Create sf array\n",
        "    sf = np.tile([0.02, 0.08, 0.32], 16)\n",
        "\n",
        "    condition_order = np.empty((3,10,48)) # the column number in chronological order\n",
        "\n",
        "    condition_in_column_order = [np.array([x,y,z]) for x,y,z in zip(orientation, tf, sf)]\n",
        "\n",
        "    for i in range(3):\n",
        "        for j in range(10):\n",
        "            conditions_in_time_order = [np.array([x,y,z]) for x,y,z in zip(ori_3d[i,j,:], tf_3d[i,j,:], sf_3d[i,j,:])]\n",
        "            for ii, ele1 in enumerate(conditions_in_time_order):\n",
        "                is_found = False\n",
        "                for jj, ele2 in enumerate(condition_in_column_order):\n",
        "                    if ele1[0] == ele2[0] and ele1[1] == ele2[1] and ele1[2] == ele2[2]:\n",
        "                        condition_order[i,j,ii] = jj\n",
        "                        is_found = True\n",
        "                if not is_found:\n",
        "                    print(\"An Element Not Found!!!\")\n",
        "\n",
        "    return condition_order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygFaNQkpx_mC"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "\n",
        "# ori_3d, tf_3d, sf_3d = extract_chronological_order(cell_name, run_num)\n",
        "# condition_order = get_condition_order(ori_3d, tf_3d, sf_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions: get red/green time sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqji3zVj1eRk"
      },
      "source": [
        "Note: only for 48 conditions, in chronological order but not continuous.\n",
        "\n",
        "The sequences got by the following function get_time_sequence are in chronological order **but not continuous** (**so they may not be helpful or used in future algorithms**), because something between them are omitted, like there are gray screen visual stimuli trials are not in the 48 columns but inserted between the 48 trials in the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi5nyUOEAw1F"
      },
      "outputs": [],
      "source": [
        "def get_time_sequence(conca_fluo_data, condition_order):\n",
        "    '''\n",
        "    This function uses conca_fluo_data and condition_order to get\n",
        "    the time sequence results for red and green data.\n",
        "    If red, return time_sequence with shape (3, 10, 1, 2976), where\n",
        "    3 is how many runs, 10 is how many repeats each run, 1 is how many\n",
        "    components (for red, only soma, it is 1), 2976=62*48 (62 points\n",
        "    are 2s, 48 are the number of conditions).\n",
        "    If green, return time_sequence with shape (3, 10, 281, 2976),\n",
        "    where 281 is how many components, which depends on the cell.\n",
        "    '''\n",
        "\n",
        "    how_many_run = conca_fluo_data.shape[0]\n",
        "    how_many_repeat = conca_fluo_data[0,0].shape[2]\n",
        "    how_many_component = conca_fluo_data[0,0].shape[1] # will be 1 if red\n",
        "\n",
        "    time_sequence = np.empty((how_many_run, how_many_repeat, how_many_component, 62*48))\n",
        "\n",
        "    for i in range(how_many_run):\n",
        "        for j in range(how_many_repeat):\n",
        "            for k in range(how_many_component):\n",
        "                for index, z in enumerate(condition_order[i,j,:]):\n",
        "                    time_sequence[i, j, k, 62*index:62*(index+1)] = conca_fluo_data[i, int(z)][31:, k, j]\n",
        "\n",
        "    return time_sequence\n",
        "\n",
        "\n",
        "def plot_time_sequence_each_repeat(time_sequence,\n",
        "            cell_name = 'CL090_230515',\n",
        "            run_num = ['4', '5', '6'],\n",
        "            color = 'red',\n",
        "            datatype = 'F',\n",
        "            run_index = 1,\n",
        "            repeat = 1,\n",
        "            component = 1):\n",
        "    '''\n",
        "    This function plots the curve of a certain repeat.\n",
        "    Return the data of that repeat.\n",
        "\n",
        "    run_index, repeat, and component all start from 1, not 0.\n",
        "    run_index = 1, ..., len(run_num)\n",
        "    repeat = 1, .., repeat number\n",
        "    component = 1, .., component number\n",
        "    For red, component can only be 1.\n",
        "    '''\n",
        "\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "    run = int(run_num[run_index-1])\n",
        "    data = time_sequence[run_index-1, repeat-1, component-1, :]\n",
        "    # we should plot all the data of 1 run, that is 10 repeats, cuz they are a sequence.\n",
        "\n",
        "    time_steps = 62 * 48 # time_sequence.shape[3]\n",
        "    time_interval = 192  # seconds, 192 s = 4 s/trial * 48 trails, trial is condition\n",
        "\n",
        "    time_values = [t * time_interval / time_steps for t in range(time_steps)]\n",
        "\n",
        "    plt.figure(figsize=(18, 6))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.plot(time_values, data, color=color, linewidth=1)\n",
        "\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xlabel('Time (s)', fontsize=16)\n",
        "    plt.ylabel(f'{datatype}', fontsize=16)\n",
        "    if color == 'red':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} Repeat {repeat} in Chronological Order', fontsize=18)\n",
        "    if color == 'green':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} Repeat {repeat} Component {component} in Chronological Order', fontsize=18)\n",
        "\n",
        "    # Add shaded rectangles for stimuli\n",
        "    for i in range(0, time_interval, 4):\n",
        "        plt.axvspan(i, i + 2, facecolor='gray', alpha=0.2)\n",
        "\n",
        "    if color == 'red':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_Run{run}_Repeat{repeat:02d}_TimeSequence.pdf'\n",
        "    if color == 'green':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_{component:03d}_Run{run}_Repeat{repeat:02d}_TimeSequence.pdf'\n",
        "\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_time_sequence_each_run(time_sequence,\n",
        "            cell_name = 'CL090_230515',\n",
        "            run_num = ['4', '5', '6'],\n",
        "            color = 'red',\n",
        "            datatype = 'F',\n",
        "            run_index = 1,\n",
        "            component = 1):\n",
        "    '''\n",
        "    This function plots the curve of a certain run.\n",
        "    Why plot all the data of 1 run, that is 10 repeats?\n",
        "    Because they are a sequence, i.e., they are done in \n",
        "    chronological order but not continuously.\n",
        "    Return the data of that run.\n",
        "\n",
        "    run_index and component both start from 1, not 0.\n",
        "    run_index = 1, ..., len(run_num)\n",
        "    component = 1, .., component number\n",
        "    For red, component can only be 1.\n",
        "    '''\n",
        "\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "    run = int(run_num[run_index-1])\n",
        "    data = time_sequence[run_index-1, :, component-1, :]\n",
        "    data = data.reshape(time_sequence.shape[1] * time_sequence.shape[3])\n",
        "\n",
        "    time_steps = 62 * 48 * 10 # 62 * 48 is time_sequence.shape[3], 10 is time_sequence.shape[1]\n",
        "    time_interval = 192 * 10  # seconds, 192 s = 4 s/trial * 48 trails, trial is condition\n",
        "\n",
        "    time_values = [t * time_interval / time_steps for t in range(time_steps)]\n",
        "\n",
        "    plt.figure(figsize=(100, 6))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.plot(time_values, data, color=color, linewidth=1)\n",
        "\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xlabel('Time (s)', fontsize=16)\n",
        "    plt.ylabel(f'{datatype}', fontsize=16)\n",
        "    if color == 'red':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} in Chronological Order', fontsize=18)\n",
        "    if color == 'green':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} Component {component} in Chronological Order', fontsize=18)\n",
        "\n",
        "    # Add shaded rectangles for stimuli\n",
        "    for i in range(0, time_interval, 4):\n",
        "        plt.axvspan(i, i + 2, facecolor='gray', alpha=0.2)\n",
        "\n",
        "    filename = f'{cell_name}_{color_name}_{datatype}_Run{run}_TimeSequence.pdf'\n",
        "    if color == 'red':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_Run{run}_TimeSequence.pdf'\n",
        "    if color == 'green':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_{component:03d}_Run{run}_TimeSequence.pdf'\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyeL1x5LBA-P"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "# color = 'green'\n",
        "# datatype = 'F'\n",
        "\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "\n",
        "# ori_3d, tf_3d, sf_3d = extract_chronological_order(cell_name, run_num)\n",
        "# condition_order = get_condition_order(ori_3d, tf_3d, sf_3d)\n",
        "\n",
        "# time_sequence = get_time_sequence(conca_fluo_data, condition_order)\n",
        "\n",
        "# # run_index = 1\n",
        "# # repeat = 1\n",
        "# # component = 2\n",
        "\n",
        "# # plot_time_sequence_each_repeat(time_sequence,\n",
        "# #             cell_name,\n",
        "# #             run_num,\n",
        "# #             color,\n",
        "# #             datatype,\n",
        "# #             run_index = run_index,\n",
        "# #             repeat = repeat,\n",
        "# #             component = component);\n",
        "\n",
        "# # plot_time_sequence_each_run(time_sequence,\n",
        "# #             cell_name,\n",
        "# #             run_num,\n",
        "# #             color,\n",
        "# #             datatype,\n",
        "# #             run_index = run_index,\n",
        "# #             component = component);\n",
        "\n",
        "# print(conca_fluo_data.shape)\n",
        "# print(conca_fluo_data[0,0].shape)\n",
        "# print(condition_order.shape)\n",
        "# print(time_sequence.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVzhX1d2-nYX"
      },
      "source": [
        "## Get valid components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions: get valid components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl3FurKE-qkd"
      },
      "outputs": [],
      "source": [
        "def get_valid_components(cell_name, max_pixels = 100, soma_index = 0,\n",
        "                         root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    this function gets the correct components from the all component (too big ones are invalid, criteria is max_pexels).\n",
        "    corresponding size list and distance list (distance to soma) are obtained.\n",
        "    soma is get from red bouton marks using soma_index (there should be only 1 but there are actually some invalid ones),\n",
        "    so we need soma_index.\n",
        "    '''\n",
        "\n",
        "    # Load red BoutonMasks\n",
        "    path_ = os.path.join(root_path, cell_name, cell_name + 'red_BoutonMasks.mat')\n",
        "    try:\n",
        "        data_ = h5py.File(path_, 'r') # Open the MATLAB v7.3 file\n",
        "        red_bouton_masks = copy.deepcopy(np.array(data_['BoutonMasks']))\n",
        "        if len(red_bouton_masks.shape) == 3:\n",
        "            red_bouton_masks = np.transpose(red_bouton_masks, (2, 1, 0))\n",
        "        elif len(red_bouton_masks.shape) == 2:\n",
        "            red_bouton_masks = np.transpose(red_bouton_masks)\n",
        "        else:\n",
        "            print(\"Unexpected red_bouton_masks shape!!!\")\n",
        "        data_.close()\n",
        "    except Exception as e:\n",
        "        data_ = scipy.io.loadmat(path_)\n",
        "        red_bouton_masks = np.array(data_['BoutonMasks'])\n",
        "\n",
        "    if len(red_bouton_masks.shape) == 3:\n",
        "        A = red_bouton_masks[:, :, soma_index]\n",
        "    elif len(red_bouton_masks.shape) == 2:\n",
        "        A = red_bouton_masks\n",
        "    else:\n",
        "        print(\"Unexpected red_bouton_masks shape!!!\")\n",
        "\n",
        "    row_indices, col_indices = np.where(A == 1)\n",
        "    soma_average_row = np.mean(row_indices)\n",
        "    soma_average_col = np.mean(col_indices)\n",
        "\n",
        "    # Load green BoutonMasks\n",
        "    path_ = os.path.join(root_path, cell_name, cell_name + 'green_BoutonMasks.mat')\n",
        "    try:\n",
        "        data_ = h5py.File(path_, 'r') # Open the MATLAB v7.3 file\n",
        "        green_bouton_masks = copy.deepcopy(np.array(data_['BoutonMasks']))\n",
        "        green_bouton_masks = np.transpose(green_bouton_masks, (2, 1, 0))\n",
        "        data_.close()\n",
        "    except Exception as e:\n",
        "        data_ = scipy.io.loadmat(path_)\n",
        "        green_bouton_masks = np.array(data_['BoutonMasks'])\n",
        "\n",
        "    n_com = green_bouton_masks.shape[2]\n",
        "    # print(green_bouton_masks.shape)\n",
        "    valid_com_index_list = []\n",
        "    valid_size_list = []\n",
        "    valid_dis_list = []\n",
        "\n",
        "    for i in range(n_com):\n",
        "        A = green_bouton_masks[:, :, i]\n",
        "        row_indices, col_indices = np.where(A == 1)\n",
        "        average_row = np.mean(row_indices)\n",
        "        average_col = np.mean(col_indices)\n",
        "        if len(row_indices) < max_pixels:\n",
        "            valid_com_index_list.append(i)\n",
        "\n",
        "            distance = np.sqrt((average_row - soma_average_row)**2 + (average_col - soma_average_col)**2)\n",
        "            valid_dis_list.append(distance)\n",
        "\n",
        "            number_of_ones = np.sum(A == 1)\n",
        "            valid_size_list.append(number_of_ones)\n",
        "\n",
        "    return valid_com_index_list, valid_dis_list, valid_size_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2wscaO_LLh7"
      },
      "outputs": [],
      "source": [
        "# valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components('CL075_230228', 100, 5)\n",
        "# print(valid_com_index_list)\n",
        "# print(valid_dis_list)\n",
        "# print(valid_size_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract singal traces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3CTt0_nifWC"
      },
      "source": [
        "### Functions: read signal traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq-YZOA4glBE"
      },
      "outputs": [],
      "source": [
        "def read_signal_traces(cell_name = 'CL090_230515',\n",
        "           run_num = ['4', '5', '6'],\n",
        "           color = 'red',\n",
        "           root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    This fuction is used to signal traces data of which the length is the 2p lengh\n",
        "    (number of 2p imgaing frames, which divided by 2p imaging frequency, 15.63 Hz, then\n",
        "    the duration of 2p imaging).\n",
        "    color = 'red' or 'green'\n",
        "    Acquired data is the raw fluorescence data, i.e., F value.\n",
        "    If len(run_num) is 3, for color 'green', the output is <class 'numpy.ndarray'> with\n",
        "    shape (3, N, M); for color 'red', the output is <class 'numpy.ndarray'> with shape\n",
        "    (3, 1, M). N is the number of components and M is 2p length (number of 2p imaging\n",
        "    frames).\n",
        "    '''\n",
        "\n",
        "    path_ = os.path.join(root_path, cell_name, cell_name + color + '_SignalTracesrun' + run_num[0] + '.mat')\n",
        "    mat_data = scipy.io.loadmat(path_)\n",
        "    singal_traces = np.empty((len(run_num), mat_data['SignalTraces']['BoutonTraces'][0,0].shape[1],\n",
        "                  mat_data['SignalTraces']['BoutonTraces'][0,0].shape[0]))\n",
        "    # print(singal_traces.shape)\n",
        "\n",
        "    for i in range(len(run_num)):\n",
        "        path_ = os.path.join(root_path, cell_name, cell_name + color + '_SignalTracesrun' + run_num[i] + '.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "\n",
        "        # print(mat_data['SignalTraces'].shape)\n",
        "        # print(mat_data['SignalTraces'][0,0].shape)\n",
        "        # print(mat_data['SignalTraces'][0,0]['BoutonTraces'].shape)\n",
        "        # print(mat_data['SignalTraces'][0,0]['BoutonTraces'][0,0].shape)\n",
        "        # print(mat_data['SignalTraces']['BoutonTraces'].shape)\n",
        "        # print(mat_data['SignalTraces']['BoutonTraces'][0,0].shape)\n",
        "        # # mat_data['SignalTraces'][0,0]['BoutonTraces'][0,0] and mat_data['SignalTraces']['BoutonTraces'][0,0] are same.\n",
        "\n",
        "        singal_traces[i, :, :] = mat_data['SignalTraces']['BoutonTraces'][0,0].T\n",
        "\n",
        "    return singal_traces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT2DDHSdf64k"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "# color = 'green'\n",
        "\n",
        "# signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "\n",
        "# print(signal_traces.shape)\n",
        "# # print(signal_traces[0,0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntcg9SgZAMAb"
      },
      "source": [
        "### Functions: check whether structure data is in signal traces and get locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcrBsSBDEPy1"
      },
      "outputs": [],
      "source": [
        "def get_condition_piece_locations(conca_fluo_data, signal_traces):\n",
        "    '''\n",
        "    This function gets the locations of conca_fluo_data (from structure data) in\n",
        "    signal_traces.\n",
        "    conca_fluo_data is the data of all conditions; the data is extracted from\n",
        "    signal_traces.\n",
        "    return locations with shape (a,b,c,d) (e.g., (3, 281, 10, 48)) and\n",
        "    its element's shape is (2,3).\n",
        "    a is how_many_run\n",
        "    b is how_many_component # will be 1 if red\n",
        "    c is how_many_repeat\n",
        "    d is how_many_condition\n",
        "    (2,3) are the data piece's start and end index in signal_traces.\n",
        "    here we assume the data piece is 93 points, corresponding to 6s.\n",
        "    '''\n",
        "\n",
        "    how_many_run = conca_fluo_data.shape[0]\n",
        "    how_many_repeat = conca_fluo_data[0,0].shape[2]\n",
        "    how_many_component = conca_fluo_data[0,0].shape[1] # will be 1 if red\n",
        "    how_many_condition = conca_fluo_data.shape[1]\n",
        "\n",
        "    locations = np.empty((how_many_run, how_many_component, how_many_repeat, how_many_condition), dtype=object)\n",
        "\n",
        "    # get the position of elements from conca_fluo_data in signal_traces\n",
        "    for run_index in range(how_many_run):\n",
        "        for component_index in range(how_many_component):\n",
        "            for repeat_index in range(how_many_repeat):\n",
        "                for condition_index in range(how_many_condition):\n",
        "                    for i, ele in enumerate(conca_fluo_data[run_index, condition_index][:, component_index, repeat_index]):\n",
        "                        if i == 0:\n",
        "                            # print(np.isin(ele, signal_traces))\n",
        "                            index_start = np.where(signal_traces[run_index, component_index, :] == ele)[0]\n",
        "                            # print(np.where(signal_traces[run_index, component_index, :] == ele))\n",
        "                            # if np.where(signal_traces[run_index, component_index, :] == ele)[0].shape[0] > 1:\n",
        "                            #     print(ele)\n",
        "                        if i == 46: # if only i == 0 and i == 92, there are still multiple outputs due to conincidence\n",
        "                            index_middle = np.where(signal_traces[run_index, component_index, :] == ele)[0]\n",
        "                        if i == 92:\n",
        "                            # print(np.isin(ele, signal_traces))\n",
        "                            index_end = np.where(signal_traces[run_index, component_index, :] == ele)[0]\n",
        "                    isfound = 0\n",
        "                    for index1_ in index_start:\n",
        "                        for index2_ in index_middle:\n",
        "                            for index3_ in index_end:\n",
        "                                if index3_ - index1_ == 92 and index2_ - index1_ == 46:\n",
        "                                    isfound = isfound + 1\n",
        "                                    locations[run_index, component_index, repeat_index, condition_index] = np.array([[run_index, component_index, index1_],\n",
        "                                                                              [run_index, component_index, index3_]])\n",
        "                                    # message = (\n",
        "                                    #       f\"condition {condition_index} -- start position: \"\n",
        "                                    #       f\"({run_index}, {component_index}, {index1_}); \"\n",
        "                                    #       f\"end position: ({run_index}, {component_index}, {index2_})\"\n",
        "                                    #       )\n",
        "                                    # print(message)\n",
        "                    if isfound != 1:\n",
        "                        print(f\"isfound is {isfound}\")\n",
        "                        print(f\"run {run_index} component {component_index} repeat {repeat_index} condition {condition_index} Not Found or Multiple Found\")\n",
        "            # print(f\"--- --- run {run_index} component {component_index} finished --- ---\")\n",
        "\n",
        "    return locations\n",
        "\n",
        "## below is the original code for finding location, which has been wrapped in the above function\n",
        "# run_index = 2\n",
        "# repeat = 9\n",
        "# component = 279\n",
        "# condition_index = 0\n",
        "\n",
        "# empty_array = np.empty((3, 4))\n",
        "\n",
        "# # get the position of elements from conca_fluo_data in signal_traces\n",
        "# for condition_index in range(48):\n",
        "#     for i, ele in enumerate(conca_fluo_data[run_index, condition_index][:, component, repeat]):\n",
        "#         if i == 0:\n",
        "#             # print(np.isin(ele, signal_traces))\n",
        "#             index1 = np.where(signal_traces[run_index, component, :] == ele)[0]\n",
        "#             # print(np.where(signal_traces[run_index, component, :] == ele))\n",
        "#             # if np.where(signal_traces[run_index, component, :] == ele)[0].shape[0] > 1:\n",
        "#             #     print(ele)\n",
        "#         if i == 92:\n",
        "#             # print(np.isin(ele, signal_traces))\n",
        "#             index2 = np.where(signal_traces[run_index, component, :] == ele)[0]\n",
        "#     isfound = False\n",
        "#     for index1_ in index1:\n",
        "#         for index2_ in index2:\n",
        "#             if index2_ - index1_ == 92:\n",
        "#                 isfound = True\n",
        "#                 print(f\"condition {condition_index} -- start position: ({run_index},{component},{index1_}); end position: ({run_index},{component},{index2_})\")\n",
        "#     if not isfound:\n",
        "#         print(\"Not Found\")\n",
        "#     print(\"======\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9EUOh58w706"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "# color = 'red'\n",
        "# datatype = 'F'\n",
        "\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "# ori_3d, tf_3d, sf_3d = extract_chronological_order(cell_name, run_num)\n",
        "# condition_order = get_condition_order(ori_3d, tf_3d, sf_3d)\n",
        "# time_sequence = get_time_sequence(conca_fluo_data, condition_order)\n",
        "\n",
        "# signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "\n",
        "# print(conca_fluo_data.shape)\n",
        "# print(conca_fluo_data[0,0].shape)\n",
        "# print(condition_order.shape)\n",
        "# print(time_sequence.shape)\n",
        "# print(signal_traces.shape)\n",
        "# print(\"-- --- --\")\n",
        "\n",
        "# locations = get_condition_piece_locations(conca_fluo_data, signal_traces)\n",
        "# print(locations.shape)\n",
        "# print(locations[0,0,0,2])\n",
        "# print(locations[0,0,0,2].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2IMWcWpbuoc"
      },
      "source": [
        "### Functions: recover conca_fluo_data (structure data) from signal traces and locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLIojlXPb0yD"
      },
      "outputs": [],
      "source": [
        "def recover_strucure_from_traces_and_locs(signal_traces, locations):\n",
        "    '''\n",
        "    This function recovers conca_fluo_data (structure data) from signal traces and locations.\n",
        "\n",
        "    This function is an inverse of function get_condition_piece_locations.\n",
        "    '''\n",
        "    how_many_run, how_many_component, how_many_repeat, how_many_condition = locations.shape\n",
        "\n",
        "    start_point = locations[0, 0, 0, 0][0, 2]\n",
        "    end_point = locations[0, 0, 0, 0][1, 2]\n",
        "    fluo_length_per_condition = end_point - start_point + 1\n",
        "    conca_fluo_data = np.empty((how_many_run, how_many_condition), dtype=object)\n",
        "    for i in range(how_many_run):\n",
        "        for j in range(how_many_condition):\n",
        "            conca_fluo_data[i, j] = np.zeros((fluo_length_per_condition, how_many_component, how_many_repeat))\n",
        "\n",
        "    for run_index in range(how_many_run):\n",
        "        for component_index in range(how_many_component):\n",
        "            for repeat_index in range(how_many_repeat):\n",
        "                for condition_index in range(how_many_condition):\n",
        "                    start_point = locations[run_index, component_index, repeat_index, condition_index][0, 2]\n",
        "                    end_point = locations[run_index, component_index, repeat_index, condition_index][1, 2]\n",
        "                    conca_fluo_data[run_index, condition_index][:, component_index, repeat_index] = signal_traces[run_index, component_index, start_point:end_point+1]\n",
        "\n",
        "    return conca_fluo_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### === Input cell_name here ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0bpofEJZrB",
        "outputId": "e6a42e24-0cb7-4607-8b81-2f18b0474e90"
      },
      "outputs": [],
      "source": [
        "cell_name = 'CL096_231018'\n",
        "run_num = ['3', '4']\n",
        "\n",
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "\n",
        "# cell_name = 'CL075_230228'\n",
        "# run_num = ['1', '2', '3']\n",
        "\n",
        "# cell_name = 'CL079_230324'\n",
        "# run_num = ['1', '2', '3']\n",
        "\n",
        "datatype = 'F'\n",
        "\n",
        "color = 'green'\n",
        "\n",
        "green_conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "green_signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "green_locations = get_condition_piece_locations(green_conca_fluo_data, green_signal_traces)\n",
        "print(green_conca_fluo_data.shape)\n",
        "print(green_conca_fluo_data[0,0].shape)\n",
        "print(green_signal_traces.shape)\n",
        "print(green_locations.shape)\n",
        "print(green_locations[0,0,0,0].shape)\n",
        "print(\"------ ------\")\n",
        "\n",
        "color = 'red'\n",
        "\n",
        "red_conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "red_signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "red_locations = get_condition_piece_locations(red_conca_fluo_data, red_signal_traces)\n",
        "print(red_conca_fluo_data.shape)\n",
        "print(red_conca_fluo_data[0,0].shape)\n",
        "print(red_signal_traces.shape)\n",
        "print(red_locations.shape)\n",
        "print(red_locations[0,0,0,0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check recover_strucure_from_traces_and_locs to see whether we can recover conca_fluo_data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neraIwlxmb8o"
      },
      "outputs": [],
      "source": [
        "# # check recover_strucure_from_traces_and_locs to see whether we can recover conca_fluo_data\n",
        "# green_conca_fluo_data_ = recover_strucure_from_traces_and_locs(green_signal_traces, green_locations)\n",
        "# red_conca_fluo_data_ = recover_strucure_from_traces_and_locs(red_signal_traces, red_locations)\n",
        "# for conca_fluo_data, conca_fluo_data_ in [(green_conca_fluo_data, green_conca_fluo_data_), (red_conca_fluo_data, red_conca_fluo_data_)]:\n",
        "#     are_equal_red = True\n",
        "#     for i in range(conca_fluo_data.shape[0]):\n",
        "#         for j in range(conca_fluo_data.shape[1]):\n",
        "#             if not np.array_equal(conca_fluo_data[i, j], conca_fluo_data_[i, j]):\n",
        "#                 are_equal_red = False\n",
        "#                 break\n",
        "#     print(are_equal_red)\n",
        "# # the printed outputs are both \"True\" -- already verified!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions: calculate the F value std of all components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_std_of_dimensions(array, dimensions):\n",
        "  \"\"\"Calculates the standard deviation of the specified dimensions of an array.\n",
        "  Args:\n",
        "    array: A numpy array.\n",
        "    dimensions: A tuple of integers representing the dimensions of the array to calculate the \n",
        "    standard deviation of.\n",
        "  Returns:\n",
        "    A numpy array containing the standard deviation of the specified dimensions of the array.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a new numpy array to store the standard deviations.\n",
        "  std_array = np.zeros(len(dimensions))\n",
        "\n",
        "  # Iterate over the range of dimensions and calculate the standard deviation of each dimension.\n",
        "  for i in range(len(dimensions)):\n",
        "    std_array[i] = np.std(array[:, dimensions[i], :])\n",
        "\n",
        "  # Return the numpy array containing the standard deviations.\n",
        "  return std_array\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eg of Function Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the standard deviation of (:,i,:) i = 0 to 127.\n",
        "F_std_component = calculate_std_of_dimensions(green_signal_traces, range(green_signal_traces.shape[1]))\n",
        "print(len(F_std_component))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqg6FsASVG0O"
      },
      "source": [
        "### Calculate and plot mean each run, std each run, mean each repeat, std each repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gcvkU84F_Um4",
        "outputId": "4265e81a-a93b-4c72-dfef-482312129e35"
      },
      "outputs": [],
      "source": [
        "### mean each run\n",
        "\n",
        "print(\"mean of each run of green:\")\n",
        "green_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(green_signal_traces[run_,:,:])\n",
        "    green_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "print(\"mean of each run of red:\")\n",
        "red_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(red_signal_traces[run_,:,:])\n",
        "    red_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_mean_run\n",
        "y2 = red_mean_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8)) # [6.4, 4.8] is deault size, same as fig, ax1 = plt.subplots()\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Run', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Runs (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each run\n",
        "\n",
        "print(\"std of each run of green:\")\n",
        "green_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(green_signal_traces[run_,:,:])\n",
        "    green_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "print(\"std of each run of red:\")\n",
        "red_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(red_signal_traces[run_,:,:])\n",
        "    red_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_std_run\n",
        "y2 = red_std_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Run', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Runs (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### mean each repeat\n",
        "\n",
        "# print(\"mean of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "green_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# print(\"mean of each repeat of red:\")\n",
        "red_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_mean_repeat\n",
        "y2 = red_mean_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Repeat', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Repeats (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each repeat\n",
        "\n",
        "# print(\"std of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "green_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(green_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# print(\"std of each repeat of red:\")\n",
        "red_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(red_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_std_repeat\n",
        "y2 = red_std_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Repeat', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "    sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "    sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Repeats (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## for later use in decay restoration\n",
        "green_mean_repeat_list_raw = copy.deepcopy(green_mean_repeat)\n",
        "red_mean_repeat_list_raw = copy.deepcopy(red_mean_repeat)\n",
        "\n",
        "green_mean_run_list_raw = copy.deepcopy(green_mean_run)\n",
        "red_mean_run_list_raw = copy.deepcopy(red_mean_run)\n",
        "\n",
        "green_std_repeat_list_raw = copy.deepcopy(green_std_repeat)\n",
        "red_std_repeat_list_raw = copy.deepcopy(red_std_repeat)\n",
        "\n",
        "green_std_run_list_raw = copy.deepcopy(green_std_run)\n",
        "red_std_run_list_raw = copy.deepcopy(red_std_run)\n",
        "\n",
        "# std is not used cuz after mean restoration we need to recalculate std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U7dVNVLU23G"
      },
      "source": [
        "### Plot a repeat of signal traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "vGCB9x7T-Eu_",
        "outputId": "c25d8228-e802-4443-9c89-3c2551a71075"
      },
      "outputs": [],
      "source": [
        "print(green_signal_traces.shape)\n",
        "print(red_signal_traces.shape)\n",
        "\n",
        "## plot a signal trace in a repeat\n",
        "# data_to_plot = green_signal_traces[0, 0, :int(32500/10)]\n",
        "data_to_plot = red_signal_traces[0, 0, 7*int(32500/10):8*int(32500/10)]\n",
        "\n",
        "plt.figure(figsize=(6.4*2, 4.8))\n",
        "\n",
        "plt.plot(data_to_plot, color='salmon', linewidth=0.8, label='Red Fluorescence')\n",
        "plt.xlabel('Frame', fontsize=16)\n",
        "plt.ylabel('F Value', fontsize=16)\n",
        "plt.tick_params(axis='y', labelsize=14)\n",
        "plt.tick_params(axis='x', labelsize=12)\n",
        "\n",
        "# Set title and legend\n",
        "plt.title('Red F Trace in One Repeat', fontsize=18)\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85T1yqQwDMRj"
      },
      "source": [
        "## Signal trace restoration (mean and std restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijw_3rI6KDhC"
      },
      "source": [
        "### Restore mean and plot mean each run, std each run, mean each repeat, std each repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQaLQEhGXXk8"
      },
      "source": [
        "#### Restore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVTi0E-xXVbV",
        "outputId": "6d2ff24e-411d-409e-c99d-1c5e58a5f635"
      },
      "outputs": [],
      "source": [
        "green_mean_repeat = np.array(green_mean_repeat_list_raw)\n",
        "red_mean_repeat = np.array(red_mean_repeat_list_raw)\n",
        "\n",
        "green_mean_run = np.array(green_mean_run_list_raw)\n",
        "red_mean_run = np.array(red_mean_run_list_raw)\n",
        "\n",
        "# print(green_mean_repeat.shape)\n",
        "# print(red_mean_repeat.shape)\n",
        "# print(green_mean_run.shape)\n",
        "# print(red_mean_run.shape)\n",
        "\n",
        "x_mean_repeat_restored_list = []\n",
        "x_mean_repeat_restored_coefficients_list = []\n",
        "for x_mean_repeat, x_mean_run in [(green_mean_repeat, green_mean_run), (red_mean_repeat, red_mean_run)]:\n",
        "    # Define the exponential function for regression of decay\n",
        "    def exponential_func(x, lambda_):\n",
        "        return np.exp(lambda_ * x)\n",
        "\n",
        "    how_many_run = 2\n",
        "    repeat_num_per_run = int(x_mean_repeat.shape[0]/how_many_run)\n",
        "\n",
        "    x_mean_repeat_restored = copy.deepcopy(x_mean_repeat)\n",
        "    x_mean_repeat_restored_coefficients = copy.deepcopy(x_mean_repeat)\n",
        "    for i in range(how_many_run):\n",
        "        y = copy.deepcopy(x_mean_repeat[i*repeat_num_per_run:(i+1)*repeat_num_per_run])\n",
        "        multiplier = 1 / y[0]\n",
        "        y = y * multiplier\n",
        "        x = np.arange(len(y))\n",
        "        popt, pcov = curve_fit(exponential_func, x, y)\n",
        "        lambda_ = popt[0]\n",
        "        print(f\"exponent in the exp func is {lambda_}\")\n",
        "        y_fit = exponential_func(x, lambda_)\n",
        "\n",
        "        recover_factor = y / y_fit\n",
        "        print(\"recover_factor.shape:\", recover_factor.shape)\n",
        "        # print(recover_factor)\n",
        "\n",
        "        x_mean_repeat_restored[i*repeat_num_per_run:(i+1)*repeat_num_per_run] *= recover_factor * x_mean_run[0] / x_mean_run[i]\n",
        "        x_mean_repeat_restored_coefficients[i*repeat_num_per_run:(i+1)*repeat_num_per_run] = recover_factor * x_mean_run[0] / x_mean_run[i]\n",
        "\n",
        "    x_mean_repeat_restored_list.append(x_mean_repeat_restored)\n",
        "    x_mean_repeat_restored_coefficients_list.append(x_mean_repeat_restored_coefficients)\n",
        "\n",
        "green_mean_repeat_restored, red_mean_repeat_restored = x_mean_repeat_restored_list\n",
        "green_mean_repeat_restored_coefficients, red_mean_repeat_restored_coefficients = x_mean_repeat_restored_coefficients_list\n",
        "\n",
        "## generate green_signal_traces_mean_restored and red_signal_traces_mean_restored\n",
        "green_signal_traces_mean_restored = copy.deepcopy(green_signal_traces)\n",
        "red_signal_traces_mean_restored = copy.deepcopy(red_signal_traces)\n",
        "\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        green_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len] *= green_mean_repeat_restored_coefficients[run_*repeat_num+i]\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        red_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len] *= red_mean_repeat_restored_coefficients[run_*repeat_num+i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYob3i9RXa0c"
      },
      "source": [
        "#### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ofnbD9kcHMjT",
        "outputId": "28ef1e2a-c5e2-478b-a22b-7509f3287da2"
      },
      "outputs": [],
      "source": [
        "### mean each run\n",
        "\n",
        "print(\"mean of each run of green:\")\n",
        "green_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(green_signal_traces_mean_restored[run_,:,:])\n",
        "    green_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "print(\"mean of each run of red:\")\n",
        "red_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(red_signal_traces_mean_restored[run_,:,:])\n",
        "    red_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_mean_run\n",
        "y2 = red_mean_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8)) # [6.4, 4.8] is deault size, same as fig, ax1 = plt.subplots()\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Run', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "print(min_y1_, max_y1_, unit_unified, '------------------')\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Runs\\n(After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each run\n",
        "\n",
        "print(\"std of each run of green:\")\n",
        "green_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(green_signal_traces_mean_restored[run_,:,:])\n",
        "    green_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "print(\"std of each run of red:\")\n",
        "red_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(red_signal_traces_mean_restored[run_,:,:])\n",
        "    red_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_std_run\n",
        "y2 = red_std_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Run', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Runs\\n(After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### mean each repeat\n",
        "\n",
        "# print(\"mean of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored.shape[2]/repeat_num)\n",
        "green_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# print(\"mean of each repeat of red:\")\n",
        "red_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_mean_repeat\n",
        "y2 = red_mean_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Repeat', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Repeats (After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each repeat\n",
        "\n",
        "# print(\"std of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored.shape[2]/repeat_num)\n",
        "green_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(green_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# print(\"std of each repeat of red:\")\n",
        "red_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(red_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_std_repeat\n",
        "y2 = red_std_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Repeat', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Repeats (After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "## store them to variables with new names\n",
        "## and for later use in decay restoration\n",
        "green_mean_repeat_list_after_mean_restored = copy.deepcopy(green_mean_repeat) # same as green_mean_repeat_restored\n",
        "red_mean_repeat_list_after_mean_restored = copy.deepcopy(red_mean_repeat) # same as red_mean_repeat_restored\n",
        "\n",
        "green_mean_run_list_after_mean_restored = copy.deepcopy(green_mean_run)\n",
        "red_mean_run_list_after_mean_restored = copy.deepcopy(red_mean_run)\n",
        "\n",
        "green_std_repeat_list_after_mean_restored = copy.deepcopy(green_std_repeat)\n",
        "red_std_repeat_list_after_mean_restored = copy.deepcopy(red_std_repeat)\n",
        "\n",
        "green_std_run_list_after_mean_restored = copy.deepcopy(green_std_run)\n",
        "red_std_run_list_after_mean_restored = copy.deepcopy(red_std_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGbPuphzKJsK"
      },
      "source": [
        "### Restore std and plot mean each run, std each run, mean each repeat, std each repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQf-wV2kXdCk"
      },
      "source": [
        "#### Restore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1KJQRHZY3kr"
      },
      "outputs": [],
      "source": [
        "## convert to numpy form\n",
        "green_mean_repeat = np.array(green_mean_repeat_list_after_mean_restored)\n",
        "red_mean_repeat = np.array(red_mean_repeat_list_after_mean_restored)\n",
        "\n",
        "green_mean_run = np.array(green_mean_run_list_after_mean_restored)\n",
        "red_mean_run = np.array(red_mean_run_list_after_mean_restored)\n",
        "\n",
        "green_std_repeat = np.array(green_std_repeat_list_after_mean_restored)\n",
        "red_std_repeat = np.array(red_std_repeat_list_after_mean_restored)\n",
        "\n",
        "green_std_run = np.array(green_std_run_list_after_mean_restored)\n",
        "red_std_run = np.array(red_std_run_list_after_mean_restored)\n",
        "\n",
        "# print(green_std_repeat, red_std_repeat, green_std_run, red_std_run)\n",
        "# print(green_mean_repeat, red_mean_repeat, green_mean_run, red_mean_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc7SLcACX55c",
        "outputId": "9e3efa11-3122-42ea-8eff-820ed8de15b6"
      },
      "outputs": [],
      "source": [
        "x_std_repeat_restored_list = []\n",
        "x_std_repeat_restored_coefficients_list = []\n",
        "for x_std_repeat, x_std_run in [(green_std_repeat, green_std_run), (red_std_repeat, red_std_run)]:\n",
        "    # Define the exponential function for regression of decay\n",
        "    def exponential_func(x, lambda_):\n",
        "        return np.exp(lambda_ * x)\n",
        "\n",
        "    how_many_run = 2\n",
        "    repeat_num_per_run = int(x_std_repeat.shape[0]/how_many_run)\n",
        "\n",
        "    x_std_repeat_restored = copy.deepcopy(x_std_repeat)\n",
        "    x_std_repeat_restored_coefficients = copy.deepcopy(x_std_repeat)\n",
        "    for i in range(how_many_run):\n",
        "        y = copy.deepcopy(x_std_repeat[i*repeat_num_per_run:(i+1)*repeat_num_per_run])\n",
        "        multiplier = 1 / y[0]\n",
        "        y = y * multiplier\n",
        "        x = np.arange(len(y))\n",
        "        popt, pcov = curve_fit(exponential_func, x, y)\n",
        "        lambda_ = popt[0]\n",
        "        print(f\"exponent in the exp func is {lambda_}\")\n",
        "        y_fit = exponential_func(x, lambda_)\n",
        "\n",
        "        recover_factor = y / y_fit\n",
        "        print(\"recover_factor.shape:\", recover_factor.shape)\n",
        "        # print(recover_factor)\n",
        "\n",
        "        x_std_repeat_restored[i*repeat_num_per_run:(i+1)*repeat_num_per_run] *= recover_factor * x_std_run[0] / x_std_run[i]\n",
        "        x_std_repeat_restored_coefficients[i*repeat_num_per_run:(i+1)*repeat_num_per_run] = recover_factor * x_std_run[0] / x_std_run[i]\n",
        "\n",
        "    x_std_repeat_restored_list.append(x_std_repeat_restored)\n",
        "    x_std_repeat_restored_coefficients_list.append(x_std_repeat_restored_coefficients)\n",
        "\n",
        "green_std_repeat_restored, red_std_repeat_restored = x_std_repeat_restored_list\n",
        "green_std_repeat_restored_coefficients, red_std_repeat_restored_coefficients = x_std_repeat_restored_coefficients_list\n",
        "\n",
        "## generate red_signal_traces_mean_restored_std_restored and red_signal_traces_mean_restored_std_restored\n",
        "green_signal_traces_mean_restored_std_restored = copy.deepcopy(green_signal_traces_mean_restored)\n",
        "red_signal_traces_mean_restored_std_restored = copy.deepcopy(red_signal_traces_mean_restored)\n",
        "\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        temp_ = green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] - mean_\n",
        "        temp_ *= green_std_repeat_restored_coefficients[run_*repeat_num+i]\n",
        "        green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] = mean_ + temp_\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        temp_ = red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] - mean_\n",
        "        temp_ *= red_std_repeat_restored_coefficients[run_*repeat_num+i]\n",
        "        red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] = mean_ + temp_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4fCPvB-Xeuc"
      },
      "source": [
        "#### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tMymkM80VRqP",
        "outputId": "5b6684b7-038a-49b9-cf6f-9c1a0c57e70a"
      },
      "outputs": [],
      "source": [
        "### mean each run\n",
        "\n",
        "print(\"mean of each run of green:\")\n",
        "green_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(green_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    green_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "print(\"mean of each run of red:\")\n",
        "red_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(red_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    red_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_mean_run\n",
        "y2 = red_mean_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8)) # [6.4, 4.8] is deault size, same as fig, ax1 = plt.subplots()\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Run', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Runs\\n(After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each run\n",
        "\n",
        "print(\"std of each run of green:\")\n",
        "green_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(green_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    green_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "print(\"std of each run of red:\")\n",
        "red_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(red_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    red_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_std_run\n",
        "y2 = red_std_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Run', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Runs\\n(After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### mean each repeat\n",
        "\n",
        "# print(\"mean of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored_std_restored.shape[2]/repeat_num)\n",
        "green_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# print(\"mean of each repeat of red:\")\n",
        "red_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_mean_repeat\n",
        "y2 = red_mean_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Repeat', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Repeats (After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each repeat\n",
        "\n",
        "# print(\"std of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored_std_restored.shape[2]/repeat_num)\n",
        "green_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# print(\"std of each repeat of red:\")\n",
        "red_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_std_repeat\n",
        "y2 = red_std_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Repeat', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "if max_y1_ <= min_y1_:\n",
        "    min_y1_, max_y1_ = min_y1_ - 100, max_y1_ + 100\n",
        "if max_y2_ <= min_y2_:\n",
        "    min_y2_, max_y2_ = min_y2_ - 100, max_y2_ + 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Repeats (After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "## store them to variables with new names\n",
        "## and for later use in decay restoration\n",
        "green_mean_repeat_list_after_mean_restored_std_restored = copy.deepcopy(green_mean_repeat) # same as green_mean_repeat_restored\n",
        "red_mean_repeat_list_after_mean_restored_std_restored = copy.deepcopy(red_mean_repeat) # same as red_mean_repeat_restored\n",
        "\n",
        "green_mean_run_list_after_mean_restored_std_restored = copy.deepcopy(green_mean_run)\n",
        "red_mean_run_list_after_mean_restored_std_restored = copy.deepcopy(red_mean_run)\n",
        "\n",
        "green_std_repeat_list_after_mean_restored_std_restored = copy.deepcopy(green_std_repeat)\n",
        "red_std_repeat_list_after_mean_restored_std_restored = copy.deepcopy(red_std_repeat)\n",
        "\n",
        "green_std_run_list_after_mean_restored_std_restored = copy.deepcopy(green_std_run)\n",
        "red_std_run_list_after_mean_restored_std_restored = copy.deepcopy(red_std_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HjT2vn1_P2O"
      },
      "source": [
        "### Plot tunning curves using retored data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEh2fzrE_gPS"
      },
      "source": [
        "This is the same process as in \"Read and plot fluorescence structure data/Eg of function use/Read red and green data then plot\", but uses restored data. Need to use function recover_strucure_from_traces_and_locs() to recover conca_fluo_data (structure data) from signal traces and locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rti0_PVf_7as"
      },
      "outputs": [],
      "source": [
        "# cell_name_list = ['CL075_230228']\n",
        "# run_num_list = [['1', '2', '3']]\n",
        "\n",
        "# cell_name_list = ['CL090_230515']\n",
        "# run_num_list = [['4', '5', '6']]\n",
        "\n",
        "# for cell_name_, run_num_ in zip(cell_name_list, run_num_list):\n",
        "#     for color_ in ['red', 'green']:\n",
        "#         for datatype_ in ['F']:\n",
        "#             if color_ == 'red':\n",
        "#                 conca_fluo_data_ = recover_strucure_from_traces_and_locs(red_signal_traces_mean_restored_std_restored, red_locations)\n",
        "#                 plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_)\n",
        "#             if color_ == 'green':\n",
        "#                 conca_fluo_data_ = recover_strucure_from_traces_and_locs(green_signal_traces_mean_restored_std_restored, green_locations)\n",
        "#                 for component_ in range(1, conca_fluo_data_[0,0].shape[1]+1):\n",
        "#                     plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_, component_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0NseRgUA97I"
      },
      "outputs": [],
      "source": [
        "# # batch download the plotted figures\n",
        "# # uncomment the code below to download figures if needed\n",
        "\n",
        "# import glob\n",
        "\n",
        "# folder_path = '.'\n",
        "# # file_prefix = 'All_Conditions_All_Rounds_All_Repeats_'\n",
        "# file_prefix = 'CL'\n",
        "\n",
        "# # Use glob to find all files with the given prefix in the folder\n",
        "# matching_files = glob.glob(f\"{folder_path}/{file_prefix}*\")\n",
        "# matching_files_new = []\n",
        "# for file_path in matching_files:\n",
        "#     # Check if the file ends with \".pdf\"\n",
        "#     if file_path.lower().endswith('.pdf'):\n",
        "#         matching_files_new.append(file_path)\n",
        "# matching_files = matching_files_new\n",
        "# # print(matching_files)\n",
        "# # # Print the matching file names\n",
        "# # for file_path in matching_files:\n",
        "# #     print(file_path)\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# zip_filename = 'files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     # Add files to the zip file\n",
        "#     for file_path in matching_files:\n",
        "#         zipf.write(file_path)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QtazLHcCM8"
      },
      "source": [
        "### Save important data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-vsdS6PcEwK"
      },
      "outputs": [],
      "source": [
        "# np.save(cell_name+'red_signal_traces.npy', red_signal_traces)\n",
        "# np.save(cell_name+'red_conca_fluo_data.npy', red_conca_fluo_data)\n",
        "# np.save(cell_name+'red_locations.npy', red_locations)\n",
        "\n",
        "# np.save(cell_name+'green_signal_traces.npy', green_signal_traces)\n",
        "# np.save(cell_name+'green_conca_fluo_data.npy', green_conca_fluo_data)\n",
        "# np.save(cell_name+'green_locations.npy', green_locations)\n",
        "\n",
        "# np.save(cell_name+'green_signal_traces_mean_restored.npy', green_signal_traces_mean_restored)\n",
        "# np.save(cell_name+'red_signal_traces_mean_restored.npy', red_signal_traces_mean_restored)\n",
        "\n",
        "# np.save(cell_name+'green_signal_traces_mean_restored_std_restored.npy', green_signal_traces_mean_restored_std_restored)\n",
        "# np.save(cell_name+'red_signal_traces_mean_restored_std_restored.npy', red_signal_traces_mean_restored_std_restored)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnnQRZQHeIS1"
      },
      "outputs": [],
      "source": [
        "# # Load red signal traces data\n",
        "# red_signal_traces = np.load(cell_name + 'red_signal_traces.npy')\n",
        "# print(\"Red Signal Traces Shape:\", red_signal_traces.shape)\n",
        "# print(\"Red Signal Traces Type:\", type(red_signal_traces))\n",
        "\n",
        "# # Load red concatenated fluorescence data\n",
        "# red_conca_fluo_data = np.load(cell_name + 'red_conca_fluo_data.npy', allow_pickle=True)\n",
        "# print(\"Red Concatenated Fluorescence Data Shape:\", red_conca_fluo_data.shape)\n",
        "# print(\"Red Concatenated Fluorescence Data Type:\", type(red_conca_fluo_data))\n",
        "# print(\"Red Concatenated Fluorescence Data Elements Shape:\", red_conca_fluo_data[0,0].shape)\n",
        "# print(\"Red Concatenated Fluorescence Data Elements Type:\", type(red_conca_fluo_data[0, 0]))\n",
        "\n",
        "# # Load red locations data\n",
        "# red_locations = np.load(cell_name + 'red_locations.npy', allow_pickle=True)\n",
        "# print(\"Red Locations Shape:\", red_locations.shape)\n",
        "# print(\"Red Locations Type:\", type(red_locations))\n",
        "# print(\"Red Locations Elements Shape:\", red_locations[0,0].shape)\n",
        "# print(\"Red Locations Elements Type:\", type(red_locations[0, 0]))\n",
        "\n",
        "# # Load green signal traces data\n",
        "# green_signal_traces = np.load(cell_name + 'green_signal_traces.npy')\n",
        "# print(\"Green Signal Traces Shape:\", green_signal_traces.shape)\n",
        "# print(\"Green Signal Traces Type:\", type(green_signal_traces))\n",
        "\n",
        "# # Load green concatenated fluorescence data\n",
        "# green_conca_fluo_data = np.load(cell_name + 'green_conca_fluo_data.npy', allow_pickle=True)\n",
        "# print(\"Green Concatenated Fluorescence Data Shape:\", green_conca_fluo_data.shape)\n",
        "# print(\"Green Concatenated Fluorescence Data Type:\", type(green_conca_fluo_data))\n",
        "# print(\"Green Concatenated Fluorescence Elements Data Shape:\", green_conca_fluo_data[0,0].shape)\n",
        "# print(\"Green Concatenated Fluorescence Elements Data Type:\", type(green_conca_fluo_data[0, 0]))\n",
        "\n",
        "# # Load green locations data\n",
        "# green_locations = np.load(cell_name + 'green_locations.npy', allow_pickle=True)\n",
        "# print(\"Green Locations Shape:\", green_locations.shape)\n",
        "# print(\"Green Locations Type:\", type(green_locations))\n",
        "# print(\"Green Locations Elements Shape:\", green_locations[0,0].shape)\n",
        "# print(\"Green Locations Elements Type:\", type(green_locations[0, 0]))\n",
        "\n",
        "# # Load green signal traces mean restored data\n",
        "# green_signal_traces_mean_restored = np.load(cell_name + 'green_signal_traces_mean_restored.npy')\n",
        "# print(\"Green Signal Traces Mean Restored Shape:\", green_signal_traces_mean_restored.shape)\n",
        "# print(\"Green Signal Traces Mean Restored Type:\", type(green_signal_traces_mean_restored))\n",
        "\n",
        "# # Load red signal traces mean restored data\n",
        "# red_signal_traces_mean_restored = np.load(cell_name + 'red_signal_traces_mean_restored.npy')\n",
        "# print(\"Red Signal Traces Mean Restored Shape:\", red_signal_traces_mean_restored.shape)\n",
        "# print(\"Red Signal Traces Mean Restored Type:\", type(red_signal_traces_mean_restored))\n",
        "\n",
        "# # Load green signal traces mean restored std restored data\n",
        "# green_signal_traces_mean_restored_std_restored = np.load(cell_name + 'green_signal_traces_mean_restored_std_restored.npy')\n",
        "# print(\"Green Signal Traces Mean Restored Std Restored Shape:\", green_signal_traces_mean_restored_std_restored.shape)\n",
        "# print(\"Green Signal Traces Mean Restored Std Restored Type:\", type(green_signal_traces_mean_restored_std_restored))\n",
        "\n",
        "# # Load red signal traces mean restored std restored data\n",
        "# red_signal_traces_mean_restored_std_restored = np.load(cell_name + 'red_signal_traces_mean_restored_std_restored.npy')\n",
        "# print(\"Red Signal Traces Mean Restored Std Restored Shape:\", red_signal_traces_mean_restored_std_restored.shape)\n",
        "# print(\"Red Signal Traces Mean Restored Std Restored Type:\", type(red_signal_traces_mean_restored_std_restored))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download files if running on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGpdQsmRlVZ7"
      },
      "outputs": [],
      "source": [
        "# # batch download the plotted figures\n",
        "# # uncomment the code below to download figures if needed\n",
        "\n",
        "# import glob\n",
        "\n",
        "# folder_path = '.'\n",
        "# # file_prefix = 'All_Conditions_All_Rounds_All_Repeats_'\n",
        "# file_prefix = 'CL'\n",
        "\n",
        "# # Use glob to find all files with the given prefix in the folder\n",
        "# matching_files = glob.glob(f\"{folder_path}/{file_prefix}*\")\n",
        "# # print(matching_files)\n",
        "# # # Print the matching file names\n",
        "# # for file_path in matching_files:\n",
        "# #     print(file_path)\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# zip_filename = 'files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     # Add files to the zip file\n",
        "#     for file_path in matching_files:\n",
        "#         zipf.write(file_path)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wV8MuOh-cOa"
      },
      "source": [
        "## Generate data and label, then train and eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4HsSpTlljaO"
      },
      "source": [
        "In this chapter, each code cell has a title so that you can navigate it easily through the table of contents. Because some code cells are long, otherwise it is hard to locate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t0kj92GkFcZ"
      },
      "source": [
        "### === Set whether including sigmoid ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtCRAMIUhrzG"
      },
      "outputs": [],
      "source": [
        "with_sigmoid = True # whether include a sigmoid unit in our model (non-hierarchical model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### === Set whether setting weights non-negative ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_nonnegative = True # whether only allow non-negative weights for linear layers in training\n",
        "strong_penalty = False # whether using strong penalty to restrict the weights to nonnegative values\n",
        "# if weights_nonnegative = False, then the value of strong_penalty doesn't matter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPpbc4sykK8u"
      },
      "source": [
        "### Get valid components (=== different cell may have different paras ===)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633X0R2-TxYM",
        "outputId": "546ca668-4f65-4db0-a168-3683c24240ca"
      },
      "outputs": [],
      "source": [
        "# # different cell has different parameters, this is for cell CL075_230228\n",
        "# valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components(cell_name, 100, 5)\n",
        "# data_set = copy.deepcopy(green_signal_traces)\n",
        "# label_set = copy.deepcopy(red_signal_traces)\n",
        "# print(data_set[:,valid_com_index_list,:].shape) # remain the valid component\n",
        "# print(valid_com_index_list)\n",
        "# print(valid_dis_list)\n",
        "\n",
        "\n",
        "# different cell has different parameters, this is for cell CL090_230515 / CL096_231018\n",
        "valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components(cell_name, 100, 0)\n",
        "data_set = copy.deepcopy(green_signal_traces)\n",
        "label_set = copy.deepcopy(red_signal_traces)\n",
        "print(data_set[:,valid_com_index_list,:].shape) # remain the valid component\n",
        "print(valid_com_index_list)\n",
        "\n",
        "# # different cell has different parameters, this is for cell CL079_230324\n",
        "# valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components(cell_name, 100, 0)\n",
        "# data_set = copy.deepcopy(green_signal_traces)\n",
        "# label_set = copy.deepcopy(red_signal_traces)\n",
        "# print(data_set[:,valid_com_index_list,:].shape) # remain the valid component\n",
        "# print(valid_com_index_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Gxr54Olbai"
      },
      "source": [
        "### Show red and green signal traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EVIvD9WZuBHh",
        "outputId": "42662866-8668-4d3f-a456-5a2c9b644f53"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "\n",
        "## plot red\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces)\n",
        "        label_set = copy.deepcopy(red_signal_traces)\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored)\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored_std_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored_std_restored)\n",
        "\n",
        "    x = range(1, label_set.shape[2] + 1)\n",
        "\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    # plt.plot(list(label_set[0,0,:])+list(label_set[1,0,:])+list(label_set[2,0,:]), color='salmon', linestyle='-', linewidth=1)\n",
        "    plt.plot(list(label_set[0,0,:])+list(label_set[1,0,:]), color='salmon', linestyle='-', linewidth=1)\n",
        "    # to use which one in above two lines depends on how many runs, 2 or 3.\n",
        "    v1 = label_set.shape[2] + 0.5\n",
        "    v2 = label_set.shape[2] * 2 + 0.5\n",
        "    plt.axvline(x=v1, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.axvline(x=v2, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('Frames', fontsize = 20)\n",
        "    plt.ylabel('F Value', fontsize = 20)\n",
        "    plt.tick_params(labelsize=18)\n",
        "    plt.title(f'Whole red signal trace ({signal_trace_type})', fontsize = 24)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## plot green\n",
        "component_index = 20\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces)\n",
        "        label_set = copy.deepcopy(red_signal_traces)\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored)\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored_std_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored_std_restored)\n",
        "\n",
        "    data_set = data_set[:, valid_com_index_list, :] # use valid components\n",
        "\n",
        "    x = range(1, data_set.shape[2] + 1)\n",
        "\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    # plt.plot(list(data_set[0,component_index,:])+list(data_set[1,component_index,:])+list(data_set[2,component_index,:]), color='limegreen', linestyle='-', linewidth=1)\n",
        "    plt.plot(list(data_set[0,component_index,:])+list(data_set[1,component_index,:]), color='limegreen', linestyle='-', linewidth=1) \n",
        "    # to use which one in above two lines depends on how many runs, 2 or 3.\n",
        "    v1 = data_set.shape[2] + 0.5\n",
        "    v2 = data_set.shape[2] * 2 + 0.5\n",
        "    plt.axvline(x=v1, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.axvline(x=v2, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('Frames', fontsize = 20)\n",
        "    plt.ylabel('F Value', fontsize = 20)\n",
        "    plt.tick_params(labelsize=18)\n",
        "    plt.title(f'Whole green signal trace of Component {component_index} ({signal_trace_type})', fontsize = 24)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-DpRkJ1_0ni"
      },
      "source": [
        "### Train, plot, save models (it takes some time) (=== set near on/offset ===)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ejCFm2ITARUU",
        "outputId": "ebcb0192-e5da-4250-bfa3-6df37473773e"
      },
      "outputs": [],
      "source": [
        "trial_times = 15  # try how many times to get the best one of them as the final result\n",
        "\n",
        "# end_near_on/offset means the data pieces selected for training is ending near on/offset \n",
        "# of the visual stimuli. Test data is still random without restriction.\n",
        "# Note: at most one of end_near_offset and end_near_onset can be True!\n",
        "end_near_offset = False\n",
        "end_near_onset = False\n",
        "\n",
        "\n",
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "list_of_train_loss_lists = []\n",
        "list_of_test_loss_lists = []\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces)\n",
        "        label_set = copy.deepcopy(red_signal_traces)\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored)\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored_std_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored_std_restored)\n",
        "\n",
        "    data_set = data_set[:,valid_com_index_list,:] # use valid components\n",
        "\n",
        "    print(f\"data_set.shape: {data_set.shape}\")\n",
        "    print(f\"label_set.shape: {label_set.shape}\")\n",
        "\n",
        "    # # Normalize data_set to [-1,1]\n",
        "    # data_set_min = np.min(data_set)\n",
        "    # data_set_max = np.max(data_set)\n",
        "    # data_set = ((data_set - data_set_min) / (data_set_max - data_set_min) - 0.5) * 2\n",
        "\n",
        "    # # Normalize label_set to [-1,1]\n",
        "    # label_set_min = np.min(label_set)\n",
        "    # label_set_max = np.max(label_set)\n",
        "    # label_set = ((label_set - label_set_min) / (label_set_max - label_set_min) - 0.5) * 2\n",
        "\n",
        "    # Normalize data_set to [0,1]\n",
        "    data_set_min = np.min(data_set)\n",
        "    data_set_max = np.max(data_set)\n",
        "    data_set = (data_set - data_set_min) / (data_set_max - data_set_min)\n",
        "\n",
        "    # Normalize label_set to [0,1]\n",
        "    label_set_min = np.min(label_set)\n",
        "    label_set_max = np.max(label_set)\n",
        "    label_set = (label_set - label_set_min) / (label_set_max - label_set_min)\n",
        "\n",
        "    # Define dataset class\n",
        "    class FluoDataset(Dataset):\n",
        "        def __init__(self, data_set, label_set, z_indices, x_indices):\n",
        "            self.data_set = data_set\n",
        "            self.label_set = label_set\n",
        "            self.z_indices = z_indices\n",
        "            self.x_indices = x_indices\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.z_indices)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            z = self.z_indices[idx]\n",
        "            x = self.x_indices[idx]\n",
        "\n",
        "            input_data = self.data_set[z, :, x-62:x]\n",
        "            target_label = np.mean(self.label_set[z, :, x-31:x])\n",
        "\n",
        "            return torch.tensor(input_data, dtype=torch.float32), torch.tensor(target_label, dtype=torch.float32)\n",
        "            # or torch.from_numpy(input_data).float(), torch.from_numpy(target_label).float()\n",
        "\n",
        "    # Set a random seed for reproducibility\n",
        "    # np.random.seed(16)  # can use any integer as the seed value\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    length = 6400*2\n",
        "    # choices = range(3)\n",
        "    choices = range(2) \n",
        "    train_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    choices = list(range(62, 10000)) + list(range(12000, 17000)) + list(range(18244, 32500)) # 90% for train\n",
        "    if end_near_offset or end_near_onset:\n",
        "        choices_ = []\n",
        "        if end_near_offset:\n",
        "            x_ = 0\n",
        "        if end_near_onset:\n",
        "            x_ = 1\n",
        "        count_ = 0\n",
        "        while(True):\n",
        "            count_ = count_ + 1\n",
        "            shape = red_locations.shape\n",
        "            random_indices = [np.random.randint(dim_size) for dim_size in shape]\n",
        "            selected_element = red_locations[random_indices[0], random_indices[1], random_indices[2], random_indices[3]]\n",
        "            end_index_ = selected_element[x_, 2]\n",
        "            end_index_ = random.randint(end_index_-5, end_index_+5)\n",
        "            if end_index_ in choices and end_index_ not in choices_:\n",
        "                choices_.append(end_index_)\n",
        "            if count_ >= length*3 or len(choices_) >= length:\n",
        "                break\n",
        "        choices = choices_\n",
        "    train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "\n",
        "    length = 640\n",
        "    # choices = range(3)\n",
        "    choices = range(2) \n",
        "    test_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    # choices = range(30000, 32500)\n",
        "    choices = list(range(10000, 12000)) + list(range(17000, 18244)) # 10% for test\n",
        "    # 12000-10000+18244-17000=3244, 3244/(32500-62)=10%, 32500-62 is the all data pieces\n",
        "    test_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640 # this is a subset of train data for test on train\n",
        "    # choices = range(3)\n",
        "    choices = range(2) \n",
        "    small_train_z_indices = np.random.choice(choices, size=length)\n",
        "    choices = list(range(9000, 10000)) + list(range(12000, 17000)) + list(range(18244, 19000))\n",
        "    small_train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    train_dataset = FluoDataset(data_set, label_set, train_z_indices, train_x_indices)\n",
        "    test_dataset = FluoDataset(data_set, label_set, test_z_indices, test_x_indices)\n",
        "    test_on_train_dataset = FluoDataset(data_set, label_set, small_train_z_indices, small_train_x_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_on_train_loader = DataLoader(test_on_train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # print(len(train_loader))\n",
        "    # print(len(test_loader))\n",
        "\n",
        "\n",
        "    # Define the model\n",
        "    class FluoModel(nn.Module):\n",
        "        def __init__(self, component_num):\n",
        "            super(FluoModel, self).__init__()\n",
        "            self.fc_shared = nn.Linear(62, 1)  # Shared fully connected layer\n",
        "            self.fc_reduce = nn.Linear(component_num, 1)  # Fully connected layer to reduce component_num (e.g., 281) channels to 1\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "            self.fc_end = nn.Linear(1, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1, 62)  # Reshape to (batch_size, component_num, 62)\n",
        "\n",
        "            # Apply the shared fully connected layer along the last dimension (62)\n",
        "            shared_output = self.fc_shared(x).squeeze(2)\n",
        "            # self.fc_shared(x) shape is (batch_size, component_num, 1), then squeeze the last dimension\n",
        "            # In PyTorch, when you apply a fully connected layer (or any other linear\n",
        "            # transformation) to a 3D tensor, by default, the operation is performed\n",
        "            # along the last dimension of the tensor.\n",
        "\n",
        "            # Reduce component_num (e.g., 281) channels to 1 using a separate fully connected layer\n",
        "            reduced_output = self.fc_reduce(shared_output)\n",
        "            if with_sigmoid:\n",
        "                pre_output = self.sigmoid(reduced_output) # if with sigmoid\n",
        "                output = self.fc_end(pre_output)\n",
        "            else:\n",
        "                output = reduced_output # if without sigmoid\n",
        "\n",
        "            return output\n",
        "\n",
        "    # Using a custom loss term during training that penalizes negative weights\n",
        "    if weights_nonnegative:\n",
        "        class NonNegLoss(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(NonNegLoss, self).__init__()\n",
        "\n",
        "            def forward(self, tensor):\n",
        "                return torch.sum(torch.relu(-tensor))\n",
        "        non_neg_loss = NonNegLoss()\n",
        "\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    for i_ in range(trial_times): # try multiple times to get the best one\n",
        "        model = FluoModel(data_set.shape[1])\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "        # optimizer = optim.Adam(model.parameters(), lr=0.004)\n",
        "        # scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 90\n",
        "        print_interval = 10\n",
        "\n",
        "        train_loss_list_ = []\n",
        "        test_loss_list_ = []\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                if weights_nonnegative:\n",
        "                    if strong_penalty:\n",
        "                        pnlty_coeff = 1 / 500\n",
        "                    else:\n",
        "                        pnlty_coeff = 1 / 1000\n",
        "                    loss = criterion(outputs.squeeze(), targets) + \\\n",
        "                    non_neg_loss(model.fc_shared.weight) * pnlty_coeff + \\\n",
        "                    non_neg_loss(model.fc_reduce.weight) * pnlty_coeff\n",
        "                    # print(non_neg_loss(model.fc_reduce.weight))\n",
        "                    # print(model.fc_reduce.weight)\n",
        "                    # print(torch.relu(-model.fc_reduce.weight))\n",
        "                else:\n",
        "                    loss = criterion(outputs.squeeze(), targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # scheduler.step()\n",
        "\n",
        "            # if epoch < 5 or (epoch + 1) % print_interval == 0:\n",
        "            #     print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item():.6f}\")\n",
        "\n",
        "            # # Enforce non-negativity constraint on weights (not including biases) at the end of each epoch\n",
        "            # # Enforcing strict non-negativity constraints on the weights after BP makes the training process \n",
        "            # # more challenging and very difficult to converge. So, basically, this method is ruled out and not\n",
        "            # # used. I add a loss to penalize the negative weights instead.\n",
        "            # if weights_nonnegative:\n",
        "            #     with torch.no_grad():\n",
        "            #         model.fc_reduce.weight.data.clamp_(min=0)\n",
        "            #         model.fc_shared.weight.data.clamp_(min=0)\n",
        "\n",
        "            # Testing on train and test sets\n",
        "            model.eval()\n",
        "\n",
        "            train_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in test_on_train_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    train_loss += criterion(outputs.squeeze(), targets).item()\n",
        "\n",
        "            average_train_loss = train_loss / len(test_on_train_dataset)\n",
        "            train_loss_list_.append(average_train_loss)\n",
        "\n",
        "            test_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in test_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    test_loss += criterion(outputs.squeeze(), targets).item()\n",
        "\n",
        "            average_test_loss = test_loss / len(test_loader)\n",
        "            if i_ != 0: # at least one complete training -- let the 1st training be complete\n",
        "                if epoch == 5 and average_test_loss >= 0.9 * np.mean(np.array(test_loss_list_)[1:]):\n",
        "                    break; # kill trials with a low probability of convergence (terminate trials that are unlikely to converge)\n",
        "            test_loss_list_.append(average_test_loss)\n",
        "\n",
        "            if epoch < 5 or (epoch + 1) % print_interval == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {average_train_loss:.6f} | Test Loss: {average_test_loss:.6f}\")\n",
        "\n",
        "        if i_ == 0:\n",
        "            train_loss_list, test_loss_list = train_loss_list_, test_loss_list_\n",
        "            model_final = model\n",
        "        elif test_loss_list_[-1] < test_loss_list[-1] and len(train_loss_list_) == num_epochs:\n",
        "            train_loss_list, test_loss_list = train_loss_list_, test_loss_list_\n",
        "            model_final = model\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(train_loss_list, color = 'skyblue', label='Train Loss', linewidth=2)\n",
        "    plt.plot(test_loss_list, color = 'salmon', label='Test Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch Index', fontsize=16)\n",
        "    plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Train and Test Loss Curves\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    list_of_train_loss_lists.append(train_loss_list)\n",
        "    list_of_test_loss_lists.append(test_loss_list)\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        torch.save(model_final.state_dict(), f)\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "    outputs_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs_list.append(outputs.numpy())\n",
        "            labels_list.append(targets.numpy())\n",
        "\n",
        "    outputs_array = np.concatenate(outputs_list)\n",
        "    labels_array = np.concatenate(labels_list)\n",
        "\n",
        "    # Sort labels and corresponding outputs\n",
        "    sorted_indices = np.argsort(labels_array)\n",
        "    sorted_labels = labels_array[sorted_indices]\n",
        "    sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(sorted_labels, color = 'skyblue', label='True Labels', linewidth=2)\n",
        "    plt.plot(sorted_outputs, color = 'salmon', label='Model Outputs', linewidth=2)\n",
        "    plt.xlabel('Sample Index', fontsize=16)\n",
        "    plt.ylabel('Normalized Value', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Comparison between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "    plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRtk4fVokdRj"
      },
      "source": [
        "### Plot comparison of losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJIaek7Ohqe6",
        "outputId": "3a1de7ab-3937-438f-bdbd-491f457e4fff"
      },
      "outputs": [],
      "source": [
        "# compare train losses and compare test loss between different types of signal traces\n",
        "\n",
        "colors_list = [('deepskyblue', 'orangered'), ('royalblue', 'salmon'), ('cornflowerblue', 'violet')]\n",
        "linestyle_list = ['dashed', 'dotted', 'solid']\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(train_loss_list, color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "    # plt.plot(train_loss_list[-10:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Train Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(test_loss_list, color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "    # plt.plot(test_loss_list[-10:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Test Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plot last epochs\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(np.arange(len(train_loss_list)+1-50, len(train_loss_list)+1), train_loss_list[-50:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "    # plt.plot(train_loss_list[-10:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Train Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(np.arange(len(test_loss_list)+1-50, len(test_loss_list)+1), test_loss_list[-50:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "    # plt.plot(test_loss_list[-10:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Test Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2xkp3JLkkqL"
      },
      "source": [
        "### Load model weights and replot (=== set weight redistribution test here ===)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwsXtyvjsfC"
      },
      "source": [
        "The next chunck plots \"Comparison between True Labels and Model Outputs\" and \"Scatter Plot between True Labels and Model Outputs\" (and some other figures) \n",
        "\n",
        "This part is done without training a model but with directly loading the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "orqdjQCiL18S",
        "outputId": "b8483419-e05a-4de4-c79b-5d1e05d6016d"
      },
      "outputs": [],
      "source": [
        "## whether redistribute weights to see robustness\n",
        "## if redistribute = False, then strong_or_weak_redistribute's value doesn't matter.\n",
        "redistribute = False\n",
        "strong_or_weak_redistribute = False # True is strong, False is weak\n",
        "\n",
        "\n",
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces)\n",
        "        label_set = copy.deepcopy(red_signal_traces)\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored)\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = copy.deepcopy(green_signal_traces_mean_restored_std_restored)\n",
        "        label_set = copy.deepcopy(red_signal_traces_mean_restored_std_restored)\n",
        "\n",
        "    data_set = data_set[:,valid_com_index_list,:] # use valid components\n",
        "\n",
        "    print(f\"data_set.shape: {data_set.shape}\")\n",
        "    print(f\"label_set.shape: {label_set.shape}\")\n",
        "\n",
        "    # # Normalize data_set to [-1,1]\n",
        "    # data_set_min = np.min(data_set)\n",
        "    # data_set_max = np.max(data_set)\n",
        "    # data_set = ((data_set - data_set_min) / (data_set_max - data_set_min) - 0.5) * 2\n",
        "\n",
        "    # # Normalize label_set to [-1,1]\n",
        "    # label_set_min = np.min(label_set)\n",
        "    # label_set_max = np.max(label_set)\n",
        "    # label_set = ((label_set - label_set_min) / (label_set_max - label_set_min) - 0.5) * 2\n",
        "\n",
        "    # Normalize data_set to [0,1]\n",
        "    data_set_min = np.min(data_set)\n",
        "    data_set_max = np.max(data_set)\n",
        "    data_set = (data_set - data_set_min) / (data_set_max - data_set_min)\n",
        "\n",
        "    # Normalize label_set to [0,1]\n",
        "    label_set_min = np.min(label_set)\n",
        "    label_set_max = np.max(label_set)\n",
        "    label_set = (label_set - label_set_min) / (label_set_max - label_set_min)\n",
        "\n",
        "    # Define dataset class\n",
        "    class FluoDataset(Dataset):\n",
        "        def __init__(self, data_set, label_set, z_indices, x_indices):\n",
        "            self.data_set = data_set\n",
        "            self.label_set = label_set\n",
        "            self.z_indices = z_indices\n",
        "            self.x_indices = x_indices\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.z_indices)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            z = self.z_indices[idx]\n",
        "            x = self.x_indices[idx]\n",
        "\n",
        "            input_data = self.data_set[z, :, x-62:x]\n",
        "            target_label = np.mean(self.label_set[z, :, x-31:x])\n",
        "\n",
        "            return torch.tensor(input_data, dtype=torch.float32), torch.tensor(target_label, dtype=torch.float32)\n",
        "            # or torch.from_numpy(input_data).float(), torch.from_numpy(target_label).float()\n",
        "\n",
        "    # Set a random seed for reproducibility\n",
        "    # np.random.seed(16)  # can use any integer as the seed value\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    length = 6400*2\n",
        "    choices = range(3)\n",
        "    choices = range(2) # delete last run\n",
        "    train_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    choices = list(range(62, 10000)) + list(range(12000, 17000)) + list(range(18244, 32500)) # 90% for train\n",
        "    # 10000-62+17000-12000+32500-18244=29194, 29194/(32500-62)=90%, 32500-62 is the all data pieces\n",
        "    train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640\n",
        "    choices = range(3)\n",
        "    choices = range(2) # delete last run\n",
        "    test_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    # choices = range(30000, 32500)\n",
        "    choices = list(range(10000, 12000)) + list(range(17000, 18244)) # 10% for test\n",
        "    # 12000-10000+18244-17000=3244, 3244/(32500-62)=10%, 32500-62 is the all data pieces\n",
        "    test_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640 # this is a subset of train data for test on train\n",
        "    choices = range(3)\n",
        "    small_train_z_indices = np.random.choice(choices, size=length)\n",
        "    choices = list(range(9000, 10000)) + list(range(12000, 17000)) + list(range(18244, 19000))\n",
        "    small_train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    train_dataset = FluoDataset(data_set, label_set, train_z_indices, train_x_indices)\n",
        "    test_dataset = FluoDataset(data_set, label_set, test_z_indices, test_x_indices)\n",
        "    test_on_train_dataset = FluoDataset(data_set, label_set, small_train_z_indices, small_train_x_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_on_train_loader = DataLoader(test_on_train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # print(len(train_loader))\n",
        "    # print(len(test_loader))\n",
        "\n",
        "\n",
        "    # Define the model\n",
        "    class FluoModel(nn.Module):\n",
        "        def __init__(self, component_num):\n",
        "            super(FluoModel, self).__init__()\n",
        "            self.fc_shared = nn.Linear(62, 1)  # Shared fully connected layer\n",
        "            self.fc_reduce = nn.Linear(component_num, 1)  # Fully connected layer to reduce component_num (e.g., 281) channels to 1\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "            self.fc_end = nn.Linear(1, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1, 62)  # Reshape to (batch_size, component_num, 62)\n",
        "\n",
        "            # Apply the shared fully connected layer along the last dimension (62)\n",
        "            shared_output = self.fc_shared(x).squeeze(2)\n",
        "            # self.fc_shared(x) shape is (batch_size, component_num, 1), then squeeze the last dimension\n",
        "            # In PyTorch, when you apply a fully connected layer (or any other linear\n",
        "            # transformation) to a 3D tensor, by default, the operation is performed\n",
        "            # along the last dimension of the tensor.\n",
        "\n",
        "            # Reduce component_num (e.g., 281) channels to 1 using a separate fully connected layer\n",
        "            reduced_output = self.fc_reduce(shared_output)\n",
        "            if with_sigmoid:\n",
        "                pre_output = self.sigmoid(reduced_output) # if with sigmoid\n",
        "                output = self.fc_end(pre_output)\n",
        "            else:\n",
        "                output = reduced_output # if without sigmoid\n",
        "\n",
        "            return output\n",
        "        \n",
        "        def get_layer_output(self, x, layer_name):\n",
        "            if layer_name == \"shared\":\n",
        "                shared_output = self.fc_shared(x).squeeze(2)\n",
        "                return shared_output\n",
        "            elif layer_name == \"reduce\":\n",
        "                shared_output = self.fc_shared(x).squeeze(2)\n",
        "                reduced_output = self.fc_reduce(shared_output)\n",
        "                return reduced_output\n",
        "            elif layer_name == \"sigmoid\":\n",
        "                shared_output = self.fc_shared(x).squeeze(2)\n",
        "                reduced_output = self.fc_reduce(shared_output)\n",
        "                pre_output = self.sigmoid(reduced_output)\n",
        "                return pre_output\n",
        "            elif layer_name == \"final\":\n",
        "                x = x.view(x.size(0), -1, 62)\n",
        "                shared_output = self.fc_shared(x).squeeze(2)\n",
        "                reduced_output = self.fc_reduce(shared_output)\n",
        "                if with_sigmoid:\n",
        "                    pre_output = self.sigmoid(reduced_output) # if with sigmoid\n",
        "                    output = self.fc_end(pre_output)\n",
        "                else:\n",
        "                    output = reduced_output # if without sigmoid\n",
        "                return output\n",
        "            else:\n",
        "                raise ValueError(\"Invalid layer name\")\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "    if redistribute:\n",
        "        # change weight distribution of componnets in a group\n",
        "        root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "        path_ = os.path.join(root_path, cell_name, cell_name + 'green_Axon.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "        axons = mat_data['Axons']\n",
        "        # Squeeze the outer array\n",
        "        axons = np.squeeze(axons, axis=0)\n",
        "        for i in range(len(axons)):\n",
        "            # Squeeze the inner array and convert the data type to 'int'\n",
        "            axons[i] = np.squeeze(axons[i].astype(int), axis=0)\n",
        "        valid_com_index_from_one_list = np.array(valid_com_index_list) + 1\n",
        "        # Convert axons to index form thereby consistent with the index of remained components\n",
        "        index_form_axons = copy.deepcopy(axons)\n",
        "        for i, axon in enumerate(axons):\n",
        "            for j, bouton in enumerate(axon):\n",
        "                index_form_axons[i][j] = np.where(valid_com_index_from_one_list == bouton)[0]\n",
        "        selected_coms = index_form_axons[0]\n",
        "        if strong_or_weak_redistribute:\n",
        "            num_ = 30\n",
        "        else:\n",
        "            num_ = 10\n",
        "        for i in range(num_):\n",
        "            index_1 = random.choice(selected_coms)\n",
        "            index_2 = random.choice(selected_coms)\n",
        "            temp_w = model.fc_reduce.weight.data[0,index_1]\n",
        "            model.fc_reduce.weight.data[0,index_1] = model.fc_reduce.weight.data[0,index_2]\n",
        "            model.fc_reduce.weight.data[0,index_2] = temp_w\n",
        "    \n",
        "    inputs_list = []\n",
        "    outputs_list = []\n",
        "    labels_list = []\n",
        "    outputs_of_shared_list = []\n",
        "    outputs_of_reduce_list = []\n",
        "    outputs_of_sigmoid_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs_of_shared = model.get_layer_output(inputs, \"shared\")\n",
        "            outputs_of_shared_list.append(outputs_of_shared.numpy())\n",
        "            outputs_of_reduce = model.get_layer_output(inputs, \"reduce\")\n",
        "            outputs_of_reduce_list.append(outputs_of_reduce.numpy())\n",
        "            if with_sigmoid:\n",
        "                outputs_of_sigmoid = model.get_layer_output(inputs, \"sigmoid\")\n",
        "                outputs_of_sigmoid_list.append(outputs_of_sigmoid.numpy())\n",
        "            inputs_list.append(inputs.numpy())\n",
        "            outputs_list.append(outputs.numpy())\n",
        "            labels_list.append(targets.numpy())\n",
        "    print(\"----\")\n",
        "    print(f\"inputs shape: {inputs_list[0].shape}\")\n",
        "    print(f\"outputs_of_shared shape: {outputs_of_shared_list[0].shape}\")\n",
        "    print(f\"outputs_of_reduce shape: {outputs_of_reduce_list[0].shape}\")\n",
        "    if with_sigmoid:\n",
        "        print(f\"outputs_of_sigmoid shape: {outputs_of_sigmoid_list[0].shape}\")\n",
        "    print(f\"outputs shape: {outputs_list[0].shape}\")\n",
        "    print(f\"labels shape: {labels_list[0].shape}\")\n",
        "    print(\"----\")\n",
        "\n",
        "    \n",
        "    # plot some input samples with indicating their reduce layer outputs\n",
        "    n_sample = 5\n",
        "    inputs = inputs_list[0][:n_sample]\n",
        "    upper_b_inputs = np.ceil(np.max(inputs) * 10) / 10\n",
        "    lower_b_inputs = np.floor(np.min(inputs) * 10) / 10\n",
        "    outputs_of_shared = outputs_of_shared_list[0][:n_sample]\n",
        "    upper_b_shared = np.ceil(np.max(outputs_of_shared) * 10) / 10\n",
        "    lower_b_shared = np.floor(np.min(outputs_of_shared) * 10) / 10\n",
        "    outputs_of_reduce = outputs_of_reduce_list[0][:n_sample]\n",
        "    if with_sigmoid:\n",
        "        outputs_of_sigmoid = outputs_of_sigmoid_list[0][:n_sample]\n",
        "    outputs = outputs_list[0][:n_sample]\n",
        "    labels = labels_list[0][:n_sample]\n",
        "    print(inputs.shape, outputs_of_shared.shape, outputs_of_reduce.shape)\n",
        "    for i in range(n_sample):\n",
        "        input_, share_out_, reduce_out_ = inputs[i], outputs_of_shared[i], outputs_of_reduce[i]\n",
        "        if with_sigmoid:\n",
        "            sigmoid_out_ = outputs_of_sigmoid[i]\n",
        "        output_, label_ = outputs[i], labels[i]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6.4*1.5, 4.8*1.5))\n",
        "        cax = ax.imshow(input_, cmap='cool', vmin=lower_b_inputs, vmax=upper_b_inputs)\n",
        "        colorbar = fig.colorbar(cax)\n",
        "        colorbar.set_label('Colorbar Label', fontsize=14)\n",
        "        ax.tick_params(axis='x', labelsize=14)\n",
        "        ax.tick_params(axis='y', labelsize=14)\n",
        "        plt.xlabel('Time frames', fontsize=16)\n",
        "        plt.ylabel('Components', fontsize=16)\n",
        "        ax.set_title(f\"Input Value Map\\n(output: {output_[0]:.5f}, label: {label_:.5f})\\n({signal_trace_type})\", fontsize=18)\n",
        "        plt.show()\n",
        "\n",
        "        share_out_ = share_out_[:, np.newaxis]\n",
        "        #  Horizontally concatenate the array to make it bold in figures\n",
        "        share_out_ = np.hstack([share_out_, share_out_, share_out_, share_out_, share_out_])\n",
        "        fig, ax = plt.subplots(figsize=(6.4*1.5, 4.8*1.5))\n",
        "        cax = ax.imshow(share_out_, cmap='cool', vmin=lower_b_shared, vmax=upper_b_shared)\n",
        "        colorbar = fig.colorbar(cax)\n",
        "        colorbar.set_label('Colorbar Label', fontsize=14)\n",
        "        ax.tick_params(axis='y', labelsize=14)\n",
        "        plt.ylabel('Components', fontsize=16)\n",
        "        # Hide x-axis ticks and labels\n",
        "        ax.set_xticks([])\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_title(f\"Shared Layer Output Value Map\\n(output: {output_[0]:.5f}, label: {label_:.5f})\\n({signal_trace_type})\", fontsize=18)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Output of the reduce layer is {reduce_out_[0]:.5f}\")\n",
        "        if with_sigmoid:\n",
        "            print(f\"Output of the sigmoid is {sigmoid_out_[0]:.5f}\")\n",
        "        print((f\"Output of the final layer is {output_[0]:.5f}\"))\n",
        "        print((f\"Label is {label_:.5f}\"))\n",
        "\n",
        "\n",
        "    outputs_array = np.concatenate(outputs_list)\n",
        "    labels_array = np.concatenate(labels_list)\n",
        "    outputs_of_reduce_array = np.concatenate(outputs_of_reduce_list)\n",
        "\n",
        "    if with_sigmoid:\n",
        "        # plot the values of reduce layer outputs (before feeding into sigmoid)\n",
        "        plt.figure(figsize=(6.4, 4.8))\n",
        "        plt.scatter(range(outputs_of_reduce_array.shape[0]), outputs_of_reduce_array, color = 'salmon')\n",
        "        plt.xlabel('Sample Index', fontsize=16)\n",
        "        plt.ylabel('Value', fontsize=16)\n",
        "        plt.title(f\"Outputs of reduce layer (before feeding into sigmoid)\\n({signal_trace_type})\", fontsize=18)\n",
        "        plt.show()\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        x_ = np.linspace(-5, 5, 100)\n",
        "        y_ = sigmoid(x_)\n",
        "        plt.figure(figsize=(6.4, 4.8))\n",
        "        plt.plot(x_, y_, color = 'skyblue', label='Sigmoid Function', linewidth=2)\n",
        "        plt.scatter(outputs_of_reduce_array, np.zeros_like(outputs_of_reduce_array), label='Inputs to Sigmoid', color = 'salmon')\n",
        "        plt.xlabel('Input Value', fontsize=16)\n",
        "        plt.ylabel('Output Value', fontsize=16)\n",
        "        plt.title(f\"Outputs of reduce layer on Sigmoid\\n({signal_trace_type})\", fontsize=18)\n",
        "        plt.legend(fontsize=14, facecolor='none')\n",
        "        plt.show()\n",
        "    else:\n",
        "        # plot the values of reduce layer outputs (final outputs)\n",
        "        plt.figure(figsize=(6.4, 4.8))\n",
        "        plt.scatter(range(outputs_of_reduce_array.shape[0]), outputs_of_reduce_array, color = 'salmon')\n",
        "        plt.xlabel('Sample Index', fontsize=16)\n",
        "        plt.ylabel('Value', fontsize=16)\n",
        "        plt.title(f\"Outputs of reduce layer (final outputs)\\n({signal_trace_type})\", fontsize=18)\n",
        "        plt.show()\n",
        "\n",
        "    # Sort labels and corresponding outputs\n",
        "    sorted_indices = np.argsort(labels_array)\n",
        "    sorted_labels = labels_array[sorted_indices]\n",
        "    sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(sorted_labels, color = 'skyblue', label='True Labels', linewidth=2)\n",
        "    plt.plot(sorted_outputs, color = 'salmon', label='Model Outputs', linewidth=2)\n",
        "    plt.xlabel('Sample Index', fontsize=16)\n",
        "    plt.ylabel('Normalized Value', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Comparison between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "    plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## below is wrong original code\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "#                           \"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# # signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "# #                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# # signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# # signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# # signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# for signal_trace_type in signal_trace_type_list:\n",
        "#     if signal_trace_type == \"Raw Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "#     elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "#     elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "#     model = FluoModel(data_set.shape[1])\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "#     model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "#     outputs_list = []\n",
        "#     labels_list = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in test_loader:\n",
        "#             outputs = model(inputs)\n",
        "#             outputs_list.append(outputs.numpy())\n",
        "#             labels_list.append(targets.numpy())\n",
        "\n",
        "#     outputs_array = np.concatenate(outputs_list)\n",
        "#     labels_array = np.concatenate(labels_list)\n",
        "\n",
        "#     # Sort labels and corresponding outputs\n",
        "#     sorted_indices = np.argsort(labels_array)\n",
        "#     sorted_labels = labels_array[sorted_indices]\n",
        "#     sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "#     # Calculate the correlation coefficient\n",
        "#     correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "#     # plot results\n",
        "#     plt.figure(figsize=(6.4, 4.8))\n",
        "#     plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "#     plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "#     plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "#     plt.tick_params(labelsize=14)\n",
        "#     plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "#     plt.legend(fontsize=14, facecolor='none')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkVCDSqEilb7"
      },
      "source": [
        "### Load model weights and see weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O-rwApW45-n"
      },
      "source": [
        "#### Plots with respect to absolute weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mlbhtQ0Aiu7Y",
        "outputId": "ef111fdf-3d61-4f93-8d2f-b0f2da0d7018"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\", \"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_Axon.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "axons = mat_data['Axons']\n",
        "# Squeeze the outer array\n",
        "axons = np.squeeze(axons, axis=0)\n",
        "for i in range(len(axons)):\n",
        "    # Squeeze the inner array and convert the data type to 'int'\n",
        "    axons[i] = np.squeeze(axons[i].astype(int), axis=0)\n",
        "\n",
        "valid_com_index_from_one_list = np.array(valid_com_index_list) + 1\n",
        "\n",
        "# Convert axons to index form thereby consistent with the index of remained components\n",
        "index_form_axons = copy.deepcopy(axons)\n",
        "for i, axon in enumerate(axons):\n",
        "    for j, bouton in enumerate(axon):\n",
        "        index_form_axons[i][j] = np.where(valid_com_index_from_one_list == bouton)[0]\n",
        "flat_index_form_axons = np.concatenate(index_form_axons)\n",
        "\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    state_dict = torch.load(model_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # access the weights for each layer\n",
        "    fc_shared_weights = model.fc_shared.weight\n",
        "    fc_reduce_weights = model.fc_reduce.weight\n",
        "    fc_end_weights = model.fc_end.weight\n",
        "\n",
        "    # access the biases if needed\n",
        "    fc_shared_biases = model.fc_shared.bias\n",
        "    fc_reduce_biases = model.fc_reduce.bias\n",
        "    fc_end_biases = model.fc_end.bias\n",
        "\n",
        "    print(f\"{signal_trace_type}\")\n",
        "\n",
        "    # Weights\n",
        "    print(\"\\nfc_shared weights:\")\n",
        "    print(fc_shared_weights)\n",
        "\n",
        "    print(\"\\nfc_reduce weights:\")\n",
        "    print(fc_reduce_weights)\n",
        "\n",
        "    print(\"\\nfc_end weights (if without sigmoid, the end layer is not used, then its weight and bias are random):\")\n",
        "    print(fc_end_weights)\n",
        "\n",
        "    # Biases\n",
        "    print(\"\\nfc_shared biases:\")\n",
        "    print(fc_shared_biases)\n",
        "\n",
        "    print(\"\\nfc_reduce biases:\")\n",
        "    print(fc_reduce_biases)\n",
        "\n",
        "    print(\"\\nfc_end biases (if without sigmoid, the end layer is not used, then its weight and bias are random):\")\n",
        "    print(fc_end_biases)\n",
        "\n",
        "    # Access the weights\n",
        "    fc_shared_weights = fc_shared_weights.detach().numpy().flatten()\n",
        "    fc_reduce_weights = fc_reduce_weights.detach().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (weights abs)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Plot fc_shared_weights\n",
        "    ax1.bar(range(1,len(fc_shared_weights)+1), np.abs(fc_shared_weights), color='skyblue')\n",
        "    ax1.set_title(f'fc_shared_weights, corresponding to frames\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Frame', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    half_len = int(len(fc_shared_weights)/2)\n",
        "    print(f\"The mean the first {half_len} weight absolute values of fc_shared_weights is {np.mean(np.abs(fc_shared_weights)[:half_len])}\")\n",
        "    print(f\"The mean the last {half_len} weight absolute values of fc_shared_weights is {np.mean(np.abs(fc_shared_weights)[half_len:])}\")\n",
        "\n",
        "\n",
        "    # Create subplots (1 y axis)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(range(1,len(fc_reduce_weights)+1), np.abs(fc_reduce_weights), color='salmon')\n",
        "    ax1.set_title(f'fc_reduce_weights, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights abs and distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, np.abs(fc_reduce_weights), color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Distance, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, valid_dis_list, color='skyblue', label='Distance to Soma')\n",
        "    ax2.set_ylabel('Distance to Soma', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_dis_list = np.array(valid_dis_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(sorted_valid_dis_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, sorted_valid_dis_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('Distance to Soma (Pixels)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and Distance to Soma\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and 1/distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_dis_list = np.array(valid_dis_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(1/sorted_valid_dis_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, 1/sorted_valid_dis_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('1/Distance to Soma (Pixels)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and 1/Distance to Soma\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights abs and distance, grouped)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Reorder fc_reduce_weights and valid_dis_list based on group_indices\n",
        "    reordered_fc_reduce_weights = [fc_reduce_weights[idx] for idx in flat_index_form_axons]\n",
        "    reordered_valid_dis_list = [valid_dis_list[idx] for idx in flat_index_form_axons]\n",
        "\n",
        "    # Create x-labels for groups\n",
        "    x_labels = [f\"Group {i+1}\" for i in range(len(index_form_axons))]\n",
        "\n",
        "    # Bar plot for fc_reduce_weights\n",
        "    x = np.arange(1, len(flat_index_form_axons) + 1)\n",
        "    index__ = 0\n",
        "    for axon in index_form_axons:\n",
        "        index__ = index__ + len(axon)\n",
        "        if index__ < len(flat_index_form_axons):\n",
        "            x[index__:] = x[index__:] + 5 # generate gap space on x axis between groups\n",
        "\n",
        "    break_indices = np.where(np.diff(x) != 1)[0]\n",
        "    x_labels_center = [(x[start] + x[end]) / 2 for start, end in zip(np.insert(break_indices + 1, 0, 0), break_indices)]\n",
        "    x_labels_center.append((x[break_indices[-1]+1] + x[-1])/2)\n",
        "\n",
        "    ax1.bar(x, np.abs(reordered_fc_reduce_weights), color='salmon', label='Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Distance, corresponding to components, grouped\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Groups', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.set_xticks(x_labels_center)\n",
        "    ax1.set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve (valid_dis_list)\n",
        "    ax2.plot(x, reordered_valid_dis_list, color='skyblue', label='Distance to Soma')\n",
        "    ax2.set_ylabel('Distance to Soma', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for both curves\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights abs and size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, np.abs(fc_reduce_weights), color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Size, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, valid_size_list, color='skyblue', label='Size/Pixels')\n",
        "    ax2.set_ylabel('Size/Pixels', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_size_list = np.array(valid_size_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(sorted_valid_size_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, sorted_valid_size_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('Size (Pixel Number)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and Size\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and 1/size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_size_list = np.array(valid_size_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(1/sorted_valid_size_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, 1/sorted_valid_size_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('1/Size (Pixel Number)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and 1/Size\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weight abs and size, grouped)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Reorder fc_reduce_weights and valid_dis_list based on group_indices\n",
        "    reordered_fc_reduce_weights = [fc_reduce_weights[idx] for idx in flat_index_form_axons]\n",
        "    reordered_valid_size_list = [valid_size_list[idx] for idx in flat_index_form_axons]\n",
        "\n",
        "    # Create x-labels for groups\n",
        "    x_labels = [f\"Group {i+1}\" for i in range(len(index_form_axons))]\n",
        "\n",
        "    # Bar plot for fc_reduce_weights\n",
        "    x = np.arange(1, len(flat_index_form_axons) + 1)\n",
        "    index__ = 0\n",
        "    for axon in index_form_axons:\n",
        "        index__ = index__ + len(axon)\n",
        "        if index__ < len(flat_index_form_axons):\n",
        "            x[index__:] = x[index__:] + 5\n",
        "\n",
        "    break_indices = np.where(np.diff(x) != 1)[0]\n",
        "    x_labels_center = [(x[start] + x[end]) / 2 for start, end in zip(np.insert(break_indices + 1, 0, 0), break_indices)]\n",
        "    x_labels_center.append((x[break_indices[-1]+1] + x[-1])/2)\n",
        "\n",
        "    ax1.bar(x, np.abs(reordered_fc_reduce_weights), color='salmon', label='Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Size, corresponding to components, grouped\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Groups', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.set_xticks(x_labels_center)\n",
        "    ax1.set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve (valid_dis_list)\n",
        "    ax2.plot(x, reordered_valid_size_list, color='skyblue', label='Size/Pixels')\n",
        "    ax2.set_ylabel('Size/Pixels', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for both curves\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"----- ----- -----\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yz-WHOH41VZ"
      },
      "source": [
        "#### Plots with respect to original weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xlMaxQfX4zIP",
        "outputId": "b4965604-9502-4d33-dba0-7edd31469893"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\", \"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_Axon.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "axons = mat_data['Axons']\n",
        "# Squeeze the outer array\n",
        "axons = np.squeeze(axons, axis=0)\n",
        "for i in range(len(axons)):\n",
        "    # Squeeze the inner array and convert the data type to 'int'\n",
        "    axons[i] = np.squeeze(axons[i].astype(int), axis=0)\n",
        "\n",
        "valid_com_index_from_one_list = np.array(valid_com_index_list) + 1\n",
        "\n",
        "# Convert axons to index form thereby consistent with the index of remained components\n",
        "index_form_axons = copy.deepcopy(axons)\n",
        "for i, axon in enumerate(axons):\n",
        "    for j, bouton in enumerate(axon):\n",
        "        index_form_axons[i][j] = np.where(valid_com_index_from_one_list == bouton)[0]\n",
        "flat_index_form_axons = np.concatenate(index_form_axons)\n",
        "\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    state_dict = torch.load(model_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # access the weights for each layer\n",
        "    fc_shared_weights = model.fc_shared.weight\n",
        "    fc_reduce_weights = model.fc_reduce.weight\n",
        "    fc_end_weights = model.fc_end.weight\n",
        "\n",
        "    # access the biases if needed\n",
        "    fc_shared_biases = model.fc_shared.bias\n",
        "    fc_reduce_biases = model.fc_reduce.bias\n",
        "    fc_end_biases = model.fc_end.bias\n",
        "\n",
        "    print(f\"{signal_trace_type}\")\n",
        "\n",
        "    # Weights\n",
        "    print(\"\\nfc_shared weights:\")\n",
        "    print(fc_shared_weights)\n",
        "\n",
        "    print(\"\\nfc_reduce weights:\")\n",
        "    print(fc_reduce_weights)\n",
        "\n",
        "    print(\"\\nfc_end weights (if without sigmoid, the end layer is not used, then its weight and bias are random):\")\n",
        "    print(fc_end_weights)\n",
        "\n",
        "    # Biases\n",
        "    print(\"\\nfc_shared biases:\")\n",
        "    print(fc_shared_biases)\n",
        "\n",
        "    print(\"\\nfc_reduce biases:\")\n",
        "    print(fc_reduce_biases)\n",
        "\n",
        "    print(\"\\nfc_end biases (if without sigmoid, the end layer is not used, then its weight and bias are random):\")\n",
        "    print(fc_end_biases)\n",
        "\n",
        "    # Access the weights\n",
        "    fc_shared_weights = fc_shared_weights.detach().numpy().flatten()\n",
        "    fc_reduce_weights = fc_reduce_weights.detach().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (weights)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Plot fc_shared_weights\n",
        "    ax1.bar(range(1,len(fc_shared_weights)+1), fc_shared_weights, color='skyblue')\n",
        "    ax1.set_title(f'fc_shared_weights, corresponding to frames\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Frame', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    half_len = int(len(fc_shared_weights)/2)\n",
        "    print(f\"The mean the first {half_len} weight values of fc_shared_weights is {np.mean(fc_shared_weights[:half_len])}\")\n",
        "    print(f\"The mean the last {half_len} weight values of fc_shared_weights is {np.mean(fc_shared_weights[half_len:])}\")\n",
        "\n",
        "\n",
        "    # Create subplots (1 y axis)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(range(1,len(fc_reduce_weights)+1), fc_reduce_weights, color='salmon')\n",
        "    ax1.set_title(f'fc_reduce_weights, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights and significant components)\n",
        "    ##### if having the data of significant components, here is for cell CL090_230515\n",
        "    sig_components = [\n",
        "    2, 5, 7, 15, 17, 19, 25, 28, 30, 36, 37, 38, 44, 49, 52, 59, 60, 62, 63, 67, 69, 71, 74, 76, 78, 80, 82, 86, 87, 88, 94, 99, 100, 103,\n",
        "    109, 112, 113, 115, 116, 117, 122, 133, 134, 136, 137, 140, 142, 147, 150, 151, 152, 155, 156, 160, 161, 166, 169, 171, 172, 176, 177,\n",
        "    181, 182, 186, 189, 190, 192, 194, 195, 197, 198, 199, 200, 201, 202, 203, 205, 209, 210, 213, 214, 216, 222, 223, 224, 225, 226, 229, 234,\n",
        "    236, 237, 238, 240, 242, 244, 250, 252, 254, 255, 256, 257, 258, 260, 261, 262, 268, 270, 271, 275, 276, 277, 278, 281\n",
        "    ]\n",
        "    sig_components = np.array(sig_components)\n",
        "    sig_components_index = sig_components - 1\n",
        "    sig_components_index = list(sig_components_index)\n",
        "\n",
        "    intersection_indices = [valid_com_index_list.index(item) for item in valid_com_index_list if item in sig_components_index]\n",
        "    whether_sig_array = np.zeros_like(fc_reduce_weights)\n",
        "    whether_sig_array[intersection_indices] = 1\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, fc_reduce_weights, color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and whether significant, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, whether_sig_array, color='skyblue', label='Whether significant')\n",
        "    ax2.set_ylabel('Significant or not', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights and distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, fc_reduce_weights, color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Distance, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, valid_dis_list, color='skyblue', label='Distance to Soma')\n",
        "    ax2.set_ylabel('Distance to Soma', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights and F value std)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(fc_reduce_weights)\n",
        "    sorted_valid_F_std_component = np.array(F_std_component[valid_com_index_list])[sorted_indices]\n",
        "    sorted_fc_reduce_weights = fc_reduce_weights[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_fc_reduce_weights, np.transpose(sorted_valid_F_std_component))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_fc_reduce_weights, sorted_valid_F_std_component, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('F Std of Components', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and F Std of Components\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights and distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(fc_reduce_weights)\n",
        "    sorted_valid_dis_list = np.array(valid_dis_list)[sorted_indices]\n",
        "    sorted_fc_reduce_weights = fc_reduce_weights[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_fc_reduce_weights, np.transpose(sorted_valid_dis_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_fc_reduce_weights, sorted_valid_dis_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('Distance to Soma (Pixels)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and Distance to Soma\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights and 1/distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(fc_reduce_weights)\n",
        "    sorted_valid_dis_list = np.array(valid_dis_list)[sorted_indices]\n",
        "    sorted_fc_reduce_weights = fc_reduce_weights[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_fc_reduce_weights, np.transpose(1/sorted_valid_dis_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_fc_reduce_weights, 1/sorted_valid_dis_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('1/Distance to Soma (Pixels)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and 1/Distance to Soma\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights and distance, grouped)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Reorder fc_reduce_weights and valid_dis_list based on group_indices\n",
        "    reordered_fc_reduce_weights = [fc_reduce_weights[idx] for idx in flat_index_form_axons]\n",
        "    reordered_valid_dis_list = [valid_dis_list[idx] for idx in flat_index_form_axons]\n",
        "\n",
        "    # Create x-labels for groups\n",
        "    x_labels = [f\"Group {i+1}\" for i in range(len(index_form_axons))]\n",
        "\n",
        "    # Bar plot for fc_reduce_weights\n",
        "    x = np.arange(1, len(flat_index_form_axons) + 1)\n",
        "    index__ = 0\n",
        "    for axon in index_form_axons:\n",
        "        index__ = index__ + len(axon)\n",
        "        if index__ < len(flat_index_form_axons):\n",
        "            x[index__:] = x[index__:] + 5 # generate gap space on x axis between groups\n",
        "\n",
        "    break_indices = np.where(np.diff(x) != 1)[0]\n",
        "    x_labels_center = [(x[start] + x[end]) / 2 for start, end in zip(np.insert(break_indices + 1, 0, 0), break_indices)]\n",
        "    x_labels_center.append((x[break_indices[-1]+1] + x[-1])/2)\n",
        "\n",
        "    ax1.bar(x, reordered_fc_reduce_weights, color='salmon', label='Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Distance, corresponding to components, grouped\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Groups', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.set_xticks(x_labels_center)\n",
        "    ax1.set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve (valid_dis_list)\n",
        "    ax2.plot(x, reordered_valid_dis_list, color='skyblue', label='Distance to Soma')\n",
        "    ax2.set_ylabel('Distance to Soma', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for both curves\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights and size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, fc_reduce_weights, color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Size, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, valid_size_list, color='skyblue', label='Size/Pixels')\n",
        "    ax2.set_ylabel('Size/Pixels', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights and size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(fc_reduce_weights)\n",
        "    sorted_valid_size_list = np.array(valid_size_list)[sorted_indices]\n",
        "    sorted_fc_reduce_weights = fc_reduce_weights[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_fc_reduce_weights, np.transpose(sorted_valid_size_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_fc_reduce_weights, sorted_valid_size_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('Size (Pixel Number)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and Size\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights and 1/size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(fc_reduce_weights)\n",
        "    sorted_valid_size_list = np.array(valid_size_list)[sorted_indices]\n",
        "    sorted_fc_reduce_weights = fc_reduce_weights[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_fc_reduce_weights, np.transpose(1/sorted_valid_size_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_fc_reduce_weights, 1/sorted_valid_size_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('1/Size (Pixel Number)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and 1/Size\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weight and size, grouped)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Reorder fc_reduce_weights and valid_dis_list based on group_indices\n",
        "    reordered_fc_reduce_weights = [fc_reduce_weights[idx] for idx in flat_index_form_axons]\n",
        "    reordered_valid_size_list = [valid_size_list[idx] for idx in flat_index_form_axons]\n",
        "\n",
        "    # Create x-labels for groups\n",
        "    x_labels = [f\"Group {i+1}\" for i in range(len(index_form_axons))]\n",
        "\n",
        "    # Bar plot for fc_reduce_weights\n",
        "    x = np.arange(1, len(flat_index_form_axons) + 1)\n",
        "    index__ = 0\n",
        "    for axon in index_form_axons:\n",
        "        index__ = index__ + len(axon)\n",
        "        if index__ < len(flat_index_form_axons):\n",
        "            x[index__:] = x[index__:] + 5\n",
        "\n",
        "    break_indices = np.where(np.diff(x) != 1)[0]\n",
        "    x_labels_center = [(x[start] + x[end]) / 2 for start, end in zip(np.insert(break_indices + 1, 0, 0), break_indices)]\n",
        "    x_labels_center.append((x[break_indices[-1]+1] + x[-1])/2)\n",
        "\n",
        "    ax1.bar(x, reordered_fc_reduce_weights, color='salmon', label='Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Size, corresponding to components, grouped\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Groups', fontsize=16)\n",
        "    ax1.set_ylabel('Weight Value', fontsize=16)\n",
        "    ax1.set_xticks(x_labels_center)\n",
        "    ax1.set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve (valid_dis_list)\n",
        "    ax2.plot(x, reordered_valid_size_list, color='skyblue', label='Size/Pixels')\n",
        "    ax2.set_ylabel('Size/Pixels', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for both curves\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"----- ----- -----\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibLvBpRo259d"
      },
      "source": [
        "#### Plot weights with component locations (colorbar map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VU-uXiHh1bYK",
        "outputId": "8eeff3fa-4b1d-4abe-e281-307a092d8c7b"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\", \"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "root_path = \"/home/chenhuimiao/dLGN_Neuron_Modeling/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_BoutonMasks.mat')\n",
        "data_ = h5py.File(path_, 'r') # Open the MATLAB v7.3 file\n",
        "green_bouton_masks = copy.deepcopy(np.array(data_['BoutonMasks']))\n",
        "green_bouton_masks = np.transpose(green_bouton_masks, (2, 1, 0))\n",
        "data_.close()\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    state_dict = torch.load(model_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    fc_reduce_weights = model.fc_reduce.weight\n",
        "\n",
        "    print(f\"{signal_trace_type}\")\n",
        "\n",
        "    print(\"\\nfc_reduce weights:\")\n",
        "    print(fc_reduce_weights)\n",
        "\n",
        "    fc_reduce_weights = fc_reduce_weights.detach().numpy().flatten()\n",
        "\n",
        "    weights_map = copy.deepcopy(green_bouton_masks)\n",
        "\n",
        "    for k, index_ in enumerate(valid_com_index_list):\n",
        "        for i in range(weights_map.shape[0]):\n",
        "            for j in range(weights_map.shape[1]):\n",
        "                if weights_map[i, j, index_] == 1:\n",
        "                    weights_map[i, j, index_] = fc_reduce_weights[k]\n",
        "\n",
        "    weights_map = np.sum(weights_map[:,:,valid_com_index_list], axis=2)\n",
        "    fig, ax = plt.subplots(figsize=(6.4*1.5, 4.8*1.5))\n",
        "    cax = ax.imshow(weights_map, cmap='cool')\n",
        "    colorbar = fig.colorbar(cax)\n",
        "    colorbar.set_label('Colorbar Label', fontsize=14)\n",
        "    ax.tick_params(axis='x', labelsize=14)\n",
        "    ax.tick_params(axis='y', labelsize=14)\n",
        "    ax.set_title(f\"Weights Map\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
