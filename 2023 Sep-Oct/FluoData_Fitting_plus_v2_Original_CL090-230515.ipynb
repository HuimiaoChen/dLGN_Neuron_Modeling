{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MSUnx8YS7-c"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwji5fbG9dW0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "import copy\n",
        "import sys\n",
        "import random\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7ZV2-UgcfYq"
      },
      "source": [
        "## Cell name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYv-qRvWa64R"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL075_230228'\n",
        "# run_num = ['1', '2', '3']\n",
        "\n",
        "# cell_name = 'CL079_230324'\n",
        "# run_num = ['1', '2', '3']\n",
        "\n",
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smIBw_NiiKd1"
      },
      "source": [
        "## Read fluorescence structure data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UaGOO7A0oxq"
      },
      "outputs": [],
      "source": [
        "# # The code below is used to see whether a trial's post 2s is same as another trial's pre 2s.\n",
        "# # However, no finding that a trial's post 2s is same as another trial's pre 2s, because the\n",
        "# # post 2s periods are processed to make them more close to the baseline.\n",
        "\n",
        "# # Here, use dFF data. Changing mat_data['Master_dFF'] to mat_data['Master_f'] to check F data.\n",
        "\n",
        "# root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "\n",
        "# i = 1\n",
        "# path_ = os.path.join(root_path, cell_name, cell_name + 'red_dFFStructuresrun' + run_num[i] + '.mat')\n",
        "# mat_data = scipy.io.loadmat(path_)\n",
        "# fluo_runx = mat_data['Master_dFF']\n",
        "\n",
        "# conca_runx = np.empty((0,fluo_runx[0,0].shape[2]))\n",
        "# index_record = []\n",
        "# action = True\n",
        "# while action:\n",
        "#     action = False\n",
        "#     for index, value in np.ndenumerate(fluo_dFF_runx):\n",
        "#         value = np.squeeze(value)\n",
        "#         # print(value.shape)\n",
        "#         # print(value[-31:, :].shape)\n",
        "#         if conca_runx.shape[0] == 0:\n",
        "#             conca_runx = np.concatenate((conca_runx, value), axis=0)\n",
        "#             index_record = index_record + [index]\n",
        "#             action = True\n",
        "#             break\n",
        "#         if np.array_equal(conca_runx[:31, :], value[-31:, :]):\n",
        "#             conca_runx = np.concatenate((value[:-31, :], conca_runx), axis=0)\n",
        "#             index_record = [index] + index_record\n",
        "#             action = True\n",
        "#             break\n",
        "#         if np.array_equal(conca_runx[-31:, :], value[:31, :]):\n",
        "#             fluo_dFconca_dFF_runxF_runx = np.concatenate((conca_runx, value[31:, :]), axis=0)\n",
        "#             index_record = index_record + [index]\n",
        "#             action = True\n",
        "#             break\n",
        "# print(conca_runx.shape)\n",
        "# # a shape (93, 10) means no found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmYcXKtyCGrc"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZMtLb2riQGM"
      },
      "outputs": [],
      "source": [
        "def read_conca_fluo_data(cell_name = 'CL090_230515',\n",
        "             run_num = ['4', '5', '6'],\n",
        "             color = 'red',\n",
        "             datatype = 'F',\n",
        "             root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    This fuction is used to concatenate dFF or F data from different pieces of\n",
        "    red/green structure data.\n",
        "    color = 'red' or 'green'\n",
        "    type = 'F' or 'dFF'\n",
        "    if len(run_num) is 3, the output is <class 'numpy.ndarray'> with shape (3, 48),\n",
        "    and its elements are <class 'numpy.ndarray'> with shape (93, 1, 10) for red\n",
        "    and shape (93, 281, 10) for green (281 is the number of components, 10 is\n",
        "    the number of repeats and 93 is 6 seconds; the 2p imaging frequency is\n",
        "    15.63 Hz so roughly 93 frames for 6 seconds).\n",
        "    '''\n",
        "\n",
        "    conca_fluo_data = np.empty((0, 48))\n",
        "\n",
        "    for i in range(len(run_num)):\n",
        "        path_ = os.path.join(root_path, cell_name, cell_name + color + '_dFFStructuresrun' + run_num[i] + '.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "        if datatype == 'dFF':\n",
        "            fluo_data_runx = mat_data['Master_dFF']\n",
        "        elif datatype == 'F':\n",
        "            fluo_data_runx = mat_data['Master_f']\n",
        "        fluo_data_runx = fluo_data_runx[:, :-1] # delete the last column (49th)\n",
        "        # print(type(fluo_data_runx))\n",
        "        # print(fluo_data_runx.shape)\n",
        "        # print(fluo_data_runx[0,0].shape)\n",
        "\n",
        "        conca_fluo_data = np.concatenate((conca_fluo_data, fluo_data_runx), axis=0) # like np.vsatck\n",
        "\n",
        "    return conca_fluo_data\n",
        "\n",
        "def plot_all_trials(conca_fluo_data,\n",
        "           cell_name = 'CL090_230515',\n",
        "           run_num = ['4', '5', '6'],\n",
        "           color = 'red',\n",
        "           datatype = 'F',\n",
        "           component = 1):\n",
        "    '''\n",
        "    This function draws the figures based on the conca_fluo_data generated by\n",
        "    the function read_conca_fluo_data.\n",
        "    color = 'red' or 'green'\n",
        "    type = 'F' or 'dFF'\n",
        "    component is for 'green'; it is from 1 to the component number.\n",
        "    if color is 'red', it can be arbitrary value.\n",
        "    '''\n",
        "    runs_string = ' '.join(run_num)\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "\n",
        "    # Create some sample data\n",
        "    x_intervals = np.linspace(0, 6, 94) # Divide [0, 6] into 94 intervals, cuz data length of a trial is 93\n",
        "    x_middle_points = (x_intervals[:-1] + x_intervals[1:]) / 2  # Calculate middle points\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots(nrows=48, ncols=len(run_num), figsize=(5 * len(run_num), 3 * 48), constrained_layout=True)\n",
        "\n",
        "    # Create sf array\n",
        "    sf = np.tile([0.02, 0.08, 0.32], 16)\n",
        "    # Create tf array\n",
        "    tf = np.tile([1, 1, 1, 4, 4, 4], 8)\n",
        "    # Create orientation array\n",
        "    orientation = np.repeat(np.arange(0, 360, 45), 6)\n",
        "\n",
        "    for x in range(48):\n",
        "        for y in range(3):\n",
        "            if color == 'green':\n",
        "                data_patch = conca_fluo_data[y, x][:,component-1,:]\n",
        "            if color == 'red':\n",
        "                data_patch = np.squeeze(conca_fluo_data[y, x])\n",
        "            for i in range(data_patch.shape[1]):\n",
        "                ax[x, y].plot(x_middle_points, data_patch[:, i])\n",
        "\n",
        "            # Set axis labels and title\n",
        "            ax[x, y].set_xlabel('Time (seconds)', fontsize=16)\n",
        "            ax[x, y].set_ylabel(datatype, fontsize=16)\n",
        "            # ax.set_title('dFF for a trial', fontsize=18)\n",
        "\n",
        "            # Set tick parameters\n",
        "            ax[x, y].tick_params(labelsize=16)  # Adjust tick size as needed\n",
        "\n",
        "            ax[x, y].axvspan(2, 4, facecolor='gray', alpha=0.2)\n",
        "\n",
        "            if y == 0:\n",
        "                x_data = ax[x, y].get_xticks()\n",
        "                y_data = ax[x, y].get_yticks()\n",
        "                x_data_min = np.min(x_data)\n",
        "                x_data_max = np.max(x_data)\n",
        "                y_data_min = np.min(y_data)\n",
        "                y_data_max = np.max(y_data)\n",
        "                x_text = x_data_min - 0.3 * (x_data_max - x_data_min)\n",
        "                y_text = 0.5 * (y_data_max + y_data_min)\n",
        "\n",
        "        # Write the group information to the right of each row\n",
        "        ax[x, 0].text(x_text, y_text, f'Ori: {orientation[x]}Â° \\nTF: {tf[x]} \\nSF: {sf[x]}', fontsize=14, va='center')\n",
        "    if color == 'red':\n",
        "        fig.suptitle(f'All Conditions All Rounds All Repests {color_name} {datatype} Data ({cell_name}) (Columns: run {runs_string})', fontsize=16)\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}.pdf'\n",
        "    if color == 'green':\n",
        "        fig.suptitle(f'All Conditions All Rounds All Repests {color_name} {datatype} Data ({cell_name} Component {component}) (Columns: run {runs_string})', fontsize=16)\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_{component:03d}.pdf'\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktxOqxhZMTnD"
      },
      "outputs": [],
      "source": [
        "# color = 'green'\n",
        "# datatype = 'F'\n",
        "# component = 1\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "# plot_all_trials(conca_fluo_data, cell_name, run_num, color, datatype, component)\n",
        "\n",
        "# print(type(conca_fluo_data))\n",
        "# print(conca_fluo_data.shape)\n",
        "# print(type(conca_fluo_data[1,1]))\n",
        "# print(conca_fluo_data[1,1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "453bKUZO3XGN"
      },
      "source": [
        "### Read red and green data then plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF_ye-UJNzX7"
      },
      "outputs": [],
      "source": [
        "# cell_name_list = ['CL075_230228', 'CL079_230324' 'CL090_230515']\n",
        "# run_num_list = [['1', '2', '3'], ['1', '2', '3'], ['4', '5', '6']]\n",
        "\n",
        "# cell_name_list = ['CL090_230515']\n",
        "# run_num_list = [['4', '5', '6']]\n",
        "\n",
        "# for cell_name_, run_num_ in zip(cell_name_list, run_num_list):\n",
        "#     for color_ in ['red', 'green']:\n",
        "#         for datatype_ in ['F', 'dFF']:\n",
        "#             conca_fluo_data_ = read_conca_fluo_data(cell_name_, run_num_, color_, datatype_)\n",
        "#             if color_ == 'red':\n",
        "#                 plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_)\n",
        "#             if color_ == 'green':\n",
        "#                 for component_ in range(1, conca_fluo_data_[0,0].shape[1]+1):\n",
        "#                     plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_, component_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB5rBXAuQwrm"
      },
      "outputs": [],
      "source": [
        "# batch download the plotted figures\n",
        "# uncomment the code below to download figures if needed\n",
        "\n",
        "# import glob\n",
        "\n",
        "# folder_path = '.'\n",
        "# # file_prefix = 'All_Conditions_All_Rounds_All_Repeats_'\n",
        "# file_prefix = 'CL'\n",
        "\n",
        "# # Use glob to find all files with the given prefix in the folder\n",
        "# matching_files = glob.glob(f\"{folder_path}/{file_prefix}*\")\n",
        "# # print(matching_files)\n",
        "# # # Print the matching file names\n",
        "# # for file_path in matching_files:\n",
        "# #     print(file_path)\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# zip_filename = 'files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     # Add files to the zip file\n",
        "#     for file_path in matching_files:\n",
        "#         zipf.write(file_path)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDDpPN0GmUEf"
      },
      "source": [
        "## Extract the chronological order of randomized conditions and the sequences; extract the singal traces and tran and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxtjWYNA1YML"
      },
      "source": [
        "### Extract the chronological order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA7kYXzomUY-"
      },
      "outputs": [],
      "source": [
        "def extract_chronological_order(cell_name = 'CL090_230515',\n",
        "                 run_num = ['4', '5', '6'],\n",
        "                 root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    This function uses the frame_2p_metadata_run data to extract the the chronological\n",
        "    order of randomized conditions.\n",
        "    Finally, it returns\n",
        "    ori_3d, tf_3d, sf_3d, all are with shape (3, 10, 48) if the length of rum_num is 3,\n",
        "    3 is the number of runs (the length of run_num); 10 is the number of repeats in each\n",
        "    run; 48 is the number of conditions.\n",
        "    The elements in \"48\" dimension show the ori/tf/sf parameters in chronological order\n",
        "    for a certain run and repeat.\n",
        "    '''\n",
        "\n",
        "    ori_3d = np.empty((len(run_num),10,48))\n",
        "    tf_3d = np.empty((len(run_num),10,48))\n",
        "    sf_3d = np.empty((len(run_num),10,48))\n",
        "\n",
        "    for i in range(len(run_num)):\n",
        "        path_ = os.path.join(root_path, cell_name, 'frame_2p_metadata_run' + run_num[i] + '.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "        stim = mat_data['stim']\n",
        "        # print(type(stim))\n",
        "        # print(stim.shape)\n",
        "        # print(type(stim[0,0]))\n",
        "        # print(stim[0,0].dtype.names)\n",
        "        # print(type(stim['frame']))\n",
        "        # print(stim['frame'].shape)\n",
        "        # print(type(stim['frame'][0,0]))\n",
        "        # print(stim['frame'][0,0].dtype.names)\n",
        "\n",
        "        ori = np.squeeze(stim['frame'][0,0]['orientation'][0,0])\n",
        "        tf = np.squeeze(stim['frame'][0,0]['temporal_frequencies_hz'][0,0])\n",
        "        sf = np.squeeze(stim['frame'][0,0]['sf'][0,0])\n",
        "        # print(type(ori))\n",
        "        # print(type(tf))\n",
        "        # print(type(sf))\n",
        "        # print(ori.shape)\n",
        "        # print(tf.shape)\n",
        "        # print(sf.shape)\n",
        "        # print(len(np.nonzero(ori)[0]))\n",
        "        # print(len(np.nonzero(tf)[0]))\n",
        "        # print(len(np.nonzero(sf)[0]))\n",
        "\n",
        "        indices = np.nonzero(tf)[0] # cannot use nonzero in ori, because it can be 0 degree\n",
        "\n",
        "        unique_indices = [indices[0]]\n",
        "        last_number = indices[0]\n",
        "        for num in indices:\n",
        "            if num - last_number > 1:\n",
        "                unique_indices.append(num)\n",
        "            last_number = num\n",
        "        # the resulting array where continuous numbers are deleted, and only the first one is retained\n",
        "        # i.e., [1,2,3,21,22,23,24,33,34,35,36,37,48,49] -> [1,21,33,48]\n",
        "\n",
        "        # print(len(unique_indices))\n",
        "        ori = ori[unique_indices].reshape(10,48)\n",
        "        tf = tf[unique_indices].reshape(10,48)\n",
        "        sf = sf[unique_indices].reshape(10,48)\n",
        "        # print(ori.shape)\n",
        "        # print(tf.shape)\n",
        "        # print(sf.shape)\n",
        "\n",
        "        ori_3d[i,:,:] = ori\n",
        "        tf_3d[i,:,:] = tf\n",
        "        sf_3d[i,:,:] = sf\n",
        "\n",
        "    # print(ori_3d.shape)\n",
        "    # print(tf_3d.shape)\n",
        "    # print(sf_3d.shape)\n",
        "\n",
        "    return ori_3d, tf_3d, sf_3d\n",
        "\n",
        "\n",
        "def get_condition_order(ori_3d, tf_3d, sf_3d):\n",
        "    '''\n",
        "    This function uses ori_3d, tf_3d, sf_3d, which are generated by the function\n",
        "    extract_chronological_order to calculate condition_order, where the column\n",
        "    number represents the condition and they are sorted in chronological order.\n",
        "\n",
        "    The conditions along the columns in structure data are given in a fixed order,\n",
        "    following the order of vectors: orientation, tf, sf (created as follows). But they\n",
        "    are temporally randomized. That is why we use this function to get the chronological\n",
        "    order of the columns (conditions).\n",
        "\n",
        "    The returned condition_order is with shape (3,10,48). For the dimension \"48\", 48 elements\n",
        "    are column indexes in structure data, like [2, 0, 23, 24, ...], meaning in structure data\n",
        "    column 2 occurs first, then column 0, ...\n",
        "    '''\n",
        "\n",
        "    # Create orientation array\n",
        "    orientation = np.repeat(np.arange(0, 360, 45), 6)\n",
        "    # Create tf array\n",
        "    tf = np.tile([1, 1, 1, 4, 4, 4], 8)\n",
        "    # Create sf array\n",
        "    sf = np.tile([0.02, 0.08, 0.32], 16)\n",
        "\n",
        "    condition_order = np.empty((3,10,48)) # the column number in chronological order\n",
        "\n",
        "    condition_in_column_order = [np.array([x,y,z]) for x,y,z in zip(orientation, tf, sf)]\n",
        "\n",
        "    for i in range(3):\n",
        "        for j in range(10):\n",
        "            conditions_in_time_order = [np.array([x,y,z]) for x,y,z in zip(ori_3d[i,j,:], tf_3d[i,j,:], sf_3d[i,j,:])]\n",
        "            for ii, ele1 in enumerate(conditions_in_time_order):\n",
        "                is_found = False\n",
        "                for jj, ele2 in enumerate(condition_in_column_order):\n",
        "                    if ele1[0] == ele2[0] and ele1[1] == ele2[1] and ele1[2] == ele2[2]:\n",
        "                        condition_order[i,j,ii] = jj\n",
        "                        is_found = True\n",
        "                if not is_found:\n",
        "                    print(\"An Element Not Found!!!\")\n",
        "\n",
        "    return condition_order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygFaNQkpx_mC"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "\n",
        "# ori_3d, tf_3d, sf_3d = extract_chronological_order(cell_name, run_num)\n",
        "# condition_order = get_condition_order(ori_3d, tf_3d, sf_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqji3zVj1eRk"
      },
      "source": [
        "### Get red/green time sequence (only for 48 conditions, in chronological oerder but not continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBJ_4y7Yhlx7"
      },
      "source": [
        "Note: the sequences got by the following function get_time_sequence are in chronological order **but not continuous** (**so they may not be helpful or used in future algorithms**), because something between them are omitted, like there are gray screen visual stimuli trials are not in the 48 columns but inserted between the 48 trials in the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi5nyUOEAw1F"
      },
      "outputs": [],
      "source": [
        "def get_time_sequence(conca_fluo_data, condition_order):\n",
        "    '''\n",
        "    This function uses conca_fluo_data and condition_order to get\n",
        "    the time sequence results for red and green data.\n",
        "    If red, return time_sequence with shape (3, 10, 1, 2976), where\n",
        "    3 is how many runs, 10 is how many repeats each run, 1 is how many\n",
        "    components (for red, only soma, it is 1), 2976=62*48 (62 points\n",
        "    are 2s, 48 are the number of conditions).\n",
        "    If green, return time_sequence with shape (3, 10, 281, 2976),\n",
        "    where 281 is how many components, which depends on the cell.\n",
        "    '''\n",
        "\n",
        "    how_many_run = conca_fluo_data.shape[0]\n",
        "    how_many_repeat = conca_fluo_data[0,0].shape[2]\n",
        "    how_many_component = conca_fluo_data[0,0].shape[1] # will be 1 if red\n",
        "\n",
        "    time_sequence = np.empty((how_many_run, how_many_repeat, how_many_component, 62*48))\n",
        "\n",
        "    for i in range(how_many_run):\n",
        "        for j in range(how_many_repeat):\n",
        "            for k in range(how_many_component):\n",
        "                for index, z in enumerate(condition_order[i,j,:]):\n",
        "                    time_sequence[i, j, k, 62*index:62*(index+1)] = conca_fluo_data[i, int(z)][31:, k, j]\n",
        "\n",
        "    return time_sequence\n",
        "\n",
        "\n",
        "def plot_time_sequence_each_repeat(time_sequence,\n",
        "            cell_name = 'CL090_230515',\n",
        "            run_num = ['4', '5', '6'],\n",
        "            color = 'red',\n",
        "            datatype = 'F',\n",
        "            run_index = 1,\n",
        "            repeat = 1,\n",
        "            component = 1):\n",
        "    '''\n",
        "    This function plots the curve of a certain repeat.\n",
        "    Return the data of that repeat.\n",
        "\n",
        "    run_index, repeat, and component all start from 1, not 0.\n",
        "    run_index = 1, ..., len(run_num)\n",
        "    repeat = 1, .., repeat number\n",
        "    component = 1, .., component number\n",
        "    For red, component can only be 1.\n",
        "    '''\n",
        "\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "    run = int(run_num[run_index-1])\n",
        "    data = time_sequence[run_index-1, repeat-1, component-1, :]\n",
        "    # we should plot all the data of 1 run, that is 10 repeats, cuz they are a sequence.\n",
        "\n",
        "    time_steps = 62 * 48 # time_sequence.shape[3]\n",
        "    time_interval = 192  # seconds, 192 s = 4 s/trial * 48 trails, trial is condition\n",
        "\n",
        "    time_values = [t * time_interval / time_steps for t in range(time_steps)]\n",
        "\n",
        "    plt.figure(figsize=(18, 6))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.plot(time_values, data, color=color, linewidth=1)\n",
        "\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xlabel('Time (s)', fontsize=16)\n",
        "    plt.ylabel(f'{datatype}', fontsize=16)\n",
        "    if color == 'red':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} Repeat {repeat} in Chronological Order', fontsize=18)\n",
        "    if color == 'green':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} Repeat {repeat} Component {component} in Chronological Order', fontsize=18)\n",
        "\n",
        "    # Add shaded rectangles for stimuli\n",
        "    for i in range(0, time_interval, 4):\n",
        "        plt.axvspan(i, i + 2, facecolor='gray', alpha=0.2)\n",
        "\n",
        "    if color == 'red':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_Run{run}_Repeat{repeat:02d}_TimeSequence.pdf'\n",
        "    if color == 'green':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_{component:03d}_Run{run}_Repeat{repeat:02d}_TimeSequence.pdf'\n",
        "\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_time_sequence_each_run(time_sequence,\n",
        "            cell_name = 'CL090_230515',\n",
        "            run_num = ['4', '5', '6'],\n",
        "            color = 'red',\n",
        "            datatype = 'F',\n",
        "            run_index = 1,\n",
        "            component = 1):\n",
        "    '''\n",
        "    This function plots the curve of a certain run.\n",
        "    Why plot all the data of 1 run, that is 10 repeats?\n",
        "    Because they are a sequence, i.e., they are done temporally\n",
        "    continuously.\n",
        "    Return the data of that run.\n",
        "\n",
        "    run_index and component both start from 1, not 0.\n",
        "    run_index = 1, ..., len(run_num)\n",
        "    component = 1, .., component number\n",
        "    For red, component can only be 1.\n",
        "    '''\n",
        "\n",
        "    color_name = 'Red' if color == 'red' else 'Green'\n",
        "    run = int(run_num[run_index-1])\n",
        "    data = time_sequence[run_index-1, :, component-1, :]\n",
        "    data = data.reshape(time_sequence.shape[1] * time_sequence.shape[3])\n",
        "\n",
        "    time_steps = 62 * 48 * 10 # 62 * 48 is time_sequence.shape[3], 10 is time_sequence.shape[1]\n",
        "    time_interval = 192 * 10  # seconds, 192 s = 4 s/trial * 48 trails, trial is condition\n",
        "\n",
        "    time_values = [t * time_interval / time_steps for t in range(time_steps)]\n",
        "\n",
        "    plt.figure(figsize=(100, 6))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.plot(time_values, data, color=color, linewidth=1)\n",
        "\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.xlabel('Time (s)', fontsize=16)\n",
        "    plt.ylabel(f'{datatype}', fontsize=16)\n",
        "    if color == 'red':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} in Chronological Order', fontsize=18)\n",
        "    if color == 'green':\n",
        "        plt.title(f'{cell_name} {color_name} {datatype} Run {run} Component {component} in Chronological Order', fontsize=18)\n",
        "\n",
        "    # Add shaded rectangles for stimuli\n",
        "    for i in range(0, time_interval, 4):\n",
        "        plt.axvspan(i, i + 2, facecolor='gray', alpha=0.2)\n",
        "\n",
        "    filename = f'{cell_name}_{color_name}_{datatype}_Run{run}_TimeSequence.pdf'\n",
        "    if color == 'red':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_Run{run}_TimeSequence.pdf'\n",
        "    if color == 'green':\n",
        "        filename = f'{cell_name}_{color_name}_{datatype}_{component:03d}_Run{run}_TimeSequence.pdf'\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyeL1x5LBA-P"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "# color = 'green'\n",
        "# datatype = 'F'\n",
        "\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "\n",
        "# ori_3d, tf_3d, sf_3d = extract_chronological_order(cell_name, run_num)\n",
        "# condition_order = get_condition_order(ori_3d, tf_3d, sf_3d)\n",
        "\n",
        "# time_sequence = get_time_sequence(conca_fluo_data, condition_order)\n",
        "\n",
        "# # run_index = 1\n",
        "# # repeat = 1\n",
        "# # component = 2\n",
        "\n",
        "# # plot_time_sequence_each_repeat(time_sequence,\n",
        "# #             cell_name,\n",
        "# #             run_num,\n",
        "# #             color,\n",
        "# #             datatype,\n",
        "# #             run_index = run_index,\n",
        "# #             repeat = repeat,\n",
        "# #             component = component);\n",
        "\n",
        "# # plot_time_sequence_each_run(time_sequence,\n",
        "# #             cell_name,\n",
        "# #             run_num,\n",
        "# #             color,\n",
        "# #             datatype,\n",
        "# #             run_index = run_index,\n",
        "# #             component = component);\n",
        "\n",
        "# print(conca_fluo_data.shape)\n",
        "# print(conca_fluo_data[0,0].shape)\n",
        "# print(condition_order.shape)\n",
        "# print(time_sequence.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVzhX1d2-nYX"
      },
      "source": [
        "### Get valid components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl3FurKE-qkd"
      },
      "outputs": [],
      "source": [
        "def get_valid_components(cell_name, max_pixels = 100, soma_index = 0,\n",
        "                         root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    this function gets the correct components from the all component (too big ones are invalid, criteria is max_pexels).\n",
        "    corresponding size list and distance list (distance to soma) are obtained.\n",
        "    soma is get from red bouton marks using soma_index (there should be only 1 but there are actually some invalid ones),\n",
        "    so we need soma_index.\n",
        "    '''\n",
        "\n",
        "    # Load red BoutonMasks\n",
        "    path_ = os.path.join(root_path, cell_name, cell_name + 'red_BoutonMasks.mat')\n",
        "    data_ = h5py.File(path_, 'r') # Open the MATLAB v7.3 file\n",
        "    red_bouton_masks = copy.deepcopy(np.array(data_['BoutonMasks']))\n",
        "    if len(red_bouton_masks.shape) == 3:\n",
        "        red_bouton_masks = np.transpose(red_bouton_masks, (2, 1, 0))\n",
        "    elif len(red_bouton_masks.shape) == 2:\n",
        "        red_bouton_masks = np.transpose(red_bouton_masks)\n",
        "    else:\n",
        "        print(\"Unexpected red_bouton_masks shape!!!\")\n",
        "    data_.close()\n",
        "\n",
        "    if len(red_bouton_masks.shape) == 3:\n",
        "        A = red_bouton_masks[:, :, soma_index]\n",
        "    elif len(red_bouton_masks.shape) == 2:\n",
        "        A = red_bouton_masks\n",
        "    else:\n",
        "        print(\"Unexpected red_bouton_masks shape!!!\")\n",
        "\n",
        "    row_indices, col_indices = np.where(A == 1)\n",
        "    soma_average_row = np.mean(row_indices)\n",
        "    soma_average_col = np.mean(col_indices)\n",
        "\n",
        "    # Load green BoutonMasks\n",
        "    path_ = os.path.join(root_path, cell_name, cell_name + 'green_BoutonMasks.mat')\n",
        "    data_ = h5py.File(path_, 'r') # Open the MATLAB v7.3 file\n",
        "    green_bouton_masks = copy.deepcopy(np.array(data_['BoutonMasks']))\n",
        "    green_bouton_masks = np.transpose(green_bouton_masks, (2, 1, 0))\n",
        "    data_.close()\n",
        "\n",
        "    n_com = green_bouton_masks.shape[2]\n",
        "    # print(green_bouton_masks.shape)\n",
        "    valid_com_index_list = []\n",
        "    valid_size_list = []\n",
        "    valid_dis_list = []\n",
        "\n",
        "    for i in range(n_com):\n",
        "        A = green_bouton_masks[:, :, i]\n",
        "        row_indices, col_indices = np.where(A == 1)\n",
        "        average_row = np.mean(row_indices)\n",
        "        average_col = np.mean(col_indices)\n",
        "        if len(row_indices) < max_pixels:\n",
        "            valid_com_index_list.append(i)\n",
        "\n",
        "            distance = np.sqrt((average_row - soma_average_row)**2 + (average_col - soma_average_col)**2)\n",
        "            valid_dis_list.append(distance)\n",
        "\n",
        "            number_of_ones = np.sum(A == 1)\n",
        "            valid_size_list.append(number_of_ones)\n",
        "\n",
        "    return valid_com_index_list, valid_dis_list, valid_size_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2wscaO_LLh7"
      },
      "outputs": [],
      "source": [
        "# valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components('CL075_230228', 100, 5)\n",
        "# print(valid_com_index_list)\n",
        "# print(valid_dis_list)\n",
        "# print(valid_size_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3CTt0_nifWC"
      },
      "source": [
        "### Get the signal traces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpcJGaLcV0nz"
      },
      "source": [
        "#### Read signal traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq-YZOA4glBE"
      },
      "outputs": [],
      "source": [
        "def read_signal_traces(cell_name = 'CL090_230515',\n",
        "           run_num = ['4', '5', '6'],\n",
        "           color = 'red',\n",
        "           root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"):\n",
        "    '''\n",
        "    This fuction is used to signal traces data of which the length is the 2p lengh\n",
        "    (number of 2p imgaing frames, which divided by 2p imaging frequency, 15.63 Hz, then\n",
        "    the duration of 2p imaging).\n",
        "    color = 'red' or 'green'\n",
        "    Acquired data is the raw fluorescence data, i.e., F value.\n",
        "    If len(run_num) is 3, for color 'green', the output is <class 'numpy.ndarray'> with\n",
        "    shape (3, N, M); for color 'red', the output is <class 'numpy.ndarray'> with shape\n",
        "    (3, 1, M). N is the number of components and M is 2p length (number of 2p imaging\n",
        "    frames).\n",
        "    '''\n",
        "\n",
        "    path_ = os.path.join(root_path, cell_name, cell_name + color + '_SignalTracesrun' + run_num[0] + '.mat')\n",
        "    mat_data = scipy.io.loadmat(path_)\n",
        "    singal_traces = np.empty((len(run_num), mat_data['SignalTraces']['BoutonTraces'][0,0].shape[1],\n",
        "                  mat_data['SignalTraces']['BoutonTraces'][0,0].shape[0]))\n",
        "    # print(singal_traces.shape)\n",
        "\n",
        "    for i in range(len(run_num)):\n",
        "        path_ = os.path.join(root_path, cell_name, cell_name + color + '_SignalTracesrun' + run_num[i] + '.mat')\n",
        "        mat_data = scipy.io.loadmat(path_)\n",
        "\n",
        "        # print(mat_data['SignalTraces'].shape)\n",
        "        # print(mat_data['SignalTraces'][0,0].shape)\n",
        "        # print(mat_data['SignalTraces'][0,0]['BoutonTraces'].shape)\n",
        "        # print(mat_data['SignalTraces'][0,0]['BoutonTraces'][0,0].shape)\n",
        "        # print(mat_data['SignalTraces']['BoutonTraces'].shape)\n",
        "        # print(mat_data['SignalTraces']['BoutonTraces'][0,0].shape)\n",
        "        # # mat_data['SignalTraces'][0,0]['BoutonTraces'][0,0] and mat_data['SignalTraces']['BoutonTraces'][0,0] are same.\n",
        "\n",
        "        singal_traces[i, :, :] = mat_data['SignalTraces']['BoutonTraces'][0,0].T\n",
        "\n",
        "    return singal_traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT2DDHSdf64k"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "# color = 'green'\n",
        "\n",
        "# signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "\n",
        "# print(signal_traces.shape)\n",
        "# # print(signal_traces[0,0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntcg9SgZAMAb"
      },
      "source": [
        "#### Check whether structure data is in signal traces data and get the locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcrBsSBDEPy1"
      },
      "outputs": [],
      "source": [
        "def get_condition_piece_locations(conca_fluo_data, signal_traces):\n",
        "    '''\n",
        "    This function gets the locations of conca_fluo_data (from structure data) in\n",
        "    signal_traces.\n",
        "    conca_fluo_data is the data of all conditions; the data is extracted from\n",
        "    signal_traces.\n",
        "    return locations with shape (a,b,c,d) (e.g., (3, 281, 10, 48)) and\n",
        "    its element's shape is (2,3).\n",
        "    a is how_many_run\n",
        "    b is how_many_component # will be 1 if red\n",
        "    c is how_many_repeat\n",
        "    d is how_many_condition\n",
        "    (2,3) are the data piece's start and end index in signal_traces.\n",
        "    here we assume the data piece is 93 points, corresponding to 6s.\n",
        "    '''\n",
        "\n",
        "    how_many_run = conca_fluo_data.shape[0]\n",
        "    how_many_repeat = conca_fluo_data[0,0].shape[2]\n",
        "    how_many_component = conca_fluo_data[0,0].shape[1] # will be 1 if red\n",
        "    how_many_condition = conca_fluo_data.shape[1]\n",
        "\n",
        "    locations = np.empty((how_many_run, how_many_component, how_many_repeat, how_many_condition), dtype=object)\n",
        "\n",
        "    # get the position of elements from conca_fluo_data in signal_traces\n",
        "    for run_index in range(how_many_run):\n",
        "        for component_index in range(how_many_component):\n",
        "            for repeat_index in range(how_many_repeat):\n",
        "                for condition_index in range(how_many_condition):\n",
        "                    for i, ele in enumerate(conca_fluo_data[run_index, condition_index][:, component_index, repeat_index]):\n",
        "                        if i == 0:\n",
        "                            # print(np.isin(ele, signal_traces))\n",
        "                            index_start = np.where(signal_traces[run_index, component_index, :] == ele)[0]\n",
        "                            # print(np.where(signal_traces[run_index, component_index, :] == ele))\n",
        "                            # if np.where(signal_traces[run_index, component_index, :] == ele)[0].shape[0] > 1:\n",
        "                            #     print(ele)\n",
        "                        if i == 46: # if only i == 0 and i == 92, there are still multiple outputs due to conincidence\n",
        "                            index_middle = np.where(signal_traces[run_index, component_index, :] == ele)[0]\n",
        "                        if i == 92:\n",
        "                            # print(np.isin(ele, signal_traces))\n",
        "                            index_end = np.where(signal_traces[run_index, component_index, :] == ele)[0]\n",
        "                    isfound = 0\n",
        "                    for index1_ in index_start:\n",
        "                        for index2_ in index_middle:\n",
        "                            for index3_ in index_end:\n",
        "                                if index3_ - index1_ == 92 and index2_ - index1_ == 46:\n",
        "                                    isfound = isfound + 1\n",
        "                                    locations[run_index, component_index, repeat_index, condition_index] = np.array([[run_index, component_index, index1_],\n",
        "                                                                              [run_index, component_index, index3_]])\n",
        "                                    # message = (\n",
        "                                    #       f\"condition {condition_index} -- start position: \"\n",
        "                                    #       f\"({run_index}, {component_index}, {index1_}); \"\n",
        "                                    #       f\"end position: ({run_index}, {component_index}, {index2_})\"\n",
        "                                    #       )\n",
        "                                    # print(message)\n",
        "                    if isfound != 1:\n",
        "                        print(f\"isfound is {isfound}\")\n",
        "                        print(f\"run {run_index} component {component_index} repeat {repeat_index} condition {condition_index} Not Found or Multiple Found\")\n",
        "            # print(f\"--- --- run {run_index} component {component_index} finished --- ---\")\n",
        "\n",
        "    return locations\n",
        "\n",
        "## below is the original code for finding location, which has been wrapped in the above function\n",
        "# run_index = 2\n",
        "# repeat = 9\n",
        "# component = 279\n",
        "# condition_index = 0\n",
        "\n",
        "# empty_array = np.empty((3, 4))\n",
        "\n",
        "# # get the position of elements from conca_fluo_data in signal_traces\n",
        "# for condition_index in range(48):\n",
        "#     for i, ele in enumerate(conca_fluo_data[run_index, condition_index][:, component, repeat]):\n",
        "#         if i == 0:\n",
        "#             # print(np.isin(ele, signal_traces))\n",
        "#             index1 = np.where(signal_traces[run_index, component, :] == ele)[0]\n",
        "#             # print(np.where(signal_traces[run_index, component, :] == ele))\n",
        "#             # if np.where(signal_traces[run_index, component, :] == ele)[0].shape[0] > 1:\n",
        "#             #     print(ele)\n",
        "#         if i == 92:\n",
        "#             # print(np.isin(ele, signal_traces))\n",
        "#             index2 = np.where(signal_traces[run_index, component, :] == ele)[0]\n",
        "#     isfound = False\n",
        "#     for index1_ in index1:\n",
        "#         for index2_ in index2:\n",
        "#             if index2_ - index1_ == 92:\n",
        "#                 isfound = True\n",
        "#                 print(f\"condition {condition_index} -- start position: ({run_index},{component},{index1_}); end position: ({run_index},{component},{index2_})\")\n",
        "#     if not isfound:\n",
        "#         print(\"Not Found\")\n",
        "#     print(\"======\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9EUOh58w706"
      },
      "outputs": [],
      "source": [
        "# cell_name = 'CL090_230515'\n",
        "# run_num = ['4', '5', '6']\n",
        "# color = 'red'\n",
        "# datatype = 'F'\n",
        "\n",
        "# conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "# ori_3d, tf_3d, sf_3d = extract_chronological_order(cell_name, run_num)\n",
        "# condition_order = get_condition_order(ori_3d, tf_3d, sf_3d)\n",
        "# time_sequence = get_time_sequence(conca_fluo_data, condition_order)\n",
        "\n",
        "# signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "\n",
        "# print(conca_fluo_data.shape)\n",
        "# print(conca_fluo_data[0,0].shape)\n",
        "# print(condition_order.shape)\n",
        "# print(time_sequence.shape)\n",
        "# print(signal_traces.shape)\n",
        "# print(\"-- --- --\")\n",
        "\n",
        "# locations = get_condition_piece_locations(conca_fluo_data, signal_traces)\n",
        "# print(locations.shape)\n",
        "# print(locations[0,0,0,2])\n",
        "# print(locations[0,0,0,2].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2IMWcWpbuoc"
      },
      "source": [
        "#### Recover conca_fluo_data (structure data) from signal traces and locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLIojlXPb0yD"
      },
      "outputs": [],
      "source": [
        "def recover_strucure_from_traces_and_locs(signal_traces, locations):\n",
        "    '''\n",
        "    This function recovers conca_fluo_data (structure data) from signal traces and locations.\n",
        "\n",
        "    This function is an inverse of function get_condition_piece_locations.\n",
        "    '''\n",
        "    how_many_run, how_many_component, how_many_repeat, how_many_condition = locations.shape\n",
        "\n",
        "    start_point = locations[0, 0, 0, 0][0, 2]\n",
        "    end_point = locations[0, 0, 0, 0][1, 2]\n",
        "    fluo_length_per_condition = end_point - start_point + 1\n",
        "    conca_fluo_data = np.empty((how_many_run, how_many_condition), dtype=object)\n",
        "    for i in range(how_many_run):\n",
        "        for j in range(how_many_condition):\n",
        "            conca_fluo_data[i, j] = np.zeros((fluo_length_per_condition, how_many_component, how_many_repeat))\n",
        "\n",
        "    for run_index in range(how_many_run):\n",
        "        for component_index in range(how_many_component):\n",
        "            for repeat_index in range(how_many_repeat):\n",
        "                for condition_index in range(how_many_condition):\n",
        "                    start_point = locations[run_index, component_index, repeat_index, condition_index][0, 2]\n",
        "                    end_point = locations[run_index, component_index, repeat_index, condition_index][1, 2]\n",
        "                    conca_fluo_data[run_index, condition_index][:, component_index, repeat_index] = signal_traces[run_index, component_index, start_point:end_point+1]\n",
        "\n",
        "    return conca_fluo_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3aCaG9H-32B"
      },
      "source": [
        "#### Visualize and Process the signal traces data (cell_name input here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0bpofEJZrB",
        "outputId": "4f909ab7-1cbf-4a07-a356-d21d8abafe12"
      },
      "outputs": [],
      "source": [
        "cell_name = 'CL090_230515'\n",
        "run_num = ['4', '5', '6']\n",
        "\n",
        "# cell_name = 'CL075_230228'\n",
        "# run_num = ['1', '2', '3']\n",
        "\n",
        "datatype = 'F'\n",
        "\n",
        "color = 'green'\n",
        "\n",
        "green_conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "green_signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "green_locations = get_condition_piece_locations(green_conca_fluo_data, green_signal_traces)\n",
        "print(green_conca_fluo_data.shape)\n",
        "print(green_conca_fluo_data[0,0].shape)\n",
        "print(green_signal_traces.shape)\n",
        "print(green_locations.shape)\n",
        "print(green_locations[0,0,0,0].shape)\n",
        "print(\"------ ------\")\n",
        "\n",
        "color = 'red'\n",
        "\n",
        "red_conca_fluo_data = read_conca_fluo_data(cell_name, run_num, color, datatype)\n",
        "red_signal_traces = read_signal_traces(cell_name, run_num, color)\n",
        "red_locations = get_condition_piece_locations(red_conca_fluo_data, red_signal_traces)\n",
        "print(red_conca_fluo_data.shape)\n",
        "print(red_conca_fluo_data[0,0].shape)\n",
        "print(red_signal_traces.shape)\n",
        "print(red_locations.shape)\n",
        "print(red_locations[0,0,0,0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neraIwlxmb8o"
      },
      "outputs": [],
      "source": [
        "# # check recover_strucure_from_traces_and_locs to see whether we can recover conca_fluo_data\n",
        "# green_conca_fluo_data_ = recover_strucure_from_traces_and_locs(green_signal_traces, green_locations)\n",
        "# red_conca_fluo_data_ = recover_strucure_from_traces_and_locs(red_signal_traces, red_locations)\n",
        "# for conca_fluo_data, conca_fluo_data_ in [(green_conca_fluo_data, green_conca_fluo_data_), (red_conca_fluo_data, red_conca_fluo_data_)]:\n",
        "#     are_equal_red = True\n",
        "#     for i in range(conca_fluo_data.shape[0]):\n",
        "#         for j in range(conca_fluo_data.shape[1]):\n",
        "#             if not np.array_equal(conca_fluo_data[i, j], conca_fluo_data_[i, j]):\n",
        "#                 are_equal_red = False\n",
        "#                 break\n",
        "#     print(are_equal_red)\n",
        "# # the printed outputs are both \"True\" -- already verified!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqg6FsASVG0O"
      },
      "source": [
        "##### Calculate and plot mean each run, std each run, mean each repeat, std each repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gcvkU84F_Um4",
        "outputId": "d871e3f8-f873-4074-faea-57cd0e09ae59"
      },
      "outputs": [],
      "source": [
        "### mean each run\n",
        "\n",
        "print(\"mean of each run of green:\")\n",
        "green_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(green_signal_traces[run_,:,:])\n",
        "    green_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "print(\"mean of each run of red:\")\n",
        "red_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(red_signal_traces[run_,:,:])\n",
        "    red_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_mean_run\n",
        "y2 = red_mean_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8)) # [6.4, 4.8] is deault size, same as fig, ax1 = plt.subplots()\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Runs (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each run\n",
        "\n",
        "print(\"std of each run of green:\")\n",
        "green_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(green_signal_traces[run_,:,:])\n",
        "    green_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "print(\"std of each run of red:\")\n",
        "red_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(red_signal_traces[run_,:,:])\n",
        "    red_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_std_run\n",
        "y2 = red_std_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Runs (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### mean each repeat\n",
        "\n",
        "# print(\"mean of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "green_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# print(\"mean of each repeat of red:\")\n",
        "red_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_mean_repeat\n",
        "y2 = red_mean_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Repeats (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each repeat\n",
        "\n",
        "# print(\"std of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "green_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(green_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# print(\"std of each repeat of red:\")\n",
        "red_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(red_signal_traces[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_std_repeat\n",
        "y2 = red_std_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "    sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "    sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Repeats (Raw)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## for later use in decay restoration\n",
        "green_mean_repeat_list_raw = green_mean_repeat.copy()\n",
        "red_mean_repeat_list_raw = red_mean_repeat.copy()\n",
        "\n",
        "green_mean_run_list_raw = green_mean_run.copy()\n",
        "red_mean_run_list_raw = red_mean_run.copy()\n",
        "\n",
        "green_std_repeat_list_raw = green_std_repeat.copy()\n",
        "red_std_repeat_list_raw = red_std_repeat.copy()\n",
        "\n",
        "green_std_run_list_raw = green_std_run.copy()\n",
        "red_std_run_list_raw = red_std_run.copy()\n",
        "\n",
        "# std is not used cuz after mean restoration we need to recalculate std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U7dVNVLU23G"
      },
      "source": [
        "##### Plot a repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "vGCB9x7T-Eu_",
        "outputId": "3de81292-2734-43dc-dd68-d66f4be709ec"
      },
      "outputs": [],
      "source": [
        "print(green_signal_traces.shape)\n",
        "print(red_signal_traces.shape)\n",
        "\n",
        "## plot a signal trace in a repeat\n",
        "# data_to_plot = green_signal_traces[0, 0, :int(32500/10)]\n",
        "data_to_plot = red_signal_traces[0, 0, 7*int(32500/10):8*int(32500/10)]\n",
        "\n",
        "plt.figure(figsize=(6.4*2, 4.8))\n",
        "\n",
        "plt.plot(data_to_plot, color='salmon', linewidth=0.8, label='Red Fluorescence')\n",
        "plt.xlabel('Frame', fontsize=16)\n",
        "plt.ylabel('F Value', fontsize=16)\n",
        "plt.tick_params(axis='y', labelsize=14)\n",
        "plt.tick_params(axis='x', labelsize=12)\n",
        "\n",
        "# Set title and legend\n",
        "plt.title('Red F Trace in One Repeat', fontsize=18)\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85T1yqQwDMRj"
      },
      "source": [
        "#### Signal trace restoration (Mean+Std restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijw_3rI6KDhC"
      },
      "source": [
        "##### Restore mean and plot mean each run, std each run, mean each repeat, std each repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQaLQEhGXXk8"
      },
      "source": [
        "###### Restore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVTi0E-xXVbV",
        "outputId": "65d6eaec-5027-4531-e13e-c5a0c3dd8974"
      },
      "outputs": [],
      "source": [
        "green_mean_repeat = np.array(green_mean_repeat_list_raw)\n",
        "red_mean_repeat = np.array(red_mean_repeat_list_raw)\n",
        "\n",
        "green_mean_run = np.array(green_mean_run_list_raw)\n",
        "red_mean_run = np.array(red_mean_run_list_raw)\n",
        "\n",
        "# print(green_mean_repeat.shape)\n",
        "# print(red_mean_repeat.shape)\n",
        "# print(green_mean_run.shape)\n",
        "# print(red_mean_run.shape)\n",
        "\n",
        "x_mean_repeat_restored_list = []\n",
        "x_mean_repeat_restored_coefficients_list = []\n",
        "for x_mean_repeat, x_mean_run in [(green_mean_repeat, green_mean_run), (red_mean_repeat, red_mean_run)]:\n",
        "    # Define the exponential function for regression of decay\n",
        "    def exponential_func(x, lambda_):\n",
        "        return np.exp(lambda_ * x)\n",
        "\n",
        "    how_many_run = 3\n",
        "    repeat_num_per_run = int(x_mean_repeat.shape[0]/how_many_run)\n",
        "\n",
        "    x_mean_repeat_restored = x_mean_repeat.copy()\n",
        "    x_mean_repeat_restored_coefficients = x_mean_repeat.copy()\n",
        "    for i in range(how_many_run):\n",
        "        y = x_mean_repeat[i*repeat_num_per_run:(i+1)*repeat_num_per_run].copy()\n",
        "        multiplier = 1 / y[0]\n",
        "        y = y * multiplier\n",
        "        x = np.arange(len(y))\n",
        "        popt, pcov = curve_fit(exponential_func, x, y)\n",
        "        lambda_ = popt[0]\n",
        "        print(f\"exponent in the exp func is {lambda_}\")\n",
        "        y_fit = exponential_func(x, lambda_)\n",
        "\n",
        "        recover_factor = y / y_fit\n",
        "        print(\"recover_factor.shape:\", recover_factor.shape)\n",
        "        # print(recover_factor)\n",
        "\n",
        "        x_mean_repeat_restored[i*repeat_num_per_run:(i+1)*repeat_num_per_run] *= recover_factor * x_mean_run[0] / x_mean_run[i]\n",
        "        x_mean_repeat_restored_coefficients[i*repeat_num_per_run:(i+1)*repeat_num_per_run] = recover_factor * x_mean_run[0] / x_mean_run[i]\n",
        "\n",
        "    x_mean_repeat_restored_list.append(x_mean_repeat_restored)\n",
        "    x_mean_repeat_restored_coefficients_list.append(x_mean_repeat_restored_coefficients)\n",
        "\n",
        "green_mean_repeat_restored, red_mean_repeat_restored = x_mean_repeat_restored_list\n",
        "green_mean_repeat_restored_coefficients, red_mean_repeat_restored_coefficients = x_mean_repeat_restored_coefficients_list\n",
        "\n",
        "## generate green_signal_traces_mean_restored and red_signal_traces_mean_restored\n",
        "green_signal_traces_mean_restored = green_signal_traces.copy()\n",
        "red_signal_traces_mean_restored = red_signal_traces.copy()\n",
        "\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        green_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len] *= green_mean_repeat_restored_coefficients[run_*repeat_num+i]\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        red_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len] *= red_mean_repeat_restored_coefficients[run_*repeat_num+i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYob3i9RXa0c"
      },
      "source": [
        "###### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ofnbD9kcHMjT",
        "outputId": "5ad911b3-b5e5-4cc5-b5a8-52b2a03e1142"
      },
      "outputs": [],
      "source": [
        "### mean each run\n",
        "\n",
        "print(\"mean of each run of green:\")\n",
        "green_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(green_signal_traces_mean_restored[run_,:,:])\n",
        "    green_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "print(\"mean of each run of red:\")\n",
        "red_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(red_signal_traces_mean_restored[run_,:,:])\n",
        "    red_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_mean_run\n",
        "y2 = red_mean_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8)) # [6.4, 4.8] is deault size, same as fig, ax1 = plt.subplots()\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Runs\\n(After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each run\n",
        "\n",
        "print(\"std of each run of green:\")\n",
        "green_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(green_signal_traces_mean_restored[run_,:,:])\n",
        "    green_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "print(\"std of each run of red:\")\n",
        "red_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(red_signal_traces_mean_restored[run_,:,:])\n",
        "    red_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_std_run\n",
        "y2 = red_std_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Runs\\n(After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### mean each repeat\n",
        "\n",
        "# print(\"mean of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored.shape[2]/repeat_num)\n",
        "green_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# print(\"mean of each repeat of red:\")\n",
        "red_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_mean_repeat\n",
        "y2 = red_mean_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Repeats (After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each repeat\n",
        "\n",
        "# print(\"std of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored.shape[2]/repeat_num)\n",
        "green_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(green_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# print(\"std of each repeat of red:\")\n",
        "red_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(red_signal_traces_mean_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_std_repeat\n",
        "y2 = red_std_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Repeats (After Mean Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "## store them to variables with new names\n",
        "## and for later use in decay restoration\n",
        "green_mean_repeat_list_after_mean_restored = green_mean_repeat.copy() # same as green_mean_repeat_restored\n",
        "red_mean_repeat_list_after_mean_restored = red_mean_repeat.copy() # same as red_mean_repeat_restored\n",
        "\n",
        "green_mean_run_list_after_mean_restored = green_mean_run.copy()\n",
        "red_mean_run_list_after_mean_restored = red_mean_run.copy()\n",
        "\n",
        "green_std_repeat_list_after_mean_restored = green_std_repeat.copy()\n",
        "red_std_repeat_list_after_mean_restored = red_std_repeat.copy()\n",
        "\n",
        "green_std_run_list_after_mean_restored = green_std_run.copy()\n",
        "red_std_run_list_after_mean_restored = red_std_run.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGbPuphzKJsK"
      },
      "source": [
        "##### Restore std and plot mean each run, std each run, mean each repeat, std each repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQf-wV2kXdCk"
      },
      "source": [
        "###### Restore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1KJQRHZY3kr"
      },
      "outputs": [],
      "source": [
        "## convert to numpy form\n",
        "green_mean_repeat = np.array(green_mean_repeat_list_after_mean_restored)\n",
        "red_mean_repeat = np.array(red_mean_repeat_list_after_mean_restored)\n",
        "\n",
        "green_mean_run = np.array(green_mean_run_list_after_mean_restored)\n",
        "red_mean_run = np.array(red_mean_run_list_after_mean_restored)\n",
        "\n",
        "green_std_repeat = np.array(green_std_repeat_list_after_mean_restored)\n",
        "red_std_repeat = np.array(red_std_repeat_list_after_mean_restored)\n",
        "\n",
        "green_std_run = np.array(green_std_run_list_after_mean_restored)\n",
        "red_std_run = np.array(red_std_run_list_after_mean_restored)\n",
        "\n",
        "# print(green_std_repeat, red_std_repeat, green_std_run, red_std_run)\n",
        "# print(green_mean_repeat, red_mean_repeat, green_mean_run, red_mean_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc7SLcACX55c",
        "outputId": "57983232-8eba-4d64-d56b-21118233d52b"
      },
      "outputs": [],
      "source": [
        "x_std_repeat_restored_list = []\n",
        "x_std_repeat_restored_coefficients_list = []\n",
        "for x_std_repeat, x_std_run in [(green_std_repeat, green_std_run), (red_std_repeat, red_std_run)]:\n",
        "    # Define the exponential function for regression of decay\n",
        "    def exponential_func(x, lambda_):\n",
        "        return np.exp(lambda_ * x)\n",
        "\n",
        "    how_many_run = 3\n",
        "    repeat_num_per_run = int(x_std_repeat.shape[0]/how_many_run)\n",
        "\n",
        "    x_std_repeat_restored = x_std_repeat.copy()\n",
        "    x_std_repeat_restored_coefficients = x_std_repeat.copy()\n",
        "    for i in range(how_many_run):\n",
        "        y = x_std_repeat[i*repeat_num_per_run:(i+1)*repeat_num_per_run].copy()\n",
        "        multiplier = 1 / y[0]\n",
        "        y = y * multiplier\n",
        "        x = np.arange(len(y))\n",
        "        popt, pcov = curve_fit(exponential_func, x, y)\n",
        "        lambda_ = popt[0]\n",
        "        print(f\"exponent in the exp func is {lambda_}\")\n",
        "        y_fit = exponential_func(x, lambda_)\n",
        "\n",
        "        recover_factor = y / y_fit\n",
        "        print(\"recover_factor.shape:\", recover_factor.shape)\n",
        "        # print(recover_factor)\n",
        "\n",
        "        x_std_repeat_restored[i*repeat_num_per_run:(i+1)*repeat_num_per_run] *= recover_factor * x_std_run[0] / x_std_run[i]\n",
        "        x_std_repeat_restored_coefficients[i*repeat_num_per_run:(i+1)*repeat_num_per_run] = recover_factor * x_std_run[0] / x_std_run[i]\n",
        "\n",
        "    x_std_repeat_restored_list.append(x_std_repeat_restored)\n",
        "    x_std_repeat_restored_coefficients_list.append(x_std_repeat_restored_coefficients)\n",
        "\n",
        "green_std_repeat_restored, red_std_repeat_restored = x_std_repeat_restored_list\n",
        "green_std_repeat_restored_coefficients, red_std_repeat_restored_coefficients = x_std_repeat_restored_coefficients_list\n",
        "\n",
        "## generate red_signal_traces_mean_restored_std_restored and red_signal_traces_mean_restored_std_restored\n",
        "green_signal_traces_mean_restored_std_restored = green_signal_traces_mean_restored.copy()\n",
        "red_signal_traces_mean_restored_std_restored = red_signal_traces_mean_restored.copy()\n",
        "\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces.shape[2]/repeat_num)\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        temp_ = green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] - mean_\n",
        "        temp_ *= green_std_repeat_restored_coefficients[run_*repeat_num+i]\n",
        "        green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] = mean_ + temp_\n",
        "\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        temp_ = red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] - mean_\n",
        "        temp_ *= red_std_repeat_restored_coefficients[run_*repeat_num+i]\n",
        "        red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len] = mean_ + temp_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4fCPvB-Xeuc"
      },
      "source": [
        "###### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tMymkM80VRqP",
        "outputId": "b671933a-d876-4e68-8544-feb4c925b8b8"
      },
      "outputs": [],
      "source": [
        "### mean each run\n",
        "\n",
        "print(\"mean of each run of green:\")\n",
        "green_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(green_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    green_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "print(\"mean of each run of red:\")\n",
        "red_mean_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    mean_ = np.mean(red_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    red_mean_run.append(mean_)\n",
        "    print(f\"run index:{run_} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_mean_run\n",
        "y2 = red_mean_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8)) # [6.4, 4.8] is deault size, same as fig, ax1 = plt.subplots()\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Runs\\n(After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each run\n",
        "\n",
        "print(\"std of each run of green:\")\n",
        "green_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(green_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    green_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "print(\"std of each run of red:\")\n",
        "red_std_run = []\n",
        "for run_ in range(len(run_num)):\n",
        "    std_ = np.std(red_signal_traces_mean_restored_std_restored[run_,:,:])\n",
        "    red_std_run.append(std_)\n",
        "    print(f\"run index:{run_} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(len(run_num))\n",
        "x_custom = [run_ for run_ in run_num]\n",
        "y1 = green_std_run\n",
        "y2 = red_std_run\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(x_custom)\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.6*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.6*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Runs\\n(After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### mean each repeat\n",
        "\n",
        "# print(\"mean of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored_std_restored.shape[2]/repeat_num)\n",
        "green_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# print(\"mean of each repeat of red:\")\n",
        "red_mean_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        mean_ = np.mean(red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_mean_repeat.append(mean_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- mean: {mean_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_mean_repeat\n",
        "y2 = red_mean_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Means in Different Repeats (After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### std each repeat\n",
        "\n",
        "# print(\"std of each repeat of green:\")\n",
        "repeat_num = 10 # 10 repeats per run\n",
        "repeat_len = int(green_signal_traces_mean_restored_std_restored.shape[2]/repeat_num)\n",
        "green_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(green_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        green_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# print(\"std of each repeat of red:\")\n",
        "red_std_repeat = []\n",
        "for run_ in range(len(run_num)):\n",
        "    for i in range(repeat_num):\n",
        "        std_ = np.std(red_signal_traces_mean_restored_std_restored[run_,:,i*repeat_len:(i+1)*repeat_len])\n",
        "        red_std_repeat.append(std_)\n",
        "        # print(f\"run index:{run_} repeat index:{i} -- std: {std_}\")\n",
        "\n",
        "# plot figure\n",
        "x = range(1, repeat_num * len(run_num) + 1)\n",
        "y1 = green_std_repeat\n",
        "y2 = red_std_repeat\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "green_line, = ax1.plot(x, y1, 's-', color='limegreen', label='Green Fluorescence', markersize=14, linewidth=3)\n",
        "ax1.set_xlabel('Runs', fontsize=16)\n",
        "ax1.set_ylabel('Green F', color='limegreen', fontsize=16)\n",
        "ax1.tick_params(axis='y', labelcolor='limegreen', labelsize=14)\n",
        "\n",
        "# set custome x axis labels\n",
        "ax1.set_xticks(x)\n",
        "if len(x) == 30 and len(run_num) == 3: # ste a better xticks for the common case\n",
        "    ax1.set_xticks([1,4,7,10,11,14,17,20,21,24,27,30])\n",
        "ax1.tick_params(axis='x', labelsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "red_line, = ax2.plot(x, y2, 's-', color='salmon', label='Red Fluorescence', markersize=14, linewidth=3)\n",
        "ax2.set_ylabel('Red F', color='salmon', fontsize=16)\n",
        "ax2.tick_params(axis='y', labelcolor='salmon', labelsize=14)\n",
        "\n",
        "# set two y axes with same value for unit length\n",
        "span = max((max(y1) - min(y1), max(y2) - min(y2)))\n",
        "average_1 = (max(y1) + min(y1)) / 2\n",
        "average_2 = (max(y2) + min(y2)) / 2\n",
        "min_y1, max_y1 = average_1 - 0.6*span, average_1 + 0.8*span\n",
        "min_y2, max_y2 = average_2 - 0.6*span, average_2 + 0.8*span\n",
        "ax1.set_ylim(min_y1, max_y1)\n",
        "ax2.set_ylim(min_y2, max_y2)\n",
        "# make ticks sparse\n",
        "min_y1_, max_y1_ = np.ceil(min_y1 / 100) * 100, np.floor(max_y1 / 100) * 100\n",
        "min_y2_, max_y2_ = np.ceil(min_y2 / 100) * 100, np.floor(max_y2 / 100) * 100\n",
        "# here use ceil for min and floor for ceil is to gurantee that the range used in\n",
        "# set_yticks is within the range of set_ylim, otherwise a new range of the axis\n",
        "# will consider both set_yticks and set_ylim and give the union of them.\n",
        "unit_1 = np.ceil((max_y1_ - min_y1_) / 3 / 100) * 100 # 3 means only 3+1 ticks\n",
        "unit_2 = np.ceil((max_y2_ - min_y2_) / 3 / 100) * 100\n",
        "unit_unified = max(unit_1, unit_2)\n",
        "if min_y1_ != max_y1_:\n",
        "    if np.arange(min_y1_, max_y1_, step=unit_unified)[-1] > max(y1):\n",
        "        sparse_y_ticks_1 = np.arange(min_y1_, max_y1_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_1 = np.append(np.arange(min_y1_, max_y1_, step=unit_unified), max_y1_)\n",
        "else:\n",
        "    sparse_y_ticks_1 = np.array([int(min(y1)), int(max(y1))])\n",
        "if min_y2_ != max_y2_:\n",
        "    if np.arange(min_y2_, max_y2_, step=unit_unified)[-1] > max(y2):\n",
        "        sparse_y_ticks_2 = np.arange(min_y2_, max_y2_, step=unit_unified)\n",
        "    else:\n",
        "        sparse_y_ticks_2 = np.append(np.arange(min_y2_, max_y2_, step=unit_unified), max_y2_)\n",
        "else:\n",
        "    sparse_y_ticks_2 = np.array([int(min(y2)), int(max(y2))])\n",
        "ax1.set_yticks(sparse_y_ticks_1)\n",
        "ax2.set_yticks(sparse_y_ticks_2)\n",
        "\n",
        "# add separating dash lines and add run names\n",
        "for i, run_ in enumerate(run_num):\n",
        "    if i < len(run_num) - 1:\n",
        "        ax1.axvline(x=repeat_num*(i+1)+0.5, color='gray', linestyle='--', linewidth=3)\n",
        "    ax1.text(repeat_num*(i+0.5)+0.5, average_1+0.7*span, 'Run ' + run_, ha='center', va='center', fontsize=16)\n",
        "\n",
        "# put two legends together\n",
        "lines = [green_line, red_line]\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax2.legend(lines, labels, loc = 'lower left', fontsize=14, facecolor='none')\n",
        "\n",
        "plt.title(\"Green and Red F Value Std in Different Repeats (After Mean and Std Restoration)\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "## store them to variables with new names\n",
        "## and for later use in decay restoration\n",
        "green_mean_repeat_list_after_mean_restored_std_restored = green_mean_repeat.copy() # same as green_mean_repeat_restored\n",
        "red_mean_repeat_list_after_mean_restored_std_restored = red_mean_repeat.copy() # same as red_mean_repeat_restored\n",
        "\n",
        "green_mean_run_list_after_mean_restored_std_restored = green_mean_run.copy()\n",
        "red_mean_run_list_after_mean_restored_std_restored = red_mean_run.copy()\n",
        "\n",
        "green_std_repeat_list_after_mean_restored_std_restored = green_std_repeat.copy()\n",
        "red_std_repeat_list_after_mean_restored_std_restored = red_std_repeat.copy()\n",
        "\n",
        "green_std_run_list_after_mean_restored_std_restored = green_std_run.copy()\n",
        "red_std_run_list_after_mean_restored_std_restored = red_std_run.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HjT2vn1_P2O"
      },
      "source": [
        "#### Plot tunning curves using retored data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEh2fzrE_gPS"
      },
      "source": [
        "This is the same process as in \"Read fluorescence structure data/Read red and green data then plot\", but uses restored data. Need to use function recover_strucure_from_traces_and_locs() to recover conca_fluo_data (structure data) from signal traces and locations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rti0_PVf_7as"
      },
      "outputs": [],
      "source": [
        "# cell_name_list = ['CL075_230228']\n",
        "# run_num_list = [['1', '2', '3']]\n",
        "\n",
        "# cell_name_list = ['CL090_230515']\n",
        "# run_num_list = [['4', '5', '6']]\n",
        "\n",
        "# for cell_name_, run_num_ in zip(cell_name_list, run_num_list):\n",
        "#     for color_ in ['red', 'green']:\n",
        "#         for datatype_ in ['F']:\n",
        "#             if color_ == 'red':\n",
        "#                 conca_fluo_data_ = recover_strucure_from_traces_and_locs(red_signal_traces_mean_restored_std_restored, red_locations)\n",
        "#                 plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_)\n",
        "#             if color_ == 'green':\n",
        "#                 conca_fluo_data_ = recover_strucure_from_traces_and_locs(green_signal_traces_mean_restored_std_restored, green_locations)\n",
        "#                 for component_ in range(1, conca_fluo_data_[0,0].shape[1]+1):\n",
        "#                     plot_all_trials(conca_fluo_data_, cell_name_, run_num_, color_, datatype_, component_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0NseRgUA97I"
      },
      "outputs": [],
      "source": [
        "# # batch download the plotted figures\n",
        "# # uncomment the code below to download figures if needed\n",
        "\n",
        "# import glob\n",
        "\n",
        "# folder_path = '.'\n",
        "# # file_prefix = 'All_Conditions_All_Rounds_All_Repeats_'\n",
        "# file_prefix = 'CL'\n",
        "\n",
        "# # Use glob to find all files with the given prefix in the folder\n",
        "# matching_files = glob.glob(f\"{folder_path}/{file_prefix}*\")\n",
        "# matching_files_new = []\n",
        "# for file_path in matching_files:\n",
        "#     # Check if the file ends with \".pdf\"\n",
        "#     if file_path.lower().endswith('.pdf'):\n",
        "#         matching_files_new.append(file_path)\n",
        "# matching_files = matching_files_new\n",
        "# # print(matching_files)\n",
        "# # # Print the matching file names\n",
        "# # for file_path in matching_files:\n",
        "# #     print(file_path)\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# zip_filename = 'files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     # Add files to the zip file\n",
        "#     for file_path in matching_files:\n",
        "#         zipf.write(file_path)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QtazLHcCM8"
      },
      "source": [
        "#### Save important data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-vsdS6PcEwK"
      },
      "outputs": [],
      "source": [
        "# np.save(cell_name+'red_signal_traces.npy', red_signal_traces)\n",
        "# np.save(cell_name+'red_conca_fluo_data.npy', red_conca_fluo_data)\n",
        "# np.save(cell_name+'red_locations.npy', red_locations)\n",
        "\n",
        "# np.save(cell_name+'green_signal_traces.npy', green_signal_traces)\n",
        "# np.save(cell_name+'green_conca_fluo_data.npy', green_conca_fluo_data)\n",
        "# np.save(cell_name+'green_locations.npy', green_locations)\n",
        "\n",
        "# np.save(cell_name+'green_signal_traces_mean_restored.npy', green_signal_traces_mean_restored)\n",
        "# np.save(cell_name+'red_signal_traces_mean_restored.npy', red_signal_traces_mean_restored)\n",
        "\n",
        "# np.save(cell_name+'green_signal_traces_mean_restored_std_restored.npy', green_signal_traces_mean_restored_std_restored)\n",
        "# np.save(cell_name+'red_signal_traces_mean_restored_std_restored.npy', red_signal_traces_mean_restored_std_restored)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnnQRZQHeIS1"
      },
      "outputs": [],
      "source": [
        "# # Load red signal traces data\n",
        "# red_signal_traces = np.load(cell_name + 'red_signal_traces.npy')\n",
        "# print(\"Red Signal Traces Shape:\", red_signal_traces.shape)\n",
        "# print(\"Red Signal Traces Type:\", type(red_signal_traces))\n",
        "\n",
        "# # Load red concatenated fluorescence data\n",
        "# red_conca_fluo_data = np.load(cell_name + 'red_conca_fluo_data.npy', allow_pickle=True)\n",
        "# print(\"Red Concatenated Fluorescence Data Shape:\", red_conca_fluo_data.shape)\n",
        "# print(\"Red Concatenated Fluorescence Data Type:\", type(red_conca_fluo_data))\n",
        "# print(\"Red Concatenated Fluorescence Data Elements Shape:\", red_conca_fluo_data[0,0].shape)\n",
        "# print(\"Red Concatenated Fluorescence Data Elements Type:\", type(red_conca_fluo_data[0, 0]))\n",
        "\n",
        "# # Load red locations data\n",
        "# red_locations = np.load(cell_name + 'red_locations.npy', allow_pickle=True)\n",
        "# print(\"Red Locations Shape:\", red_locations.shape)\n",
        "# print(\"Red Locations Type:\", type(red_locations))\n",
        "# print(\"Red Locations Elements Shape:\", red_locations[0,0].shape)\n",
        "# print(\"Red Locations Elements Type:\", type(red_locations[0, 0]))\n",
        "\n",
        "# # Load green signal traces data\n",
        "# green_signal_traces = np.load(cell_name + 'green_signal_traces.npy')\n",
        "# print(\"Green Signal Traces Shape:\", green_signal_traces.shape)\n",
        "# print(\"Green Signal Traces Type:\", type(green_signal_traces))\n",
        "\n",
        "# # Load green concatenated fluorescence data\n",
        "# green_conca_fluo_data = np.load(cell_name + 'green_conca_fluo_data.npy', allow_pickle=True)\n",
        "# print(\"Green Concatenated Fluorescence Data Shape:\", green_conca_fluo_data.shape)\n",
        "# print(\"Green Concatenated Fluorescence Data Type:\", type(green_conca_fluo_data))\n",
        "# print(\"Green Concatenated Fluorescence Elements Data Shape:\", green_conca_fluo_data[0,0].shape)\n",
        "# print(\"Green Concatenated Fluorescence Elements Data Type:\", type(green_conca_fluo_data[0, 0]))\n",
        "\n",
        "# # Load green locations data\n",
        "# green_locations = np.load(cell_name + 'green_locations.npy', allow_pickle=True)\n",
        "# print(\"Green Locations Shape:\", green_locations.shape)\n",
        "# print(\"Green Locations Type:\", type(green_locations))\n",
        "# print(\"Green Locations Elements Shape:\", green_locations[0,0].shape)\n",
        "# print(\"Green Locations Elements Type:\", type(green_locations[0, 0]))\n",
        "\n",
        "# # Load green signal traces mean restored data\n",
        "# green_signal_traces_mean_restored = np.load(cell_name + 'green_signal_traces_mean_restored.npy')\n",
        "# print(\"Green Signal Traces Mean Restored Shape:\", green_signal_traces_mean_restored.shape)\n",
        "# print(\"Green Signal Traces Mean Restored Type:\", type(green_signal_traces_mean_restored))\n",
        "\n",
        "# # Load red signal traces mean restored data\n",
        "# red_signal_traces_mean_restored = np.load(cell_name + 'red_signal_traces_mean_restored.npy')\n",
        "# print(\"Red Signal Traces Mean Restored Shape:\", red_signal_traces_mean_restored.shape)\n",
        "# print(\"Red Signal Traces Mean Restored Type:\", type(red_signal_traces_mean_restored))\n",
        "\n",
        "# # Load green signal traces mean restored std restored data\n",
        "# green_signal_traces_mean_restored_std_restored = np.load(cell_name + 'green_signal_traces_mean_restored_std_restored.npy')\n",
        "# print(\"Green Signal Traces Mean Restored Std Restored Shape:\", green_signal_traces_mean_restored_std_restored.shape)\n",
        "# print(\"Green Signal Traces Mean Restored Std Restored Type:\", type(green_signal_traces_mean_restored_std_restored))\n",
        "\n",
        "# # Load red signal traces mean restored std restored data\n",
        "# red_signal_traces_mean_restored_std_restored = np.load(cell_name + 'red_signal_traces_mean_restored_std_restored.npy')\n",
        "# print(\"Red Signal Traces Mean Restored Std Restored Shape:\", red_signal_traces_mean_restored_std_restored.shape)\n",
        "# print(\"Red Signal Traces Mean Restored Std Restored Type:\", type(red_signal_traces_mean_restored_std_restored))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGpdQsmRlVZ7"
      },
      "outputs": [],
      "source": [
        "# # batch download the plotted figures\n",
        "# # uncomment the code below to download figures if needed\n",
        "\n",
        "# import glob\n",
        "\n",
        "# folder_path = '.'\n",
        "# # file_prefix = 'All_Conditions_All_Rounds_All_Repeats_'\n",
        "# file_prefix = 'CL'\n",
        "\n",
        "# # Use glob to find all files with the given prefix in the folder\n",
        "# matching_files = glob.glob(f\"{folder_path}/{file_prefix}*\")\n",
        "# # print(matching_files)\n",
        "# # # Print the matching file names\n",
        "# # for file_path in matching_files:\n",
        "# #     print(file_path)\n",
        "\n",
        "# import zipfile\n",
        "\n",
        "# zip_filename = 'files.zip'\n",
        "# with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "#     # Add files to the zip file\n",
        "#     for file_path in matching_files:\n",
        "#         zipf.write(file_path)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wV8MuOh-cOa"
      },
      "source": [
        "#### Generate the data and label, and train and eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633X0R2-TxYM",
        "outputId": "3faa401c-c433-4764-cb72-ffd6dbda511e"
      },
      "outputs": [],
      "source": [
        "# # different cell has different parameters, this is for cell CL075_230228\n",
        "# valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components(cell_name, 100, 5)\n",
        "# data_set = green_signal_traces.copy()\n",
        "# label_set = red_signal_traces.copy()\n",
        "# print(data_set[:,valid_com_index_list,:].shape) # remain the valid component\n",
        "\n",
        "# different cell has different parameters, this is for cell CL090_230515\n",
        "valid_com_index_list, valid_dis_list, valid_size_list = get_valid_components(cell_name, 100, 0)\n",
        "data_set = green_signal_traces.copy()\n",
        "label_set = red_signal_traces.copy()\n",
        "print(data_set[:,valid_com_index_list,:].shape) # remain the valid component\n",
        "print(valid_com_index_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EVIvD9WZuBHh",
        "outputId": "95fee96b-e492-49d9-901b-c98fabc1a82b"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "\n",
        "## plot red\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = green_signal_traces.copy()\n",
        "        label_set = red_signal_traces.copy()\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored.copy()\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored_std_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored_std_restored.copy()\n",
        "\n",
        "    x = range(1, label_set.shape[2] + 1)\n",
        "\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    plt.plot(list(label_set[0,0,:])+list(label_set[1,0,:])+list(label_set[2,0,:]), color='salmon', linestyle='-', linewidth=1)\n",
        "    v1 = label_set.shape[2] + 0.5\n",
        "    v2 = label_set.shape[2] * 2 + 0.5\n",
        "    plt.axvline(x=v1, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.axvline(x=v2, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('Frames', fontsize = 20)\n",
        "    plt.ylabel('F Value', fontsize = 20)\n",
        "    plt.tick_params(labelsize=18)\n",
        "    plt.title(f'Whole red signal trace ({signal_trace_type})', fontsize = 24)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## plot green\n",
        "component_index = 20\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = green_signal_traces.copy()\n",
        "        label_set = red_signal_traces.copy()\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored.copy()\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored_std_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored_std_restored.copy()\n",
        "\n",
        "    data_set = data_set[:, valid_com_index_list, :] # use valid components\n",
        "\n",
        "    x = range(1, data_set.shape[2] + 1)\n",
        "\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    plt.plot(list(data_set[0,component_index,:])+list(data_set[1,component_index,:])+list(data_set[2,component_index,:]), color='limegreen', linestyle='-', linewidth=1)\n",
        "    v1 = data_set.shape[2] + 0.5\n",
        "    v2 = data_set.shape[2] * 2 + 0.5\n",
        "    plt.axvline(x=v1, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.axvline(x=v2, color='gray', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('Frames', fontsize = 20)\n",
        "    plt.ylabel('F Value', fontsize = 20)\n",
        "    plt.tick_params(labelsize=18)\n",
        "    plt.title(f'Whole green signal trace of Component {component_index} ({signal_trace_type})', fontsize = 24)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-DpRkJ1_0ni"
      },
      "source": [
        "The next chunck is traning and plotting. It takes some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ejCFm2ITARUU",
        "outputId": "452ecc22-790a-4cb0-85be-55471c91d480"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "list_of_train_loss_lists = []\n",
        "list_of_test_loss_lists = []\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = green_signal_traces.copy()\n",
        "        label_set = red_signal_traces.copy()\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored.copy()\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored_std_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored_std_restored.copy()\n",
        "\n",
        "    data_set = data_set[:,valid_com_index_list,:] # use valid components\n",
        "\n",
        "    print(f\"data_set.shape: {data_set.shape}\")\n",
        "    print(f\"label_set.shape: {label_set.shape}\")\n",
        "\n",
        "    # Normalize data_set to [-1,1]\n",
        "    data_set_min = np.min(data_set)\n",
        "    data_set_max = np.max(data_set)\n",
        "    data_set = ((data_set - data_set_min) / (data_set_max - data_set_min) - 0.5) * 2\n",
        "\n",
        "    # Normalize label_set to [-1,1]\n",
        "    label_set_min = np.min(label_set)\n",
        "    label_set_max = np.max(label_set)\n",
        "    label_set = ((label_set - label_set_min) / (label_set_max - label_set_min) - 0.5) * 2\n",
        "\n",
        "    # Define dataset class\n",
        "    class FluoDataset(Dataset):\n",
        "        def __init__(self, data_set, label_set, z_indices, x_indices):\n",
        "            self.data_set = data_set\n",
        "            self.label_set = label_set\n",
        "            self.z_indices = z_indices\n",
        "            self.x_indices = x_indices\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.z_indices)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            z = self.z_indices[idx]\n",
        "            x = self.x_indices[idx]\n",
        "\n",
        "            input_data = self.data_set[z, :, x-62:x]\n",
        "            target_label = np.mean(self.label_set[z, :, x-31:x])\n",
        "\n",
        "            return torch.tensor(input_data, dtype=torch.float32), torch.tensor(target_label, dtype=torch.float32)\n",
        "            # or torch.from_numpy(input_data).float(), torch.from_numpy(target_label).float()\n",
        "\n",
        "    # Set a random seed for reproducibility\n",
        "    # np.random.seed(16)  # can use any integer as the seed value\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    length = 6400*2\n",
        "    choices = range(3)\n",
        "    # choices = range(2) # delete last run\n",
        "    train_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    choices = list(range(62, 10000)) + list(range(12000, 17000)) + list(range(18244, 32500)) # 90% for train\n",
        "    # 10000-62+17000-12000+32500-18244=29194, 29194/(32500-62)=90%, 32500-62 is the all data pieces\n",
        "    train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640\n",
        "    choices = range(3)\n",
        "    # choices = range(2) # delete last run\n",
        "    test_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    # choices = range(30000, 32500)\n",
        "    choices = list(range(10000, 12000)) + list(range(17000, 18244)) # 10% for test\n",
        "    # 12000-10000+18244-17000=3244, 3244/(32500-62)=10%, 32500-62 is the all data pieces\n",
        "    test_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640 # this is a subset of train data for test on train\n",
        "    choices = range(3)\n",
        "    # choices = range(2) # delete last run\n",
        "    small_train_z_indices = np.random.choice(choices, size=length)\n",
        "    choices = list(range(9000, 10000)) + list(range(12000, 17000)) + list(range(18244, 19000))\n",
        "    small_train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    train_dataset = FluoDataset(data_set, label_set, train_z_indices, train_x_indices)\n",
        "    test_dataset = FluoDataset(data_set, label_set, test_z_indices, test_x_indices)\n",
        "    test_on_train_dataset = FluoDataset(data_set, label_set, small_train_z_indices, small_train_x_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_on_train_loader = DataLoader(test_on_train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # print(len(train_loader))\n",
        "    # print(len(test_loader))\n",
        "\n",
        "\n",
        "    # Define the model\n",
        "    class FluoModel(nn.Module):\n",
        "        def __init__(self, component_num):\n",
        "            super(FluoModel, self).__init__()\n",
        "            self.fc_shared = nn.Linear(62, 1)  # Shared fully connected layer\n",
        "            self.fc_reduce = nn.Linear(component_num, 1)  # Fully connected layer to reduce component_num (e.g., 281) channels to 1\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "            self.fc_end = nn.Linear(1, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1, 62)  # Reshape to (batch_size, component_num, 62)\n",
        "\n",
        "            # Apply the shared fully connected layer along the last dimension (62)\n",
        "            shared_output = self.fc_shared(x).squeeze(2)\n",
        "            # self.fc_shared(x) shape is (batch_size, component_num, 1), then squeeze the last dimension\n",
        "            # In PyTorch, when you apply a fully connected layer (or any other linear\n",
        "            # transformation) to a 3D tensor, by default, the operation is performed\n",
        "            # along the last dimension of the tensor.\n",
        "\n",
        "            # Reduce component_num (e.g., 281) channels to 1 using a separate fully connected layer\n",
        "            reduced_output = self.fc_reduce(shared_output)\n",
        "            pre_output = self.sigmoid(reduced_output)\n",
        "            output = self.fc_end(pre_output)\n",
        "\n",
        "            return output\n",
        "\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    for i_ in range(30): # try 5 times to get the best one\n",
        "        model = FluoModel(data_set.shape[1])\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "        # optimizer = optim.Adam(model.parameters(), lr=0.004)\n",
        "        # scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 90\n",
        "        print_interval = 10\n",
        "\n",
        "        train_loss_list_ = []\n",
        "        test_loss_list_ = []\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # scheduler.step()\n",
        "\n",
        "            # if epoch < 5 or (epoch + 1) % print_interval == 0:\n",
        "            #     print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item():.6f}\")\n",
        "\n",
        "            # Testing on train and test sets\n",
        "            model.eval()\n",
        "\n",
        "            train_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in test_on_train_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    train_loss += criterion(outputs.squeeze(), targets).item()\n",
        "\n",
        "            average_train_loss = train_loss / len(test_on_train_dataset)\n",
        "            train_loss_list_.append(average_train_loss)\n",
        "\n",
        "            test_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in test_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    test_loss += criterion(outputs.squeeze(), targets).item()\n",
        "\n",
        "            average_test_loss = test_loss / len(test_loader)\n",
        "            if epoch == 5 and average_test_loss >= 0.9 * np.mean(np.array(test_loss_list_)[1:]):\n",
        "                break; # kill trials with a low probability of convergence (terminate trials that are unlikely to converge)\n",
        "            test_loss_list_.append(average_test_loss)\n",
        "\n",
        "            if epoch < 5 or (epoch + 1) % print_interval == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {average_train_loss:.6f} | Test Loss: {average_test_loss:.6f}\")\n",
        "\n",
        "        if i_ == 0:\n",
        "            train_loss_list, test_loss_list = train_loss_list_, test_loss_list_\n",
        "            model_final = model\n",
        "        elif test_loss_list_[-1] < test_loss_list[-1]:\n",
        "            train_loss_list, test_loss_list = train_loss_list_, test_loss_list_\n",
        "            model_final = model\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(train_loss_list, color = 'skyblue', label='Train Loss', linewidth=2)\n",
        "    plt.plot(test_loss_list, color = 'salmon', label='Test Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch Index', fontsize=16)\n",
        "    plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Train and Test Loss Curves\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    list_of_train_loss_lists.append(train_loss_list)\n",
        "    list_of_test_loss_lists.append(test_loss_list)\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        torch.save(model_final.state_dict(), f)\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "    outputs_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs_list.append(outputs.numpy())\n",
        "            labels_list.append(targets.numpy())\n",
        "\n",
        "    outputs_array = np.concatenate(outputs_list)\n",
        "    labels_array = np.concatenate(labels_list)\n",
        "\n",
        "    # Sort labels and corresponding outputs\n",
        "    sorted_indices = np.argsort(labels_array)\n",
        "    sorted_labels = labels_array[sorted_indices]\n",
        "    sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(sorted_labels, color = 'skyblue', label='True Labels', linewidth=2)\n",
        "    plt.plot(sorted_outputs, color = 'salmon', label='Model Outputs', linewidth=2)\n",
        "    plt.xlabel('Sample Index', fontsize=16)\n",
        "    plt.ylabel('Normalized Value', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Comparison between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "    plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJIaek7Ohqe6",
        "outputId": "b718dec6-f817-4dd9-eefe-b7e061cc0b7e"
      },
      "outputs": [],
      "source": [
        "# compare train losses and compare test loss between different types of signal traces\n",
        "\n",
        "colors_list = [('deepskyblue', 'orangered'), ('royalblue', 'salmon'), ('cornflowerblue', 'violet')]\n",
        "linestyle_list = ['dashed', 'dotted', 'solid']\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(train_loss_list, color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "    # plt.plot(train_loss_list[-10:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Train Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(test_loss_list, color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "    # plt.plot(test_loss_list[-10:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Test Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plot last epochs\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(np.arange(len(train_loss_list)+1-50, len(train_loss_list)+1), train_loss_list[-50:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "    # plt.plot(train_loss_list[-10:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Train Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(np.arange(len(test_loss_list)+1-50, len(test_loss_list)+1), test_loss_list[-50:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "    # plt.plot(test_loss_list[-10:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Test Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwsXtyvjsfC"
      },
      "source": [
        "The next chunck plots \"Comparison between True Labels and Model Outputs\" and \"Scatter Plot between True Labels and Model Outputs\" without training a model but with directly loading the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "orqdjQCiL18S",
        "outputId": "b8043ec7-58f7-4cdf-8d1f-90360cd5382a"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = green_signal_traces.copy()\n",
        "        label_set = red_signal_traces.copy()\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored.copy()\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored_std_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored_std_restored.copy()\n",
        "\n",
        "    data_set = data_set[:,valid_com_index_list,:] # use valid components\n",
        "\n",
        "    print(f\"data_set.shape: {data_set.shape}\")\n",
        "    print(f\"label_set.shape: {label_set.shape}\")\n",
        "\n",
        "    # Normalize data_set to [-1,1]\n",
        "    data_set_min = np.min(data_set)\n",
        "    data_set_max = np.max(data_set)\n",
        "    data_set = ((data_set - data_set_min) / (data_set_max - data_set_min) - 0.5) * 2\n",
        "\n",
        "    # Normalize label_set to [-1,1]\n",
        "    label_set_min = np.min(label_set)\n",
        "    label_set_max = np.max(label_set)\n",
        "    label_set = ((label_set - label_set_min) / (label_set_max - label_set_min) - 0.5) * 2\n",
        "\n",
        "    # Define dataset class\n",
        "    class FluoDataset(Dataset):\n",
        "        def __init__(self, data_set, label_set, z_indices, x_indices):\n",
        "            self.data_set = data_set\n",
        "            self.label_set = label_set\n",
        "            self.z_indices = z_indices\n",
        "            self.x_indices = x_indices\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.z_indices)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            z = self.z_indices[idx]\n",
        "            x = self.x_indices[idx]\n",
        "\n",
        "            input_data = self.data_set[z, :, x-62:x]\n",
        "            target_label = np.mean(self.label_set[z, :, x-31:x])\n",
        "\n",
        "            return torch.tensor(input_data, dtype=torch.float32), torch.tensor(target_label, dtype=torch.float32)\n",
        "            # or torch.from_numpy(input_data).float(), torch.from_numpy(target_label).float()\n",
        "\n",
        "    # Set a random seed for reproducibility\n",
        "    # np.random.seed(16)  # can use any integer as the seed value\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    length = 6400*2\n",
        "    choices = range(3)\n",
        "    choices = range(2) # delete last run\n",
        "    train_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    choices = list(range(62, 10000)) + list(range(12000, 17000)) + list(range(18244, 32500)) # 90% for train\n",
        "    # 10000-62+17000-12000+32500-18244=29194, 29194/(32500-62)=90%, 32500-62 is the all data pieces\n",
        "    train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640\n",
        "    choices = range(3)\n",
        "    choices = range(2) # delete last run\n",
        "    test_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    # choices = range(30000, 32500)\n",
        "    choices = list(range(10000, 12000)) + list(range(17000, 18244)) # 10% for test\n",
        "    # 12000-10000+18244-17000=3244, 3244/(32500-62)=10%, 32500-62 is the all data pieces\n",
        "    test_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640 # this is a subset of train data for test on train\n",
        "    choices = range(3)\n",
        "    small_train_z_indices = np.random.choice(choices, size=length)\n",
        "    choices = list(range(9000, 10000)) + list(range(12000, 17000)) + list(range(18244, 19000))\n",
        "    small_train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    train_dataset = FluoDataset(data_set, label_set, train_z_indices, train_x_indices)\n",
        "    test_dataset = FluoDataset(data_set, label_set, test_z_indices, test_x_indices)\n",
        "    test_on_train_dataset = FluoDataset(data_set, label_set, small_train_z_indices, small_train_x_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_on_train_loader = DataLoader(test_on_train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # print(len(train_loader))\n",
        "    # print(len(test_loader))\n",
        "\n",
        "\n",
        "    # Define the model\n",
        "    class FluoModel(nn.Module):\n",
        "        def __init__(self, component_num):\n",
        "            super(FluoModel, self).__init__()\n",
        "            self.fc_shared = nn.Linear(62, 1)  # Shared fully connected layer\n",
        "            self.fc_reduce = nn.Linear(component_num, 1)  # Fully connected layer to reduce component_num (e.g., 281) channels to 1\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "            self.fc_end = nn.Linear(1, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1, 62)  # Reshape to (batch_size, component_num, 62)\n",
        "\n",
        "            # Apply the shared fully connected layer along the last dimension (62)\n",
        "            shared_output = self.fc_shared(x).squeeze(2)\n",
        "            # self.fc_shared(x) shape is (batch_size, component_num, 1), then squeeze the last dimension\n",
        "            # In PyTorch, when you apply a fully connected layer (or any other linear\n",
        "            # transformation) to a 3D tensor, by default, the operation is performed\n",
        "            # along the last dimension of the tensor.\n",
        "\n",
        "            # Reduce component_num (e.g., 281) channels to 1 using a separate fully connected layer\n",
        "            reduced_output = self.fc_reduce(shared_output)\n",
        "            pre_output = self.sigmoid(reduced_output)\n",
        "            output = self.fc_end(pre_output)\n",
        "\n",
        "            return output\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "    outputs_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs_list.append(outputs.numpy())\n",
        "            labels_list.append(targets.numpy())\n",
        "\n",
        "    outputs_array = np.concatenate(outputs_list)\n",
        "    labels_array = np.concatenate(labels_list)\n",
        "\n",
        "    # Sort labels and corresponding outputs\n",
        "    sorted_indices = np.argsort(labels_array)\n",
        "    sorted_labels = labels_array[sorted_indices]\n",
        "    sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(sorted_labels, color = 'skyblue', label='True Labels', linewidth=2)\n",
        "    plt.plot(sorted_outputs, color = 'salmon', label='Model Outputs', linewidth=2)\n",
        "    plt.xlabel('Sample Index', fontsize=16)\n",
        "    plt.ylabel('Normalized Value', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Comparison between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "    plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "## below is wrong original code\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "#                           \"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# # signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "# #                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# # signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# # signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# # signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# for signal_trace_type in signal_trace_type_list:\n",
        "#     if signal_trace_type == \"Raw Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "#     elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "#     elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "#     model = FluoModel(data_set.shape[1])\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "#     model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "#     outputs_list = []\n",
        "#     labels_list = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in test_loader:\n",
        "#             outputs = model(inputs)\n",
        "#             outputs_list.append(outputs.numpy())\n",
        "#             labels_list.append(targets.numpy())\n",
        "\n",
        "#     outputs_array = np.concatenate(outputs_list)\n",
        "#     labels_array = np.concatenate(labels_list)\n",
        "\n",
        "#     # Sort labels and corresponding outputs\n",
        "#     sorted_indices = np.argsort(labels_array)\n",
        "#     sorted_labels = labels_array[sorted_indices]\n",
        "#     sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "#     # Calculate the correlation coefficient\n",
        "#     correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "#     # plot results\n",
        "#     plt.figure(figsize=(6.4, 4.8))\n",
        "#     plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "#     plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "#     plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "#     plt.tick_params(labelsize=14)\n",
        "#     plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "#     plt.legend(fontsize=14, facecolor='none')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkVCDSqEilb7"
      },
      "source": [
        "#### Load trained model and see weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mlbhtQ0Aiu7Y",
        "outputId": "29489e78-0fb3-4b0c-bcfb-159577933f58"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\", \"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\", \"Mean and Std Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_Axon.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "axons = mat_data['Axons']\n",
        "# Squeeze the outer array\n",
        "axons = np.squeeze(axons, axis=0)\n",
        "for i in range(len(axons)):\n",
        "    # Squeeze the inner array and convert the data type to 'int'\n",
        "    axons[i] = np.squeeze(axons[i].astype(int), axis=0)\n",
        "\n",
        "valid_com_index_from_one_list = np.array(valid_com_index_list) + 1\n",
        "\n",
        "# Convert axons to index form thereby consistent with the index of remained components\n",
        "index_form_axons = copy.deepcopy(axons)\n",
        "for i, axon in enumerate(axons):\n",
        "    for j, bouton in enumerate(axon):\n",
        "        index_form_axons[i][j] = np.where(valid_com_index_from_one_list == bouton)[0]\n",
        "flat_index_form_axons = np.concatenate(index_form_axons)\n",
        "\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    model = FluoModel(data_set.shape[1])\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored.pth\"\n",
        "\n",
        "    state_dict = torch.load(model_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # access the weights for each layer\n",
        "    fc_shared_weights = model.fc_shared.weight\n",
        "    fc_reduce_weights = model.fc_reduce.weight\n",
        "    fc_end_weights = model.fc_end.weight\n",
        "\n",
        "    # access the biases if needed\n",
        "    fc_shared_biases = model.fc_shared.bias\n",
        "    fc_reduce_biases = model.fc_reduce.bias\n",
        "    fc_end_biases = model.fc_end.bias\n",
        "\n",
        "    print(f\"{signal_trace_type}\")\n",
        "\n",
        "    # Weights\n",
        "    print(\"\\nfc_shared weights:\")\n",
        "    print(fc_shared_weights)\n",
        "\n",
        "    print(\"\\nfc_reduce weights:\")\n",
        "    print(fc_reduce_weights)\n",
        "\n",
        "    print(\"\\nfc_end weights:\")\n",
        "    print(fc_end_weights)\n",
        "\n",
        "    # Biases\n",
        "    print(\"\\nfc_shared biases:\")\n",
        "    print(fc_shared_biases)\n",
        "\n",
        "    print(\"\\nfc_reduce biases:\")\n",
        "    print(fc_reduce_biases)\n",
        "\n",
        "    print(\"\\nfc_end biases:\")\n",
        "    print(fc_end_biases)\n",
        "\n",
        "    # Access the weights\n",
        "    fc_shared_weights = fc_shared_weights.detach().numpy().flatten()\n",
        "    fc_reduce_weights = fc_reduce_weights.detach().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (weights abs)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Plot fc_shared_weights\n",
        "    ax1.bar(range(1,len(fc_shared_weights)+1), np.abs(fc_shared_weights), color='skyblue')\n",
        "    ax1.set_title(f'fc_shared_weights, corresponding to frames\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Frame', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    half_len = int(len(fc_shared_weights)/2)\n",
        "    print(f\"The mean the first {half_len} weight absolute values of fc_shared_weights is {np.mean(np.abs(fc_shared_weights)[:half_len])}\")\n",
        "    print(f\"The mean the last {half_len} weight absolute values of fc_shared_weights is {np.mean(np.abs(fc_shared_weights)[half_len:])}\")\n",
        "\n",
        "\n",
        "    # Create subplots (1 y axis)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(range(1,len(fc_reduce_weights)+1), np.abs(fc_reduce_weights), color='salmon')\n",
        "    ax1.set_title(f'fc_reduce_weights, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights abs and distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, np.abs(fc_reduce_weights), color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Distance, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, valid_dis_list, color='skyblue', label='Distance to Soma')\n",
        "    ax2.set_ylabel('Distance to Soma', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_dis_list = np.array(valid_dis_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(sorted_valid_dis_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, sorted_valid_dis_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('Distance to Soma (Pixels)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and Distance to Soma\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and 1/distance)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_dis_list = np.array(valid_dis_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(1/sorted_valid_dis_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, 1/sorted_valid_dis_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('1/Distance to Soma (Pixels)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and 1/Distance to Soma\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights abs and distance, grouped)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Reorder fc_reduce_weights and valid_dis_list based on group_indices\n",
        "    reordered_fc_reduce_weights = [fc_reduce_weights[idx] for idx in flat_index_form_axons]\n",
        "    reordered_valid_dis_list = [valid_dis_list[idx] for idx in flat_index_form_axons]\n",
        "\n",
        "    # Create x-labels for groups\n",
        "    x_labels = [f\"Group {i+1}\" for i in range(len(index_form_axons))]\n",
        "\n",
        "    # Bar plot for fc_reduce_weights\n",
        "    x = np.arange(1, len(flat_index_form_axons) + 1)\n",
        "    index__ = 0\n",
        "    for axon in index_form_axons:\n",
        "        index__ = index__ + len(axon)\n",
        "        if index__ < len(flat_index_form_axons):\n",
        "            x[index__:] = x[index__:] + 5 # generate gap space on x axis between groups\n",
        "\n",
        "    break_indices = np.where(np.diff(x) != 1)[0]\n",
        "    x_labels_center = [(x[start] + x[end]) / 2 for start, end in zip(np.insert(break_indices + 1, 0, 0), break_indices)]\n",
        "    x_labels_center.append((x[break_indices[-1]+1] + x[-1])/2)\n",
        "\n",
        "    ax1.bar(x, np.abs(reordered_fc_reduce_weights), color='salmon', label='Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Distance, corresponding to components, grouped\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Groups', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.set_xticks(x_labels_center)\n",
        "    ax1.set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve (valid_dis_list)\n",
        "    ax2.plot(x, reordered_valid_dis_list, color='skyblue', label='Distance to Soma')\n",
        "    ax2.set_ylabel('Distance to Soma', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for both curves\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weights abs and size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "    x = np.arange(1, len(fc_reduce_weights) + 1)\n",
        "\n",
        "    # Plot fc_reduce_weights\n",
        "    ax1.bar(x, np.abs(fc_reduce_weights), color='salmon', label = 'Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Size, corresponding to components\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Weight Index/Component', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.tick_params(axis='x', labelsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve\n",
        "    ax2.plot(x, valid_size_list, color='skyblue', label='Size/Pixels')\n",
        "    ax2.set_ylabel('Size/Pixels', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for the second curve\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_size_list = np.array(valid_size_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(sorted_valid_size_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, sorted_valid_size_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('Size (Pixel Number)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and Size\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (correlation between weights abs and 1/size)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4, 4.8))\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    sorted_indices = np.argsort(np.abs(fc_reduce_weights))\n",
        "    sorted_valid_size_list = np.array(valid_size_list)[sorted_indices]\n",
        "    sorted_abs_fc_reduce_weights = np.abs(fc_reduce_weights)[sorted_indices]\n",
        "    correlation_coefficient = np.corrcoef(sorted_abs_fc_reduce_weights, np.transpose(1/sorted_valid_size_list))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    ax1.scatter(sorted_abs_fc_reduce_weights, 1/sorted_valid_size_list, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    ax1.set_xlabel('Weights', fontsize=16)\n",
        "    ax1.set_ylabel('1/Size (Pixel Number)', fontsize=16)\n",
        "    ax1.tick_params(labelsize=14)\n",
        "    ax1.set_title(f\"Scatter Plot between Weights and 1/Size\\n({signal_trace_type})\", fontsize=18)\n",
        "    ax1.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create subplots (2 y axes, weight abs and size, grouped)\n",
        "    fig, ax1 = plt.subplots(figsize=(6.4*2, 4.8))\n",
        "\n",
        "    # Reorder fc_reduce_weights and valid_dis_list based on group_indices\n",
        "    reordered_fc_reduce_weights = [fc_reduce_weights[idx] for idx in flat_index_form_axons]\n",
        "    reordered_valid_size_list = [valid_size_list[idx] for idx in flat_index_form_axons]\n",
        "\n",
        "    # Create x-labels for groups\n",
        "    x_labels = [f\"Group {i+1}\" for i in range(len(index_form_axons))]\n",
        "\n",
        "    # Bar plot for fc_reduce_weights\n",
        "    x = np.arange(1, len(flat_index_form_axons) + 1)\n",
        "    index__ = 0\n",
        "    for axon in index_form_axons:\n",
        "        index__ = index__ + len(axon)\n",
        "        if index__ < len(flat_index_form_axons):\n",
        "            x[index__:] = x[index__:] + 5\n",
        "\n",
        "    break_indices = np.where(np.diff(x) != 1)[0]\n",
        "    x_labels_center = [(x[start] + x[end]) / 2 for start, end in zip(np.insert(break_indices + 1, 0, 0), break_indices)]\n",
        "    x_labels_center.append((x[break_indices[-1]+1] + x[-1])/2)\n",
        "\n",
        "    ax1.bar(x, np.abs(reordered_fc_reduce_weights), color='salmon', label='Weights')\n",
        "    ax1.set_title(f'fc_reduce_weights and Size, corresponding to components, grouped\\n({signal_trace_type})', fontsize=18)\n",
        "    ax1.set_xlabel('Groups', fontsize=16)\n",
        "    ax1.set_ylabel('Absolute Weight Value', fontsize=16)\n",
        "    ax1.set_xticks(x_labels_center)\n",
        "    ax1.set_xticklabels(x_labels, rotation=45, fontsize=14)\n",
        "    ax1.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Create a second y-axis\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot the second curve (valid_dis_list)\n",
        "    ax2.plot(x, reordered_valid_size_list, color='skyblue', label='Size/Pixels')\n",
        "    ax2.set_ylabel('Size/Pixels', fontsize=16)\n",
        "    ax2.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "    # Adding legend for both curves\n",
        "    ax1.legend(loc='upper left', fontsize=14)\n",
        "    ax2.legend(loc='upper right', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"----- ----- -----\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yXSF6-WituD"
      },
      "source": [
        "#### Generate the data and label, and train and eval (hierarchy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC0Ul-KtbrG4",
        "outputId": "ae40a105-9ac9-42bc-dd2e-ce2c1288e3c7"
      },
      "outputs": [],
      "source": [
        "median_value = np.median(valid_dis_list)\n",
        "larger_half_indexes = []\n",
        "for i, value in enumerate(valid_dis_list):\n",
        "    if value >= median_value:\n",
        "        larger_half_indexes.append(i)\n",
        "print(np.array(larger_half_indexes).shape)\n",
        "all_indexes = list(range(len(valid_dis_list)))\n",
        "smaller_half_indexes = list(set(all_indexes) - set(larger_half_indexes))\n",
        "# print(larger_half_indexes)\n",
        "# print(smaller_half_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SmLSIoopi5i7",
        "outputId": "4582d2b8-7d76-410a-e15c-edae8133e631"
      },
      "outputs": [],
      "source": [
        "signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "                          \"Mean Restored Signal Traces\",\n",
        "                          \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "list_of_train_loss_lists = []\n",
        "list_of_test_loss_lists = []\n",
        "\n",
        "for signal_trace_type in signal_trace_type_list:\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        data_set = green_signal_traces.copy()\n",
        "        label_set = red_signal_traces.copy()\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored.copy()\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        data_set = green_signal_traces_mean_restored_std_restored.copy()\n",
        "        label_set = red_signal_traces_mean_restored_std_restored.copy()\n",
        "\n",
        "    data_set = data_set[:,valid_com_index_list,:]\n",
        "\n",
        "    print(f\"data_set.shape: {data_set.shape}\")\n",
        "    print(f\"label_set.shape: {label_set.shape}\")\n",
        "\n",
        "    # Normalize data_set to [-1,1]\n",
        "    data_set_min = np.min(data_set)\n",
        "    data_set_max = np.max(data_set)\n",
        "    data_set = ((data_set - data_set_min) / (data_set_max - data_set_min) - 0.5) * 2\n",
        "\n",
        "    # Normalize label_set to [-1,1]\n",
        "    label_set_min = np.min(label_set)\n",
        "    label_set_max = np.max(label_set)\n",
        "    label_set = ((label_set - label_set_min) / (label_set_max - label_set_min) - 0.5) * 2\n",
        "\n",
        "    # Define dataset class\n",
        "    class FluoDataset(Dataset):\n",
        "        def __init__(self, data_set, label_set, z_indices, x_indices):\n",
        "            self.data_set = data_set\n",
        "            self.label_set = label_set\n",
        "            self.z_indices = z_indices\n",
        "            self.x_indices = x_indices\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.z_indices)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            z = self.z_indices[idx]\n",
        "            x = self.x_indices[idx]\n",
        "\n",
        "            input_data = self.data_set[z, :, x-62:x]\n",
        "            target_label = np.mean(self.label_set[z, :, x-31:x])\n",
        "\n",
        "            return torch.tensor(input_data, dtype=torch.float32), torch.tensor(target_label, dtype=torch.float32)\n",
        "            # or torch.from_numpy(input_data).float(), torch.from_numpy(target_label).float()\n",
        "\n",
        "    # Set a random seed for reproducibility\n",
        "    # np.random.seed(16)  # can use any integer as the seed value\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    length = 6400*2\n",
        "    choices = range(3)\n",
        "    # choices = range(2) # delete last run\n",
        "    train_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    choices = list(range(62, 10000)) + list(range(12000, 17000)) + list(range(18244, 32500)) # 90% for train\n",
        "    # 10000-62+17000-12000+32500-18244=29194, 29194/(32500-62)=90%, 32500-62 is the all data pieces\n",
        "    train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640\n",
        "    choices = range(3)\n",
        "    # choices = range(2) # delete last run\n",
        "    test_z_indices = np.random.choice(choices, size=length)\n",
        "    # choices = range(62, 30000)\n",
        "    # choices = range(30000, 32500)\n",
        "    choices = list(range(10000, 12000)) + list(range(17000, 18244)) # 10% for test\n",
        "    # 12000-10000+18244-17000=3244, 3244/(32500-62)=10%, 32500-62 is the all data pieces\n",
        "    test_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    length = 640 # this is a subset of train data for test on train\n",
        "    choices = range(3)\n",
        "    # choices = range(2) # delete last run\n",
        "    small_train_z_indices = np.random.choice(choices, size=length)\n",
        "    choices = list(range(9000, 10000)) + list(range(12000, 17000)) + list(range(18244, 19000))\n",
        "    small_train_x_indices = np.random.choice(choices, size=length)\n",
        "\n",
        "    train_dataset = FluoDataset(data_set, label_set, train_z_indices, train_x_indices)\n",
        "    test_dataset = FluoDataset(data_set, label_set, test_z_indices, test_x_indices)\n",
        "    test_on_train_dataset = FluoDataset(data_set, label_set, small_train_z_indices, small_train_x_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_on_train_loader = DataLoader(test_on_train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # print(len(train_loader))\n",
        "    # print(len(test_loader))\n",
        "\n",
        "\n",
        "    # Define the model\n",
        "    class FluoModel(nn.Module):\n",
        "        def __init__(self, component_num, fisrt_branch_indexes):\n",
        "            super(FluoModel, self).__init__()\n",
        "            self.fc_shared = nn.Linear(62, 1)  # Shared fully connected layer\n",
        "            self.fc_reduce_2nd_branch = nn.Linear(component_num - len(fisrt_branch_indexes), 1)\n",
        "            self.fc_reduce_1st_branch = nn.Linear(len(fisrt_branch_indexes) + 1, 1)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "            self.fc_end = nn.Linear(1, 1)\n",
        "\n",
        "            all_indexes = list(range(component_num))\n",
        "            self.second_branch_indexes = list(set(all_indexes) - set(fisrt_branch_indexes))\n",
        "            self.first_branch_indexes = fisrt_branch_indexes\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1, 62)  # Reshape to (batch_size, component_num, 62)\n",
        "\n",
        "            # Apply the shared fully connected layer along the last dimension (62)\n",
        "            shared_output = self.fc_shared(x).squeeze(2)\n",
        "            # self.fc_shared(x) shape is (batch_size, component_num, 1), then squeeze the last dimension\n",
        "            # In PyTorch, when you apply a fully connected layer (or any other linear\n",
        "            # transformation) to a 3D tensor, by default, the operation is performed\n",
        "            # along the last dimension of the tensor.\n",
        "\n",
        "            # Reduce component_num (e.g., 281) channels to 1 using a separate fully connected layer\n",
        "            reduced_output_1 = self.fc_reduce_2nd_branch(shared_output[:,self.second_branch_indexes])\n",
        "            reduced_output_1_ = torch.cat((shared_output[:,self.first_branch_indexes], reduced_output_1), dim=1)\n",
        "            reduced_output_2 = self.fc_reduce_1st_branch(reduced_output_1_)\n",
        "            pre_output = self.sigmoid(reduced_output_2)\n",
        "            output = self.fc_end(pre_output)\n",
        "\n",
        "            return output\n",
        "\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    for i_ in range(30): # try 8 times to get the best one\n",
        "        model = FluoModel(data_set.shape[1], larger_half_indexes)\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "        # optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "        # scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 90\n",
        "        print_interval = 10\n",
        "\n",
        "        train_loss_list_ = []\n",
        "        test_loss_list_ = []\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # scheduler.step()\n",
        "\n",
        "            # if epoch < 5 or (epoch + 1) % print_interval == 0:\n",
        "            #     print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {loss.item():.6f}\")\n",
        "\n",
        "            # Testing on train and test sets\n",
        "            model.eval()\n",
        "\n",
        "            train_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in test_on_train_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    train_loss += criterion(outputs.squeeze(), targets).item()\n",
        "\n",
        "            average_train_loss = train_loss / len(test_on_train_dataset)\n",
        "            train_loss_list_.append(average_train_loss)\n",
        "\n",
        "            test_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in test_loader:\n",
        "                    outputs = model(inputs)\n",
        "                    test_loss += criterion(outputs.squeeze(), targets).item()\n",
        "\n",
        "            average_test_loss = test_loss / len(test_loader)\n",
        "            if epoch == 5 and average_test_loss >= 0.9 * np.mean(np.array(test_loss_list_)[1:]):\n",
        "                break; # kill trials with a low probability of convergence (terminate trials that are unlikely to converge)\n",
        "            test_loss_list_.append(average_test_loss)\n",
        "\n",
        "            if epoch < 5 or (epoch + 1) % print_interval == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {average_train_loss:.6f} | Test Loss: {average_test_loss:.6f}\")\n",
        "\n",
        "        if i_ == 0:\n",
        "            train_loss_list, test_loss_list = train_loss_list_, test_loss_list_\n",
        "            model_final = model\n",
        "        elif test_loss_list_[-1] < test_loss_list[-1]:\n",
        "            train_loss_list, test_loss_list = train_loss_list_, test_loss_list_\n",
        "            model_final = model\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(train_loss_list, color = 'skyblue', label='Train Loss', linewidth=2)\n",
        "    plt.plot(test_loss_list, color = 'salmon', label='Test Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch Index', fontsize=16)\n",
        "    plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Train and Test Loss Curves\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    list_of_train_loss_lists.append(train_loss_list)\n",
        "    list_of_test_loss_lists.append(test_loss_list)\n",
        "\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_raw_2level.pth\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_2level.pth\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        model_path = f\"./{cell_name}_model_with_mean_restored_std_restored_2level.pth\"\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        torch.save(model_final.state_dict(), f)\n",
        "\n",
        "    model = FluoModel(data_set.shape[1], larger_half_indexes)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "    outputs_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            outputs_list.append(outputs.numpy())\n",
        "            labels_list.append(targets.numpy())\n",
        "\n",
        "    outputs_array = np.concatenate(outputs_list)\n",
        "    labels_array = np.concatenate(labels_list)\n",
        "\n",
        "    # Sort labels and corresponding outputs\n",
        "    sorted_indices = np.argsort(labels_array)\n",
        "    sorted_labels = labels_array[sorted_indices]\n",
        "    sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.plot(sorted_labels, color = 'skyblue', label='True Labels', linewidth=2)\n",
        "    plt.plot(sorted_outputs, color = 'salmon', label='Model Outputs', linewidth=2)\n",
        "    plt.xlabel('Sample Index', fontsize=16)\n",
        "    plt.ylabel('Normalized Value', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Comparison between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Calculate the correlation coefficient\n",
        "    correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "    # plot results\n",
        "    plt.figure(figsize=(6.4, 4.8))\n",
        "    plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "    plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "    plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "    plt.tick_params(labelsize=14)\n",
        "    plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "    plt.legend(fontsize=14, facecolor='none')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UftitvO-iiTL",
        "outputId": "394ead91-4021-4062-b5c1-dbec56b7a57e"
      },
      "outputs": [],
      "source": [
        "# compare train losses and compare test loss between different types of signal traces\n",
        "\n",
        "colors_list = [('deepskyblue', 'orangered'), ('royalblue', 'salmon'), ('cornflowerblue', 'violet')]\n",
        "linestyle_list = ['dashed', 'dotted', 'solid']\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(train_loss_list, color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "    # plt.plot(train_loss_list[-10:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Train Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(test_loss_list, color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "    # plt.plot(test_loss_list[-10:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Test Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plot last epochs\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(np.arange(len(train_loss_list)+1-50, len(train_loss_list)+1), train_loss_list[-50:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "    # plt.plot(train_loss_list[-10:], color = colors[0], linestyle = linestyle, label=f'Train Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Train Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6.4, 4.8))\n",
        "for (signal_trace_type, train_loss_list, test_loss_list, colors, linestyle) in zip(\n",
        "    signal_trace_type_list, list_of_train_loss_lists, list_of_test_loss_lists,\n",
        "    colors_list, linestyle_list\n",
        "):\n",
        "    if signal_trace_type == \"Raw Signal Traces\":\n",
        "        type_ = \"(Raw)\"\n",
        "    elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "        type_ = \"(Mean Restored)\"\n",
        "    elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "        type_ = \"(Mean and Std Restored)\"\n",
        "    plt.plot(np.arange(len(test_loss_list)+1-50, len(test_loss_list)+1), test_loss_list[-50:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "    # plt.plot(test_loss_list[-10:], color = colors[1], linestyle = linestyle, label=f'Test Loss {type_}', linewidth=2)\n",
        "plt.xlabel('Epoch Index', fontsize=16)\n",
        "plt.ylabel('MSE Loss of Normalized Data', fontsize=16)\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title(f\"Test Loss Comparison Between Different Datasets\", fontsize=18)\n",
        "plt.legend(fontsize=14, facecolor='none')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNCQzEWrhT5G"
      },
      "outputs": [],
      "source": [
        "# signal_trace_type_list = [\"Raw Signal Traces\",\n",
        "#                           \"Mean Restored Signal Traces\",\n",
        "#                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# # signal_trace_type_list = [\"Mean Restored Signal Traces\",\n",
        "# #                           \"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# # signal_trace_type_list = [\"Raw Signal Traces\"]\n",
        "# # signal_trace_type_list = [\"Mean Restored Signal Traces\"]\n",
        "# # signal_trace_type_list = [\"Mean and Std Restored Signal Traces\"]\n",
        "\n",
        "# for signal_trace_type in signal_trace_type_list:\n",
        "#     if signal_trace_type == \"Raw Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_raw_2level.pth\"\n",
        "#     elif signal_trace_type == \"Mean Restored Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_mean_restored_2level.pth\"\n",
        "#     elif signal_trace_type == \"Mean and Std Restored Signal Traces\":\n",
        "#         model_path = f\"./{cell_name}_model_with_mean_restored_std_restored_2level.pth\"\n",
        "\n",
        "#     model = FluoModel(data_set.shape[1], larger_half_indexes)\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "#     model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "#     outputs_list = []\n",
        "#     labels_list = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, targets in test_loader:\n",
        "#             outputs = model(inputs)\n",
        "#             outputs_list.append(outputs.numpy())\n",
        "#             labels_list.append(targets.numpy())\n",
        "\n",
        "#     outputs_array = np.concatenate(outputs_list)\n",
        "#     labels_array = np.concatenate(labels_list)\n",
        "\n",
        "#     # Sort labels and corresponding outputs\n",
        "#     sorted_indices = np.argsort(labels_array)\n",
        "#     sorted_labels = labels_array[sorted_indices]\n",
        "#     sorted_outputs = outputs_array[sorted_indices]\n",
        "\n",
        "#     # Calculate the correlation coefficient\n",
        "#     correlation_coefficient = np.corrcoef(sorted_labels, np.transpose(sorted_outputs))[0, 1]\n",
        "\n",
        "#     # plot results\n",
        "#     plt.figure(figsize=(6.4, 4.8))\n",
        "#     plt.scatter(sorted_labels, sorted_outputs, color = 'salmon', label=f'Correlation Coefficient = {correlation_coefficient:.5f}')\n",
        "#     plt.xlabel('Normalized Value (Labels)', fontsize=16)\n",
        "#     plt.ylabel('Normalized Value (Outputs)', fontsize=16)\n",
        "#     plt.tick_params(labelsize=14)\n",
        "#     plt.title(f\"Scatter Plot between True Labels and Model Outputs\\n({signal_trace_type})\", fontsize=18)\n",
        "#     plt.legend(fontsize=14, facecolor='none')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CACwWD6_GJ_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHzR34BV_MKQ"
      },
      "source": [
        "## =================================== Here is a the separator =================================== (The following section is currently unused and is unlikely to be utilized in the future.) ==================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aP9BLlzgPO8"
      },
      "source": [
        "## Read pupil data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WEjERxLV6o3"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-zh6PXKgcJL"
      },
      "source": [
        "### Read pupil F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8OzCmjJno7R"
      },
      "outputs": [],
      "source": [
        "# fucntion for calculate pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "def calculate_means_by_ranges(arr): # input shape is (93, 1, 10)\n",
        "    range1_mean = np.mean(arr[0:31], axis=0)\n",
        "    range2_mean = np.mean(arr[31:62], axis=0)\n",
        "    range3_mean = np.mean(arr[62:93], axis=0)\n",
        "\n",
        "    # Transpose\n",
        "    range1_mean = np.transpose(range1_mean)\n",
        "    range2_mean = np.transpose(range2_mean)\n",
        "    range3_mean = np.transpose(range3_mean)\n",
        "\n",
        "    # Combine means into a 10x3 array\n",
        "    combined_means = np.concatenate((range1_mean, range2_mean, range3_mean), axis=1)\n",
        "\n",
        "    return combined_means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ZOOIv4y3t-"
      },
      "source": [
        "#### sum/mean along conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "lf6g2qaLkgTv",
        "outputId": "d58067f7-a12f-474d-ec86-3f26f21c4dfb"
      },
      "outputs": [],
      "source": [
        "### read dFF values for pupil size for each run (round)\n",
        "\n",
        "## run a\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[0] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run4 = mat_data['Master_f']\n",
        "pupil_area_run4 = pupil_area_run4[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run4):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run4\n",
        "    pupil_area_run4[index] = value # change original pupil_area_run4\n",
        "# print(pupil_area_run4.shape)\n",
        "# print(type(pupil_area_run4))\n",
        "# print(type(pupil_area_run4[0,1]))\n",
        "# print(pupil_area_run4[0,1].shape)\n",
        "\n",
        "# sum and average red data to see the decay\n",
        "sum_mat_pupil_run4 = np.zeros((pupil_area_run4.shape[0], 1), dtype=np.ndarray)\n",
        "mean_mat_pupil_run4 = np.zeros((pupil_area_run4.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(pupil_area_run4.shape[0]):\n",
        "    sum = np.zeros((pupil_area_run4[0,0].shape[0], 1))\n",
        "    for j in range(pupil_area_run4.shape[1]):\n",
        "        sum = sum + pupil_area_run4[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_pupil_run4[i, 0] = sum\n",
        "    mean_mat_pupil_run4[i, 0] = sum / pupil_area_run4.shape[1]\n",
        "\n",
        "\n",
        "## run b\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[1] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run5 = mat_data['Master_f']\n",
        "pupil_area_run5 = pupil_area_run5[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run5):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run5\n",
        "    pupil_area_run5[index] = value # change original pupil_area_run5\n",
        "# print(pupil_area_run5.shape)\n",
        "# print(type(pupil_area_run5))\n",
        "# print(type(pupil_area_run5[0,1]))\n",
        "# print(pupil_area_run5[0,1].shape)\n",
        "\n",
        "# sum and average red data to see the decay\n",
        "sum_mat_pupil_run5 = np.zeros((pupil_area_run5.shape[0], 1), dtype=np.ndarray)\n",
        "mean_mat_pupil_run5 = np.zeros((pupil_area_run5.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(pupil_area_run5.shape[0]):\n",
        "    sum = np.zeros((pupil_area_run5[0,0].shape[0], 1))\n",
        "    for j in range(pupil_area_run5.shape[1]):\n",
        "        sum = sum + pupil_area_run5[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_pupil_run5[i, 0] = sum\n",
        "    mean_mat_pupil_run5[i, 0] = sum / pupil_area_run5.shape[1]\n",
        "\n",
        "\n",
        "## run c\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[2] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run6 = mat_data['Master_f']\n",
        "pupil_area_run6 = pupil_area_run6[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run6):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run6\n",
        "    pupil_area_run6[index] = value # change original pupil_area_run6\n",
        "# print(pupil_area_run6.shape)\n",
        "# print(type(pupil_area_run6))\n",
        "# print(type(pupil_area_run6[0,1]))\n",
        "# print(pupil_area_run6[0,1].shape)\n",
        "\n",
        "# sum and average red data to see the decay\n",
        "sum_mat_pupil_run6 = np.zeros((pupil_area_run6.shape[0], 1), dtype=np.ndarray)\n",
        "mean_mat_pupil_run6 = np.zeros((pupil_area_run6.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(pupil_area_run6.shape[0]):\n",
        "    sum = np.zeros((pupil_area_run6[0,0].shape[0], 1))\n",
        "    for j in range(pupil_area_run6.shape[1]):\n",
        "        sum = sum + pupil_area_run6[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_pupil_run6[i, 0] = sum\n",
        "    mean_mat_pupil_run6[i, 0] = sum / pupil_area_run6.shape[1]\n",
        "\n",
        "\n",
        "# print(sum_mat_pupil_run4)\n",
        "# print(sum_mat_pupil_run5)\n",
        "# print(sum_mat_pupil_run6)\n",
        "sum_mat_pupil = np.concatenate((sum_mat_pupil_run4[0,0], sum_mat_pupil_run5[0,0], sum_mat_pupil_run6[0,0]), axis=0)\n",
        "mean_mat_pupil = np.concatenate((mean_mat_pupil_run4[0,0], mean_mat_pupil_run5[0,0], mean_mat_pupil_run6[0,0]), axis=0)\n",
        "# print(sum_mat_pupil.T)\n",
        "# print(mean_mat_pupil.T)\n",
        "\n",
        "\n",
        "# plot\n",
        "data = mean_mat_pupil\n",
        "x = np.arange(1, 31)\n",
        "fig, ax = plt.subplots(figsize=(8, 4))  # Adjust the figure size as desired\n",
        "ax.plot(x,data[:, 0], label='Pre-stimulation (2s)')\n",
        "ax.plot(x,data[:, 1], label='During stimulation (2s)')\n",
        "ax.plot(x,data[:, 2], label='Post-stimulation (2s)')\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Repeat', fontsize=12)  # Adjust the label size as desired\n",
        "ax.set_ylabel('Pupil size in F', fontsize=12)  # Adjust the label size as desired\n",
        "ax.axvline(x=10.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=20.5, color='orange', linestyle='--')\n",
        "# ax.text(5, 7200, 'Round 4', ha='center', va='center', fontsize=12) # need to change coordinates specificly\n",
        "# ax.text(15.5, 7200, 'Round 5', ha='center', va='center', fontsize=12)\n",
        "# ax.text(25.5, 7200, 'Round 6', ha='center', va='center', fontsize=12)\n",
        "ax.set_title('Pupil size curve (mean of conditions)', fontsize=14)  # Adjust the title size as desired\n",
        "ax.tick_params(axis='both', labelsize=10)  # Adjust the tick size as desired\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWT5hizbzDv1"
      },
      "source": [
        "#### sum/mean along repeats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "tYfsoB8WzFUs",
        "outputId": "6127b846-2855-4ea2-bfe4-06de7a532801"
      },
      "outputs": [],
      "source": [
        "### read dFF values for pupil size for each run (round)\n",
        "\n",
        "## run 4\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[0] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run4 = mat_data['Master_f']\n",
        "pupil_area_run4 = pupil_area_run4[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run4):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run4\n",
        "    pupil_area_run4[index] = value # change original pupil_area_run4\n",
        "# print(pupil_area_run4.shape)\n",
        "# print(type(pupil_area_run4))\n",
        "# print(type(pupil_area_run4[0,1]))\n",
        "# print(pupil_area_run4[0,1].shape)\n",
        "\n",
        "## run 5\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[1] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run5 = mat_data['Master_f']\n",
        "pupil_area_run5 = pupil_area_run5[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run5):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run5\n",
        "    pupil_area_run5[index] = value # change original pupil_area_run5\n",
        "# print(pupil_area_run5.shape)\n",
        "# print(type(pupil_area_run5))\n",
        "# print(type(pupil_area_run5[0,1]))\n",
        "# print(pupil_area_run5[0,1].shape)\n",
        "\n",
        "## run 6\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[2] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run6 = mat_data['Master_f']\n",
        "pupil_area_run6 = pupil_area_run6[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run6):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run6\n",
        "    pupil_area_run6[index] = value # change original pupil_area_run6\n",
        "# print(pupil_area_run6.shape)\n",
        "# print(type(pupil_area_run6))\n",
        "# print(type(pupil_area_run6[0,1]))\n",
        "# print(pupil_area_run6[0,1].shape)\n",
        "\n",
        "# stack/concatenate all conditions\n",
        "\n",
        "# satstackck elements in pupil_area_run4 along the 0th axis\n",
        "concatenated_run4 = np.stack(pupil_area_run4[0,:], axis=2)\n",
        "# stack elements in pupil_area_run5 along the 0th axis\n",
        "concatenated_run5 = np.stack(pupil_area_run5[0,:], axis=2)\n",
        "# stack elements in pupil_area_run6 along the 0th axis\n",
        "concatenated_run6 = np.stack(pupil_area_run6[0,:], axis=2)\n",
        "# Concatenate the results from all three runs along the 1st axis\n",
        "result = np.concatenate((concatenated_run4, concatenated_run5, concatenated_run6), axis=0)\n",
        "# print(concatenated_run4.shape)\n",
        "# print(result.shape)\n",
        "\n",
        "\n",
        "# plot\n",
        "data = np.mean(result, axis=0)\n",
        "x = np.arange(1, 49)\n",
        "fig, ax = plt.subplots(figsize=(8, 4))  # Adjust the figure size as desired\n",
        "ax.plot(x,data[0, :], label='Pre-stimulation (2s)')\n",
        "ax.plot(x,data[1, :], label='During stimulation (2s)')\n",
        "ax.plot(x,data[2, :], label='Post-stimulation (2s)')\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Conditions', fontsize=12)  # Adjust the label size as desired\n",
        "ax.set_ylabel('Pupil size in F', fontsize=12)  # Adjust the label size as desired\n",
        "ax.axvline(x=6.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=12.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=18.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=24.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=30.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=36.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=42.5, color='orange', linestyle='--')\n",
        "ax.set_title('Pupil size curve (mean of repeats)', fontsize=14)  # Adjust the title size as desired\n",
        "ax.tick_params(axis='both', labelsize=10)  # Adjust the tick size as desired\n",
        "# Customize x-axis ticks\n",
        "tick_positions = [1,5,10,15,20,25,30,35,40,45,48]\n",
        "plt.xticks(tick_positions)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5lNq5y9x1yu"
      },
      "source": [
        "#### mean for each (condition, repeat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-xGmksoyBbs",
        "outputId": "e76c8f06-96c8-4d87-c0e0-067af1cfa20b"
      },
      "outputs": [],
      "source": [
        "### read dFF values for pupil size for each run (round)\n",
        "\n",
        "## run 4\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[0] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run4 = mat_data['Master_f']\n",
        "pupil_area_run4 = pupil_area_run4[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run4):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run4\n",
        "    pupil_area_run4[index] = value # change original pupil_area_run4\n",
        "# print(pupil_area_run4.shape)\n",
        "# print(type(pupil_area_run4))\n",
        "# print(type(pupil_area_run4[0,1]))\n",
        "# print(pupil_area_run4[0,1].shape)\n",
        "fine_mean_mat_pupil_run4 = copy.deepcopy(pupil_area_run4)\n",
        "\n",
        "## run 5\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[1] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run5 = mat_data['Master_f']\n",
        "pupil_area_run5 = pupil_area_run5[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run5):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run5\n",
        "    pupil_area_run5[index] = value # change original pupil_area_run5\n",
        "# print(pupil_area_run5.shape)\n",
        "# print(type(pupil_area_run5))\n",
        "# print(type(pupil_area_run5[0,1]))\n",
        "# print(pupil_area_run5[0,1].shape)\n",
        "fine_mean_mat_pupil_run5 = copy.deepcopy(pupil_area_run5)\n",
        "\n",
        "## run 6\n",
        "\n",
        "# read and calculate the pre 2s mean, stimulation 2s mean, and post 2s mean\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[2] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run6 = mat_data['Master_f']\n",
        "pupil_area_run6 = pupil_area_run6[:, :-1] # delete the last column (49th)\n",
        "for index, value in np.ndenumerate(pupil_area_run6):\n",
        "    value = calculate_means_by_ranges(value) # here, value is a copy, so changing value will not change pupil_area_run6\n",
        "    pupil_area_run6[index] = value # change original pupil_area_run6\n",
        "# print(pupil_area_run6.shape)\n",
        "# print(type(pupil_area_run6))\n",
        "# print(type(pupil_area_run6[0,1]))\n",
        "# print(pupil_area_run6[0,1].shape)\n",
        "fine_mean_mat_pupil_run6 = copy.deepcopy(pupil_area_run6)\n",
        "\n",
        "# concatenate\n",
        "fine_mean_mat_pupil = np.concatenate((fine_mean_mat_pupil_run4,\n",
        "                    fine_mean_mat_pupil_run5,\n",
        "                    fine_mean_mat_pupil_run6),\n",
        "                    axis = 0)\n",
        "print(fine_mean_mat_pupil.shape)\n",
        "print(fine_mean_mat_pupil[0,1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbhfm0N7gVAs"
      },
      "source": [
        "### Read pupil dFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "fX22DT1hnreh",
        "outputId": "671c3c2b-1abb-48c8-82a4-e4caeb450401"
      },
      "outputs": [],
      "source": [
        "### read dFF values for pupil size for each run (round)\n",
        "\n",
        "# run 4\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[0] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run4 = mat_data['Master_dFF_mean']\n",
        "pupil_area_run4 = pupil_area_run4[:, :-1]\n",
        "print(type(pupil_area_run4[0,1]))\n",
        "print(pupil_area_run4[0,1].shape)\n",
        "# sum red data to see the decay\n",
        "sum_mat_pupil_run4 = np.zeros((pupil_area_run4.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(pupil_area_run4.shape[0]):\n",
        "    sum = np.zeros((pupil_area_run4[0,0].shape[0], 1))\n",
        "    for j in range(pupil_area_run4.shape[1]):\n",
        "        sum = sum + pupil_area_run4[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_pupil_run4[i, 0] = sum\n",
        "\n",
        "# run 5\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[1] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run5 = mat_data['Master_dFF_mean']\n",
        "pupil_area_run5 = pupil_area_run5[:, :-1]\n",
        "print(type(pupil_area_run5[0,1]))\n",
        "print(pupil_area_run5[0,1].shape)\n",
        "# sum red data to see the decay\n",
        "sum_mat_pupil_run5 = np.zeros((pupil_area_run5.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(pupil_area_run5.shape[0]):\n",
        "    sum = np.zeros((pupil_area_run5[0,0].shape[0], 1))\n",
        "    for j in range(pupil_area_run5.shape[1]):\n",
        "        sum = sum + pupil_area_run5[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_pupil_run5[i, 0] = sum\n",
        "\n",
        "# run 6\n",
        "path_ = os.path.join(root_path, cell_name, cell_name + 'green_PupilArea_Run' + run_num[2] + '.mat')\n",
        "mat_data = scipy.io.loadmat(path_)\n",
        "pupil_area_run6 = mat_data['Master_dFF_mean']\n",
        "pupil_area_run6 = pupil_area_run6[:, :-1]\n",
        "print(type(pupil_area_run6[0,1]))\n",
        "print(pupil_area_run6[0,1].shape)\n",
        "# sum red data to see the decay\n",
        "sum_mat_pupil_run6 = np.zeros((pupil_area_run6.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(pupil_area_run6.shape[0]):\n",
        "    sum = np.zeros((pupil_area_run6[0,0].shape[0], 1))\n",
        "    for j in range(pupil_area_run6.shape[1]):\n",
        "        sum = sum + pupil_area_run6[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_pupil_run6[i, 0] = sum\n",
        "\n",
        "\n",
        "# print(sum_mat_pupil_run4)\n",
        "# print(sum_mat_pupil_run5)\n",
        "# print(sum_mat_pupil_run6)\n",
        "sum_mat_pupil = np.concatenate((sum_mat_pupil_run4[0,0], sum_mat_pupil_run5[0,0], sum_mat_pupil_run6[0,0]), axis=0)\n",
        "print(sum_mat_pupil.T)\n",
        "\n",
        "\n",
        "data = sum_mat_pupil\n",
        "x = np.arange(1, 31)\n",
        "fig, ax = plt.subplots(figsize=(8, 4))  # Adjust the figure size as desired\n",
        "ax.plot(x, data)\n",
        "ax.set_xlabel('Repeat', fontsize=12)  # Adjust the label size as desired\n",
        "ax.set_ylabel('Pupil size in dFF', fontsize=12)  # Adjust the label size as desired\n",
        "ax.axvline(x=10.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=20.5, color='orange', linestyle='--')\n",
        "ax.text(5, -1.6, 'Round 4', ha='center', va='center', fontsize=12)\n",
        "ax.text(15.5, -1.6, 'Round 5', ha='center', va='center', fontsize=12)\n",
        "ax.text(25.5, -1.6, 'Round 6', ha='center', va='center', fontsize=12)\n",
        "ax.set_title('Pupil size curve', fontsize=14)  # Adjust the title size as desired\n",
        "ax.tick_params(axis='both', labelsize=10)  # Adjust the tick size as desired\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKQzJzxGetqT"
      },
      "source": [
        "## Read fluorescence dFF mean data into dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-epCndVKP88F",
        "outputId": "fabc8670-b024-416a-f0f2-c0ffeb6cc582"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/Fluorescence_Data/FluoData4Fitting_Average\"\n",
        "\n",
        "# Get a list of all the subdirectories: subfolders are viewed as cell names\n",
        "cell_names = [f for f in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, f))]\n",
        "# for cell in cell_names:\n",
        "#     print(cell)\n",
        "\n",
        "# Create a dictionary with default values\n",
        "default_value = 0\n",
        "cell_data_dict = {cell: default_value for cell in cell_names}\n",
        "# print(cell_data_dict)\n",
        "\n",
        "file_suffixes = ['green_Axon.mat', 'green_dFFMeanValues.mat', 'red_dFFMeanValues.mat']\n",
        "\n",
        "for cell in cell_names:\n",
        "    print(cell)\n",
        "    file_names = [cell + suffix for suffix in file_suffixes]\n",
        "\n",
        "    path_ = file_names[0] # green_Axon.mat\n",
        "    path_ = os.path.join(root_path, cell, path_)\n",
        "    mat_data = scipy.io.loadmat(path_)\n",
        "    axons = mat_data['Axons'] # array containing nested arrays/sub-arrays\n",
        "    # Squeeze the outer array\n",
        "    axons = np.squeeze(axons, axis=0)\n",
        "    for i in range(len(axons)):\n",
        "        # Squeeze the inner array and convert the data type to 'int'\n",
        "        axons[i] = np.squeeze(axons[i].astype(int), axis=0)\n",
        "    # final axons' length is the number of groups with\n",
        "    # each elements being a nested array of components\n",
        "\n",
        "    path_ = file_names[1] # green_dFFMeanValues.mat\n",
        "    path_ = os.path.join(root_path, cell, path_)\n",
        "    mat_data = scipy.io.loadmat(path_)\n",
        "    dFFMeanValues_green = mat_data['dFFMeanValues'] # 3 by 49\n",
        "\n",
        "    path_ = file_names[2] # red_dFFMeanValues.mat\n",
        "    path_ = os.path.join(root_path, cell, path_)\n",
        "    mat_data = scipy.io.loadmat(path_)\n",
        "    dFFMeanValues_red = mat_data['dFFMeanValues'] # 3 by 49\n",
        "\n",
        "    cell_data_dict[cell] = {'axons': axons,\n",
        "                'green_dFFMeanValues': dFFMeanValues_green,\n",
        "                'red_dFFMeanValues': dFFMeanValues_red}\n",
        "\n",
        "# # Print keys and types\n",
        "# for key, value in cell_data_dict.items():\n",
        "#     print(\"-- * * * * * --\")\n",
        "#     print(key, type(value))\n",
        "#     for key_, value_ in value.items():\n",
        "#         print(key_, type(value_))\n",
        "# print(\"-- * * * * * --\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1aYXRxYe1h8"
      },
      "source": [
        "## Decay restoration of red and trend visualization (including setting whether considering pupil size in regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUoPnkOa8vm0"
      },
      "source": [
        "In the following chunks, the code\n",
        "\n",
        "\n",
        "```\n",
        "# at most one of the following options is True\n",
        "mean_pupil_size_along_conditions_multiply = False # mean\n",
        "mean_pupil_size_each_condition_each_repeat_multiply = True # fine\n",
        "```\n",
        "\n",
        "are used for setting how to considering pupile size in regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzk1ddNePYCp"
      },
      "outputs": [],
      "source": [
        "# at most one of the following options is True\n",
        "mean_pupil_size_along_conditions_multiply = False # mean\n",
        "mean_pupil_size_each_condition_each_repeat_multiply = True # fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "yMCwzb_3RhAg",
        "outputId": "7eadbeb5-d1e8-4104-c94e-a5623c9b71a1"
      },
      "outputs": [],
      "source": [
        "cell_data = copy.deepcopy(cell_data_dict[cell_name])\n",
        "# cell_data = copy.deepcopy(cell_data_dict['CL075_230303'])\n",
        "# cell_data = cell_data_dict['CL090_230515']\n",
        "# cell_data = cell_data_dict['CL075_230303']\n",
        "\n",
        "# # the following 4 lines of code used for checking if cell_data and\n",
        "# # cell_data_dict changed after running the code of this chunk; compare\n",
        "# # with the printing at the end\n",
        "# print(cell_data['red_dFFMeanValues'][1,1][0,-1])\n",
        "# print(cell_data['green_dFFMeanValues'][1,1][0,-1])\n",
        "# print(cell_data_dict['CL090_230515']['red_dFFMeanValues'][1,1][0,-1])\n",
        "# print(cell_data_dict['CL090_230515']['green_dFFMeanValues'][1,1][0,-1])\n",
        "\n",
        "\n",
        "delete_small_group = True # delete groups (axons) with less than 3 components\n",
        "\n",
        "data_green = cell_data['green_dFFMeanValues'][:,:-1] # exclude 49th column\n",
        "data_red = cell_data['red_dFFMeanValues'][:,:-1] # exclude 49th column\n",
        "# data_red = cell_data['red_dFFMeanValues'][:,:-1].copy() # exclude 49th column\n",
        "# the .copy() method is for a NumPy array. Using .copy() on a NumPy array,\n",
        "# it creates a new copied array but it is a separate and independent object.\n",
        "# Modifying the copied array will not affect the original array.\n",
        "# # here uses .copy() to avoid cell_data and cell_data['red_dFFMeanValues'] be changed in\n",
        "# # the later code line data_red[i,j] = red_recover_factor[i,0] * data_red[i,j], or we can use\n",
        "# # cell_data = copy.deepcopy(cell_data_dict['CL090_230515']) to guarantee cell_data['red_dFFMeanValues']\n",
        "# # will not be changed (cell_data will be changed).\n",
        "# # copy.deepcopy will recursively copy all nested objects; copy.copy cannot do this.\n",
        "data_axons = cell_data['axons']\n",
        "\n",
        "# sum red data to see the decay\n",
        "sum_mat_red = np.zeros((data_red.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(data_red.shape[0]):\n",
        "    sum = np.zeros((data_red[0,0].shape[0], 1))\n",
        "    for j in range(data_red.shape[1]):\n",
        "        sum = sum + data_red[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_red[i, 0] = sum\n",
        "# print(\"sum_mat_red:\", sum_mat_red)\n",
        "\n",
        "# Normalization of red\n",
        "sum_mat_red_n = np.zeros((data_red.shape[0], 1), dtype=np.ndarray)\n",
        "max_value = -np.inf\n",
        "for i in range(sum_mat_red.shape[0]):\n",
        "    for j in range(sum_mat_red.shape[1]):\n",
        "        max_value = max(max_value, np.max(np.abs(sum_mat_red[i, j])))\n",
        "for i in range(sum_mat_red.shape[0]):\n",
        "    for j in range(sum_mat_red.shape[1]):\n",
        "        element = sum_mat_red[i, j]\n",
        "        sum_mat_red_n[i, j] = element / max_value\n",
        "# print(sum_mat_red_n.shape)\n",
        "\n",
        "# vstack sum_mat_red_n\n",
        "stacked_sum_mat_red_n = np.empty((0, sum_mat_red_n[0,0].shape[1]))\n",
        "# Enumerate the elements in the np array and vstack them\n",
        "for index, value in np.ndenumerate(sum_mat_red_n):\n",
        "    stacked_sum_mat_red_n = np.vstack((stacked_sum_mat_red_n, value))\n",
        "stacked_sum_mat_red_n = np.squeeze(stacked_sum_mat_red_n)\n",
        "# print(stacked_sum_mat_red_n.shape)\n",
        "\n",
        "\n",
        "# sum green data to see the decay\n",
        "sum_mat_green = np.zeros((data_green.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(data_green.shape[0]):\n",
        "    sum = np.zeros((data_green[0,0].shape[0], 1))\n",
        "    for j in range(data_green.shape[1]):\n",
        "        sum = sum + np.sum(data_green[i,j], axis=1, keepdims=True)\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_green[i, 0] = sum\n",
        "# print(sum_mat_green)\n",
        "\n",
        "# Normalization of green\n",
        "sum_mat_green_n = np.zeros((data_green.shape[0], 1), dtype=np.ndarray)\n",
        "max_value = -np.inf\n",
        "for i in range(sum_mat_green.shape[0]):\n",
        "    for j in range(sum_mat_green.shape[1]):\n",
        "        max_value = max(max_value, np.max(np.abs(sum_mat_green[i, j])))\n",
        "for i in range(sum_mat_green.shape[0]):\n",
        "    for j in range(sum_mat_green.shape[1]):\n",
        "        element = sum_mat_green[i, j]\n",
        "        sum_mat_green_n[i, j] = element / max_value\n",
        "# print(sum_mat_green_n.shape)\n",
        "\n",
        "# vstack sum_mat_green_n\n",
        "stacked_sum_mat_green_n = np.empty((0, sum_mat_green_n[0,0].shape[1]))\n",
        "# Enumerate the elements in the np array and vstack them\n",
        "for index, value in np.ndenumerate(sum_mat_green_n):\n",
        "    stacked_sum_mat_green_n = np.vstack((stacked_sum_mat_green_n, value))\n",
        "stacked_sum_mat_green_n = np.squeeze(stacked_sum_mat_green_n)\n",
        "# print(stacked_sum_mat_green_n.shape)\n",
        "\n",
        "\n",
        "# plot stacked_sum_mat_red_n and stacked_sum_mat_green_n\n",
        "x = np.arange(1, stacked_sum_mat_green_n.shape[0] + 1)\n",
        "fig, ax = plt.subplots(figsize=(8, 3))  # Adjust the figure size as desired\n",
        "ax.plot(x, stacked_sum_mat_red_n, color='red', label='Red data decay')\n",
        "ax.plot(x, stacked_sum_mat_green_n, color='green', label='Green data decay')\n",
        "ax.axvline(x=10.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=20.5, color='orange', linestyle='--')\n",
        "ax.text(5, 1, 'Round 4', ha='center', va='center', fontsize=12)\n",
        "ax.text(15.5, 1, 'Round 5', ha='center', va='center', fontsize=12)\n",
        "ax.text(25.5, 1, 'Round 6', ha='center', va='center', fontsize=12)\n",
        "ax.set_title('Decay curves of red and green data')\n",
        "ax.tick_params(axis='both', labelsize=12)  # Adjust the tick size as desired\n",
        "ax.set_xlabel('Repeat', fontsize=14)  # Adjust the label size as desired\n",
        "ax.set_ylabel('Normalized Value', fontsize=14)  # Adjust the label size as desired\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Define the exponential function for regression of decay\n",
        "def exponential_func(x, lambda_):\n",
        "    return np.exp(lambda_ * x)\n",
        "red_recover_factor = sum_mat_red_n.copy()\n",
        "for i in range(sum_mat_red_n.shape[0]):\n",
        "    for j in range(sum_mat_red_n.shape[1]): # sum_mat_red_n.shape[1] = 1\n",
        "        y = sum_mat_red_n[i, j].copy()\n",
        "        y = np.squeeze(y)\n",
        "        multiplier = 1 / y[0]\n",
        "        y = y * multiplier\n",
        "        x = np.arange(10)\n",
        "        popt, pcov = curve_fit(exponential_func, x, y)\n",
        "        lambda_ = popt[0]\n",
        "        print(f\"round {i}, exponent in the exp func is {lambda_}\")\n",
        "        y_fit = exponential_func(x, lambda_)\n",
        "        y = y.reshape(-1, 1)\n",
        "        y_fit = y_fit.reshape(-1, 1)\n",
        "        red_recover_factor[i, j] = (y / y_fit) * multiplier\n",
        "print(\"red_recover_factor.shape:\", red_recover_factor.shape)\n",
        "\n",
        "for i in range(data_red.shape[0]):\n",
        "    for j in range(data_red.shape[1]):\n",
        "        data_red[i,j] = red_recover_factor[i,0] * data_red[i,j]\n",
        "\n",
        "# sum red data to see the decay\n",
        "sum_mat_red = np.zeros((data_red.shape[0], 1), dtype=np.ndarray)\n",
        "for i in range(data_red.shape[0]):\n",
        "    sum = np.zeros((data_red[0,0].shape[0], 1))\n",
        "    for j in range(data_red.shape[1]):\n",
        "        sum = sum + data_red[i,j]\n",
        "    # print(np.sum(sum))\n",
        "    sum_mat_red[i, 0] = sum\n",
        "# print(sum_mat_red)\n",
        "\n",
        "# Normalization of red\n",
        "sum_mat_red_n = np.zeros((data_red.shape[0], 1), dtype=np.ndarray)\n",
        "max_value = -np.inf\n",
        "for i in range(sum_mat_red.shape[0]):\n",
        "    for j in range(sum_mat_red.shape[1]):\n",
        "        max_value = max(max_value, np.max(np.abs(sum_mat_red[i, j])))\n",
        "for i in range(sum_mat_red.shape[0]):\n",
        "    for j in range(sum_mat_red.shape[1]):\n",
        "        element = sum_mat_red[i, j]\n",
        "        sum_mat_red_n[i, j] = element / max_value\n",
        "# print(\"sum_mat_red_n.shape:\", sum_mat_red_n.shape)\n",
        "# print(\"sum_mat_red_n[0,0].shape:\", sum_mat_red_n[0,0].shape)\n",
        "\n",
        "# vstack sum_mat_red_n\n",
        "stacked_sum_mat_red_n = np.empty((0, sum_mat_red_n[0,0].shape[1]))\n",
        "# Enumerate the elements in the np array and vstack them\n",
        "for index, value in np.ndenumerate(sum_mat_red_n):\n",
        "    stacked_sum_mat_red_n = np.vstack((stacked_sum_mat_red_n, value))\n",
        "stacked_sum_mat_red_n = np.squeeze(stacked_sum_mat_red_n)\n",
        "# print(stacked_sum_mat_red_n.shape)\n",
        "\n",
        "\n",
        "# plot stacked_sum_mat_red_n and stacked_sum_mat_green_n\n",
        "x = np.arange(1, stacked_sum_mat_green_n.shape[0] + 1)\n",
        "fig, ax = plt.subplots(figsize=(8, 3))  # Adjust the figure size as desired\n",
        "ax.plot(x, stacked_sum_mat_red_n, color='red', label='Red data decay')\n",
        "ax.plot(x, stacked_sum_mat_green_n, color='green', label='Green data decay')\n",
        "ax.axvline(x=10.5, color='orange', linestyle='--')\n",
        "ax.axvline(x=20.5, color='orange', linestyle='--')\n",
        "ax.text(5, 1, 'Round 4', ha='center', va='center', fontsize=12)\n",
        "ax.text(15.5, 1, 'Round 5', ha='center', va='center', fontsize=12)\n",
        "ax.text(25.5, 1, 'Round 6', ha='center', va='center', fontsize=12)\n",
        "ax.set_title('Decay curves of red and green data (after recovering)')\n",
        "ax.tick_params(axis='both', labelsize=12)  # Adjust the tick size as desired\n",
        "ax.set_xlabel('Repeat', fontsize=14)  # Adjust the label size as desired\n",
        "ax.set_ylabel('Normalized Value', fontsize=14)  # Adjust the label size as desired\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "if mean_pupil_size_along_conditions_multiply:\n",
        "    for i in range(data_green.shape[0]):\n",
        "        if i == 0:\n",
        "            mul_mat = mean_mat_pupil[:10,[1]] / np.mean(mean_mat_pupil)\n",
        "        if i == 1:\n",
        "            mul_mat = mean_mat_pupil[10:20,[1]] / np.mean(mean_mat_pupil)\n",
        "        if i == 2:\n",
        "            mul_mat = mean_mat_pupil[20:30,[1]] / np.mean(mean_mat_pupil)\n",
        "        for j in range(data_green.shape[1]):\n",
        "            data_green[i,j] = data_green[i,j] * mul_mat\n",
        "if mean_pupil_size_each_condition_each_repeat_multiply:\n",
        "    for i in range(data_green.shape[0]):\n",
        "        for j in range(data_green.shape[1]):\n",
        "            data_green[i,j] = data_green[i,j] * fine_mean_mat_pupil[i,j][:,[1]] / np.mean(mean_mat_pupil)\n",
        "\n",
        "\n",
        "### the following is stacking opearion to make green and red data both 2-D\n",
        "### so that thery are prepared for later regression\n",
        "\n",
        "# print(data_axons)\n",
        "# print(type(data_axons),len(data_axons))\n",
        "if delete_small_group:\n",
        "    data_axons = np.array([axons_ for axons_ in data_axons if len(axons_) >= 3], dtype=object)\n",
        "# print(data_axons)\n",
        "# print(type(data_axons),len(data_axons))\n",
        "\n",
        "# vstack green data\n",
        "stacked_green = np.empty((0, data_green[0,0].shape[1]))\n",
        "# Enumerate the elements in the np array and vstack them\n",
        "for index, value in np.ndenumerate(data_green):\n",
        "    stacked_green = np.vstack((stacked_green, value))\n",
        "print(f\"stacked_green.shape is {stacked_green.shape} (compare with 48*3*10={48*3*10})\")\n",
        "\n",
        "# group columns of green data\n",
        "group_num = data_axons.shape[0]\n",
        "group_satcked_green = np.zeros((stacked_green.shape[0], group_num))\n",
        "for i, cols in enumerate(data_axons):\n",
        "    group_satcked_green[:, i] = np.sum(stacked_green[:, cols-1], axis=1)\n",
        "print(\"group_satcked_green.shape, data_axons.shape are:\", group_satcked_green.shape, data_axons.shape)\n",
        "\n",
        "# vstack red data\n",
        "stacked_red = np.empty((0, data_red[0,0].shape[1]))\n",
        "# Enumerate the elements in the np array and vstack them\n",
        "for index, value in np.ndenumerate(data_red):\n",
        "    stacked_red = np.vstack((stacked_red, value))\n",
        "print(f\"stacked_red.shape is {stacked_red.shape} (compare with 48*3*10={48*3*10})\")\n",
        "\n",
        "print(\"np.max(group_satcked_green), np.min(group_satcked_green) are:\", np.max(group_satcked_green), np.min(group_satcked_green))\n",
        "print(\"np.max(stacked_red), np.min(stacked_red) are:\", np.max(stacked_red), np.min(stacked_red))\n",
        "\n",
        "# # the following 4 lines of code used for checking if cell_data and\n",
        "# # cell_data_dict changed after running the code of this chunk; compare\n",
        "# # with the beginning printing\n",
        "# print(cell_data['red_dFFMeanValues'][1,1][0,-1])\n",
        "# print(cell_data['green_dFFMeanValues'][1,1][0,-1])\n",
        "# print(cell_data_dict['CL090_230515']['red_dFFMeanValues'][1,1][0,-1])\n",
        "# print(cell_data_dict['CL090_230515']['green_dFFMeanValues'][1,1][0,-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwVQE6jcU3TP"
      },
      "source": [
        "## Set for no decay restoration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFjSRx6dOk7s"
      },
      "outputs": [],
      "source": [
        "# if wants to use the decay_restoration = False situation, reset data_red and data_green\n",
        "decay_restoration = True\n",
        "if decay_restoration == False: # if decay_restoration = False situation, reset data_red and data_green\n",
        "    print(\"decay_restoration is False\")\n",
        "    cell_data = copy.deepcopy(cell_data_dict[cell_name])\n",
        "    data_green = cell_data['green_dFFMeanValues'][:,:-1] # exclude 49th column\n",
        "    data_red = cell_data['red_dFFMeanValues'][:,:-1] # exclude 49th column\n",
        "    data_axons = cell_data['axons']\n",
        "\n",
        "    ### the following is stacking opearion to make green and red data both 2-D\n",
        "    ### so that thery are prepared for later regression\n",
        "\n",
        "    # print(data_axons)\n",
        "    # print(type(data_axons),len(data_axons))\n",
        "    if delete_small_group:\n",
        "        data_axons = np.array([axons_ for axons_ in data_axons if len(axons_) >= 3], dtype=object)\n",
        "    # print(data_axons)\n",
        "    # print(type(data_axons),len(data_axons))\n",
        "\n",
        "    # vstack green data\n",
        "    stacked_green = np.empty((0, data_green[0,0].shape[1]))\n",
        "    # Enumerate the elements in the np array and vstack them\n",
        "    for index, value in np.ndenumerate(data_green):\n",
        "        stacked_green = np.vstack((stacked_green, value))\n",
        "    print(f\"stacked_green.shape is {stacked_green.shape} (compare with 48*3*10={48*3*10})\")\n",
        "\n",
        "    # group columns of green data\n",
        "    group_num = data_axons.shape[0]\n",
        "    group_satcked_green = np.zeros((stacked_green.shape[0], group_num))\n",
        "    for i, cols in enumerate(data_axons):\n",
        "        group_satcked_green[:, i] = np.sum(stacked_green[:, cols-1], axis=1)\n",
        "    print(\"group_satcked_green.shape, data_axons.shape are:\", group_satcked_green.shape, data_axons.shape)\n",
        "\n",
        "    # vstack red data\n",
        "    stacked_red = np.empty((0, data_red[0,0].shape[1]))\n",
        "    # Enumerate the elements in the np array and vstack them\n",
        "    for index, value in np.ndenumerate(data_red):\n",
        "        stacked_red = np.vstack((stacked_red, value))\n",
        "    print(f\"stacked_red.shape is {stacked_red.shape} (compare with 48*3*10={48*3*10})\")\n",
        "\n",
        "    print(\"np.max(group_satcked_green), np.min(group_satcked_green) are:\", np.max(group_satcked_green), np.min(group_satcked_green))\n",
        "    print(\"np.max(stacked_red), np.min(stacked_red) are:\", np.max(stacked_red), np.min(stacked_red))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqGVC3zrxDni"
      },
      "source": [
        "## Analysis for pupil and fluorescence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH35solIOXA6"
      },
      "source": [
        "### Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VMmjXc-xLJ4",
        "outputId": "f055cbee-6555-44d7-ecc7-fb1f1ffad135"
      },
      "outputs": [],
      "source": [
        "cell_data = copy.deepcopy(cell_data_dict[cell_name])\n",
        "# cell_data = copy.deepcopy(cell_data_dict['CL075_230303'])\n",
        "# cell_data = cell_data_dict['CL090_230515']\n",
        "# cell_data = cell_data_dict['CL075_230303']\n",
        "\n",
        "data_green = cell_data['green_dFFMeanValues'][:,:-1] # exclude 49th column\n",
        "data_red = cell_data['red_dFFMeanValues'][:,:-1] # exclude 49th column\n",
        "\n",
        "data_axons = cell_data['axons']\n",
        "delete_small_group = True # delete groups (axons) with less than 3 components\n",
        "if delete_small_group:\n",
        "    data_axons = np.array([axons_ for axons_ in data_axons if len(axons_) >= 3], dtype=object)\n",
        "\n",
        "decay_restoration = True\n",
        "if decay_restoration:\n",
        "    # sum red data to see the decay\n",
        "    sum_mat_red = np.zeros((data_red.shape[0], 1), dtype=np.ndarray)\n",
        "    for i in range(data_red.shape[0]):\n",
        "        sum = np.zeros((data_red[0,0].shape[0], 1))\n",
        "        for j in range(data_red.shape[1]):\n",
        "            sum = sum + data_red[i,j]\n",
        "        # print(np.sum(sum))\n",
        "        sum_mat_red[i, 0] = sum\n",
        "    # print(\"sum_mat_red:\", sum_mat_red)\n",
        "\n",
        "    # Normalization of red\n",
        "    sum_mat_red_n = np.zeros((data_red.shape[0], 1), dtype=np.ndarray)\n",
        "    max_value = -np.inf\n",
        "    for i in range(sum_mat_red.shape[0]):\n",
        "        for j in range(sum_mat_red.shape[1]):\n",
        "            max_value = max(max_value, np.max(np.abs(sum_mat_red[i, j])))\n",
        "    for i in range(sum_mat_red.shape[0]):\n",
        "        for j in range(sum_mat_red.shape[1]):\n",
        "            element = sum_mat_red[i, j]\n",
        "            sum_mat_red_n[i, j] = element / max_value\n",
        "    # print(sum_mat_red_n.shape)\n",
        "\n",
        "    # Define the exponential function for regression of decay\n",
        "    def exponential_func(x, lambda_):\n",
        "        return np.exp(lambda_ * x)\n",
        "    red_recover_factor = sum_mat_red_n.copy()\n",
        "    for i in range(sum_mat_red_n.shape[0]):\n",
        "        for j in range(sum_mat_red_n.shape[1]): # sum_mat_red_n.shape[1] = 1\n",
        "            y = sum_mat_red_n[i, j].copy()\n",
        "            y = np.squeeze(y)\n",
        "            multiplier = 1 / y[0]\n",
        "            y = y * multiplier\n",
        "            x = np.arange(10)\n",
        "            popt, pcov = curve_fit(exponential_func, x, y)\n",
        "            lambda_ = popt[0]\n",
        "            print(f\"round {i}, exponent in the exp func is {lambda_}\")\n",
        "            y_fit = exponential_func(x, lambda_)\n",
        "            y = y.reshape(-1, 1)\n",
        "            y_fit = y_fit.reshape(-1, 1)\n",
        "            red_recover_factor[i, j] = (y / y_fit) * multiplier\n",
        "    print(\"red_recover_factor.shape:\", red_recover_factor.shape)\n",
        "\n",
        "    for i in range(data_red.shape[0]):\n",
        "        for j in range(data_red.shape[1]):\n",
        "            data_red[i,j] = red_recover_factor[i,0] * data_red[i,j]\n",
        "\n",
        "print(\"green:\")\n",
        "print(type(data_green))\n",
        "print(data_green.shape)\n",
        "print(data_green[1,1].shape)\n",
        "\n",
        "print(\"red:\")\n",
        "print(type(data_red))\n",
        "print(data_red.shape)\n",
        "print(data_red[1,1].shape)\n",
        "\n",
        "print(\"fine_mean_mat_pupil:\")\n",
        "print(fine_mean_mat_pupil.shape)\n",
        "print(fine_mean_mat_pupil[0,1].shape)\n",
        "\n",
        "\n",
        "data_green_mean = copy.deepcopy(data_green)\n",
        "data_green_std = copy.deepcopy(data_green)\n",
        "\n",
        "green_mean_vstack_array = np.empty((0, 1))\n",
        "green_std_vstack_array = np.empty((0, 1))\n",
        "red_vstack_array = np.empty((0, 1))\n",
        "mean_pupil_vstack_array = np.empty((0, 1))\n",
        "\n",
        "num_column = data_green.shape[1]\n",
        "green_mean_vstack_array_condition = np.empty((0, num_column))\n",
        "green_std_vstack_array_condition = np.empty((0, num_column))\n",
        "red_vstack_array_condition = np.empty((0, num_column))\n",
        "mean_pupil_vstack_array_condition = np.empty((0, num_column))\n",
        "\n",
        "num_group = len(data_axons)\n",
        "print(\"num_group:\", num_group)\n",
        "green_mean_vstack_array_condition_groups = [np.empty((0, num_column)) for _ in range(num_group)]\n",
        "green_std_vstack_array_condition_groups = [np.empty((0, num_column)) for _ in range(num_group)]\n",
        "\n",
        "for i in range(data_green.shape[0]):\n",
        "    data_green_mean_condition = np.empty((data_green[0,0].shape[0], 0))\n",
        "    data_green_std_condition = np.empty((data_green[0,0].shape[0], 0))\n",
        "    data_red_condition = np.empty((data_red[0,0].shape[0], 0))\n",
        "    fine_mean_mat_pupil_condition = np.empty((fine_mean_mat_pupil[0,0].shape[0], 0))\n",
        "\n",
        "    data_green_mean_condition_groups = [np.empty((data_green[0,0].shape[0], 0)) for _ in range(num_group)]\n",
        "    data_green_std_condition_groups = [np.empty((data_green[0,0].shape[0], 0)) for _ in range(num_group)]\n",
        "    for j in range(data_green.shape[1]):\n",
        "        data_green_mean[i,j] = np.mean(data_green[i,j], axis=1, keepdims=True)\n",
        "        data_green_std[i,j] = np.std(data_green[i,j], axis=1, keepdims=True)\n",
        "        green_mean_vstack_array = np.vstack((green_mean_vstack_array, data_green_mean[i,j]))\n",
        "        green_std_vstack_array = np.vstack((green_std_vstack_array, data_green_std[i,j]))\n",
        "        red_vstack_array = np.vstack((red_vstack_array, data_red[i,j]))\n",
        "        mean_pupil_vstack_array = np.vstack((mean_pupil_vstack_array, fine_mean_mat_pupil[i,j][:,[1]]))\n",
        "\n",
        "        data_green_mean_condition = np.hstack((data_green_mean_condition, data_green_mean[i,j]))\n",
        "        data_green_std_condition = np.hstack((data_green_std_condition, data_green_std[i,j]))\n",
        "        data_red_condition = np.hstack((data_red_condition, data_red[i,j]))\n",
        "        fine_mean_mat_pupil_condition = np.hstack((fine_mean_mat_pupil_condition, fine_mean_mat_pupil[i,j][:,[1]]))\n",
        "\n",
        "        for ii, cols in enumerate(data_axons):\n",
        "            data_green_group_mean = np.mean(data_green[i,j][:, cols-1], axis=1, keepdims=True)\n",
        "            data_green_group_std = np.std(data_green[i,j][:, cols-1], axis=1, keepdims=True)\n",
        "            data_green_mean_condition_groups[ii] = np.hstack((data_green_mean_condition_groups[ii], data_green_group_mean))\n",
        "            data_green_std_condition_groups[ii] = np.hstack((data_green_std_condition_groups[ii], data_green_group_std))\n",
        "\n",
        "    green_mean_vstack_array_condition = np.vstack((green_mean_vstack_array_condition, data_green_mean_condition))\n",
        "    green_std_vstack_array_condition = np.vstack((green_std_vstack_array_condition, data_green_std_condition))\n",
        "    red_vstack_array_condition = np.vstack((red_vstack_array_condition, data_red_condition))\n",
        "    mean_pupil_vstack_array_condition = np.vstack((mean_pupil_vstack_array_condition, fine_mean_mat_pupil_condition))\n",
        "\n",
        "    for ii in range(num_group):\n",
        "        green_mean_vstack_array_condition_groups[ii] = np.vstack((green_mean_vstack_array_condition_groups[ii],\n",
        "                                                                  data_green_mean_condition_groups[ii]))\n",
        "        green_std_vstack_array_condition_groups[ii] = np.vstack((green_std_vstack_array_condition_groups[ii],\n",
        "                                                                 data_green_std_condition_groups[ii]))\n",
        "\n",
        "## Overall Charactreistics of All Components Under All Conditions\n",
        "## statistics without distinguishing components and conditions\n",
        "\n",
        "print(\"green_mean_vstack_array:\")\n",
        "print(type(green_mean_vstack_array))\n",
        "print(green_mean_vstack_array.shape)\n",
        "\n",
        "print(\"green_std_vstack_array:\")\n",
        "print(type(green_std_vstack_array))\n",
        "print(green_std_vstack_array.shape)\n",
        "\n",
        "print(\"red_vstack_array:\")\n",
        "print(type(red_vstack_array))\n",
        "print(red_vstack_array.shape)\n",
        "\n",
        "print(\"mean_pupil_vstack_array:\")\n",
        "print(type(mean_pupil_vstack_array))\n",
        "print(mean_pupil_vstack_array.shape)\n",
        "\n",
        "pupil_fluorescence = np.hstack((mean_pupil_vstack_array, green_mean_vstack_array, green_std_vstack_array, red_vstack_array))\n",
        "\n",
        "print(\"pupil_fluorescence:\")\n",
        "print(type(pupil_fluorescence))\n",
        "print(pupil_fluorescence.shape)\n",
        "\n",
        "## Charactreistics of All Components Across Different Conditions\n",
        "## statistics distinguishing conditions but not components\n",
        "\n",
        "print(\"green_mean_vstack_array_condition:\")\n",
        "print(type(green_mean_vstack_array_condition))\n",
        "print(green_mean_vstack_array_condition.shape)\n",
        "\n",
        "print(\"green_std_vstack_array_condition:\")\n",
        "print(type(green_std_vstack_array_condition))\n",
        "print(green_std_vstack_array_condition.shape)\n",
        "\n",
        "print(\"red_vstack_array_condition:\")\n",
        "print(type(red_vstack_array_condition))\n",
        "print(red_vstack_array_condition.shape)\n",
        "\n",
        "print(\"mean_pupil_vstack_array_condition:\")\n",
        "print(type(mean_pupil_vstack_array_condition))\n",
        "print(mean_pupil_vstack_array_condition.shape)\n",
        "\n",
        "pupil_fluorescence_condition = np.stack((mean_pupil_vstack_array_condition, green_mean_vstack_array_condition, green_std_vstack_array_condition,\n",
        "                red_vstack_array_condition), axis=0)\n",
        "\n",
        "print(\"pupil_fluorescence_condition:\")\n",
        "print(type(pupil_fluorescence_condition))\n",
        "print(pupil_fluorescence_condition.shape)\n",
        "\n",
        "## Charactreistics of Different Components Across Different Conditions\n",
        "## statistics distinguishing conditions and components\n",
        "## components are grouped with small groups (<3 components) neglected\n",
        "\n",
        "data_list_to_be_stacked = [mean_pupil_vstack_array_condition, green_mean_vstack_array_condition, green_std_vstack_array_condition]\n",
        "for i, (arr1, arr2) in enumerate(zip(green_mean_vstack_array_condition_groups, green_std_vstack_array_condition_groups)):\n",
        "    data_list_to_be_stacked.append(arr1)\n",
        "    data_list_to_be_stacked.append(arr2)\n",
        "data_list_to_be_stacked.append(red_vstack_array_condition)\n",
        "\n",
        "pupil_fluorescence_condition_groups = np.stack(data_list_to_be_stacked, axis=0)\n",
        "\n",
        "print(\"pupil_fluorescence_condition_groups:\")\n",
        "print(type(pupil_fluorescence_condition_groups))\n",
        "print(pupil_fluorescence_condition_groups.shape)\n",
        "\n",
        "# print(green_mean_vstack_array_condition_groups[0].shape)\n",
        "# print(data_axons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vYX1GmNObsr"
      },
      "source": [
        "### Plot graphs (condition not separately)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "cM38E4-0JZvU",
        "outputId": "a070e819-3312-436d-b2b1-682138313f05"
      },
      "outputs": [],
      "source": [
        "# Get the indices that would sort the first column\n",
        "sorted_indices = np.argsort(pupil_fluorescence[:, 0])\n",
        "\n",
        "# Use the sorted indices to sort the entire array\n",
        "sorted_pupil_fluorescence = pupil_fluorescence[sorted_indices]\n",
        "\n",
        "# Separate the columns\n",
        "p_size = sorted_pupil_fluorescence[:, 0]\n",
        "green_m = sorted_pupil_fluorescence[:, 1]\n",
        "green_s = sorted_pupil_fluorescence[:, 2]\n",
        "red = sorted_pupil_fluorescence[:, 3]\n",
        "\n",
        "# Set the figure size (width=10, height=6) and create subplots with 3 rows\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 7.5), sharex=True)\n",
        "\n",
        "if decay_restoration:\n",
        "    red_label = 'Red (bleach correction)'\n",
        "else:\n",
        "    red_label = 'Red'\n",
        "\n",
        "# Plot the data on each subplot\n",
        "axes[0].plot(p_size, green_m, label='Green Mean', color='green')\n",
        "axes[1].plot(p_size, green_s, label='Green Std', color='lime')\n",
        "axes[2].plot(p_size, red, label=red_label, color='red')\n",
        "\n",
        "# Add labels, legends, and title to each subplot\n",
        "axes[2].set_xlabel('Pupil Size')\n",
        "for ax in axes:\n",
        "    ax.set_ylabel('Fluorescence')\n",
        "    ax.legend()\n",
        "    ax.tick_params(axis='both', labelsize=10)  # Adjust tick size as desired\n",
        "    # ax.set_title('Plot of Green and Red against Pupil Size', fontsize=14)  # Adjust title size as desired\n",
        "\n",
        "# Add the title for the entire figure\n",
        "fig.suptitle('Plot of Green and Red against Pupil Size', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "3AZZq6SxBP-2",
        "outputId": "1627186b-b92a-4ed2-eab4-8563b323f2b4"
      },
      "outputs": [],
      "source": [
        "# Set the figure size (width=10, height=6) and create subplots with 3 rows\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 7.5), sharex=True)\n",
        "\n",
        "if decay_restoration:\n",
        "    red_label = 'Red (bleach correction)'\n",
        "else:\n",
        "    red_label = 'Red'\n",
        "\n",
        "# Size of the points for scatter plots\n",
        "point_size = 10\n",
        "# Scatter plots on each subplot\n",
        "axes[0].scatter(p_size, green_m, label='Green Mean', color='green', s=point_size)\n",
        "axes[1].scatter(p_size, green_s, label='Green Std', color='lime', s=point_size)\n",
        "axes[2].scatter(p_size, red, label=red_label, color='red', s=point_size)\n",
        "\n",
        "# Add linear fitted lines to each subplot\n",
        "for ax, y_data, label in zip(axes, [green_m, green_s, red], ['Green Mean', 'Green Std', 'Red']):\n",
        "\n",
        "    # Calculate R-squared and p-value\n",
        "    from scipy.stats import linregress\n",
        "    slope, intercept, r_value, p_value, _ = linregress(p_size, y_data)\n",
        "    r_squared = r_value ** 2\n",
        "\n",
        "    coeffs = np.polyfit(p_size, y_data, 1)\n",
        "    fitted_line = np.polyval(coeffs, p_size)\n",
        "    ax.plot(p_size, fitted_line, color='blue', linestyle='dashed',\n",
        "            label=f'Fitted Line ({label}): y = {coeffs[0]:.8f}x + {coeffs[1]:.4f}\\n(RÂ² = {r_squared:.3f}, p = {p_value:.3f})')\n",
        "\n",
        "\n",
        "# Add labels, legends, and title to each subplot\n",
        "axes[2].set_xlabel('Pupil Size')\n",
        "for ax in axes:\n",
        "    ax.set_ylabel('Fluorescence')\n",
        "    ax.legend()\n",
        "    ax.tick_params(axis='both', labelsize=10)  # Adjust tick size as desired\n",
        "    # ax.set_title('Plot of Green and Red against Pupil Size', fontsize=14)  # Adjust title size as desired\n",
        "\n",
        "# Add the title for the entire figure\n",
        "fig.suptitle('Plot of Green and Red against Pupil Size', fontsize=14)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WuChmzGOrvk"
      },
      "source": [
        "### Plot graphs (condition separately)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26Z0yrE8UaA"
      },
      "source": [
        "#### Set sf, tf, and ori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS5LY73BRTab",
        "outputId": "41ac73ae-de2f-44bb-8511-81c678179773"
      },
      "outputs": [],
      "source": [
        "# Create sf array\n",
        "sf = np.tile([0.02, 0.08, 0.32], 16)\n",
        "\n",
        "# Create tf array\n",
        "tf = np.tile([1, 1, 1, 4, 4, 4], 8)\n",
        "\n",
        "# Create orientation array\n",
        "orientation = np.repeat(np.arange(0, 360, 45), 6)\n",
        "\n",
        "print(\"sf:\", sf)\n",
        "print(\"tf:\", tf)\n",
        "print(\"orientation:\", orientation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4pgNGeH8gek"
      },
      "source": [
        "#### Statistics of all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCW3NrdE-MI"
      },
      "source": [
        "(1 )Overall Charactreistics of All Components Under All Conditions\n",
        "\n",
        "statistics without distinguishing components and conditions\n",
        "\n",
        "\n",
        "(2) Charactreistics of All Components Across Different Conditions\n",
        "\n",
        "statistics distinguishing conditions but not components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SJzeWOaBLhSy",
        "outputId": "04198087-1ed5-4706-c689-cb786bb955c4"
      },
      "outputs": [],
      "source": [
        "point_size = 30  # Adjust the point size as desired\n",
        "\n",
        "# Create a big figure with 48x3 subplots\n",
        "fig, axes = plt.subplots(nrows=48, ncols=3, figsize=(15, 150), constrained_layout=True)  # Adjust the figure size as desired\n",
        "\n",
        "data_types = ['Green Mean', 'Green Std', 'Red']\n",
        "if decay_restoration:\n",
        "    data_types = ['Green Mean', 'Green Std', 'Red (bleach correction)']\n",
        "colors = ['green', 'lime', 'red']\n",
        "\n",
        "# Create 48 rows of figures\n",
        "for x in range(48):\n",
        "    for i, y_data_idx in enumerate([1, 2, 3]):\n",
        "        y_data = pupil_fluorescence_condition[y_data_idx, :, x]\n",
        "        x_data = pupil_fluorescence_condition[0, :, x]\n",
        "        if i == 0:\n",
        "            x_data_min = np.min(x_data)\n",
        "            x_data_max = np.max(x_data)\n",
        "            y_data_min = np.min(y_data)\n",
        "            y_data_max = np.max(y_data)\n",
        "            x_text = x_data_min - 0.6 * (x_data_max - x_data_min)\n",
        "            y_text = 0.5 * (y_data_max + y_data_min)\n",
        "\n",
        "        # Scatter plots on each subplot\n",
        "        axes[x, i].scatter(x_data, y_data, label='Scatter Plot', color=colors[i], s=point_size)\n",
        "\n",
        "        # Calculate R-squared and p-value\n",
        "        from scipy.stats import linregress\n",
        "        slope, intercept, r_value, p_value, _ = linregress(x_data, y_data)\n",
        "        r_squared = r_value ** 2\n",
        "\n",
        "        # Add linear fitted line to each subplot\n",
        "        coeffs = np.polyfit(x_data, y_data, 1)\n",
        "        fitted_line = np.polyval(coeffs, x_data)\n",
        "        axes[x, i].plot(x_data, fitted_line, color='#1f77b4', linestyle='solid',\n",
        "                         label=f'Fitted Line: y = {coeffs[0]:.8f}x + {coeffs[1]:.4f}\\n(RÂ² = {r_squared:.2f}, p = {p_value:.2f})')\n",
        "\n",
        "        # Customize the plot\n",
        "        axes[x, i].set_xlabel('Pupil Size', fontsize=12)  # Adjust the label size as desired\n",
        "        axes[x, i].set_ylabel('Fluorescence', fontsize=12)  # Adjust the label size as desired\n",
        "        axes[x, i].set_title(data_types[i], fontsize=14)  # Adjust the title size as desired\n",
        "        axes[x, i].legend(fontsize='medium')\n",
        "\n",
        "    # Write the group information to the right of each row\n",
        "    axes[x, 0].text(x_text, y_text, f'Ori: {orientation[x]}Â° \\nTF: {tf[x]} \\nSF: {sf[x]}', fontsize=14, va='center')  # Adjust the position and font size as desired\n",
        "\n",
        "fig.suptitle(f'Scatter Plots and Fitted lines of Green Mean, Green Std, and Red against Pupil Size ({cell_name})', fontsize=16)\n",
        "# plt.tight_layout()  # Adjust the spacing between subplots\n",
        "# using plt.tight_layout() will let fig.suptitle overlap the plots, so use constrained_layout=True in plt.subplots instead.\n",
        "# filename = f'Scatter_Plots_Green_Mean_Std_Red_against_Pupil_{cell_name}.pdf'\n",
        "# plt.savefig(filename)\n",
        "# filename = f'Scatter_Plots_Green_Mean_Std_Red_against_Pupil_{cell_name}.png'\n",
        "# plt.savefig(filename)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taI_hQ9h8w2u"
      },
      "source": [
        "#### Statistics of groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSpblr0YFTEg"
      },
      "source": [
        "(3) Overall Charactreistics of Different Components Across Different Conditions\n",
        "\n",
        "statistics distinguishing conditions and components\n",
        "\n",
        "components are grouped with small groups (<3 components) neglected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_aBfv_c806X"
      },
      "outputs": [],
      "source": [
        "point_size = 30  # Adjust the point size as desired\n",
        "\n",
        "# Create a big figure with 48-row subplots\n",
        "n_cols = pupil_fluorescence_condition_groups.shape[0] - 1\n",
        "figsize_width = 15 / 3 * n_cols\n",
        "fig, axes = plt.subplots(nrows=48, ncols=n_cols, figsize=(figsize_width, 150), constrained_layout=True)  # Adjust the figure size as desired\n",
        "\n",
        "data_types = ['Green Mean (all)', 'Green Std (all)']\n",
        "for i in range(num_group):\n",
        "    data_types.append(f\"Green Mean (group {i+1})\")\n",
        "    data_types.append(f\"Green Std (group {i+1})\")\n",
        "if decay_restoration:\n",
        "    data_types.append('Red (bleach correction)')\n",
        "else:\n",
        "    data_types.append('Red')\n",
        "\n",
        "colors = ['green' if i % 2 == 0 else 'lime' for i in range(2 * (num_group + 1))]\n",
        "colors.append('red')\n",
        "\n",
        "# print(data_types)\n",
        "# print(colors)\n",
        "\n",
        "# Create 48 rows of figures\n",
        "for x in range(48):\n",
        "    for i, y_data_idx in enumerate([_ for _ in range(1, n_cols + 1)]):\n",
        "        y_data = pupil_fluorescence_condition_groups[y_data_idx, :, x]\n",
        "        x_data = pupil_fluorescence_condition_groups[0, :, x]\n",
        "        if i == 0:\n",
        "            x_data_min = np.min(x_data)\n",
        "            x_data_max = np.max(x_data)\n",
        "            y_data_min = np.min(y_data)\n",
        "            y_data_max = np.max(y_data)\n",
        "            x_text = x_data_min - 0.6 * (x_data_max - x_data_min)\n",
        "            y_text = 0.5 * (y_data_max + y_data_min)\n",
        "\n",
        "        # Scatter plots on each subplot\n",
        "        axes[x, i].scatter(x_data, y_data, label='Scatter Plot', color=colors[i], s=point_size)\n",
        "\n",
        "        # Calculate R-squared and p-value\n",
        "        from scipy.stats import linregress\n",
        "        slope, intercept, r_value, p_value, _ = linregress(x_data, y_data)\n",
        "        r_squared = r_value ** 2\n",
        "\n",
        "        # Add linear fitted line to each subplot\n",
        "        coeffs = np.polyfit(x_data, y_data, 1)\n",
        "        fitted_line = np.polyval(coeffs, x_data)\n",
        "        axes[x, i].plot(x_data, fitted_line, color='#1f77b4', linestyle='solid',\n",
        "                         label=f'Fitted Line: y = {coeffs[0]:.8f}x + {coeffs[1]:.4f}\\n(RÂ² = {r_squared:.2f}, p = {p_value:.2f})')\n",
        "\n",
        "        # Customize the plot\n",
        "        axes[x, i].set_xlabel('Pupil Size', fontsize=12)  # Adjust the label size as desired\n",
        "        axes[x, i].set_ylabel('Fluorescence', fontsize=12)  # Adjust the label size as desired\n",
        "        axes[x, i].set_title(data_types[i], fontsize=14)  # Adjust the title size as desired\n",
        "        axes[x, i].legend(fontsize='medium')\n",
        "\n",
        "    # Write the group information to the right of each row\n",
        "    axes[x, 0].text(x_text, y_text, f'Ori: {orientation[x]}Â° \\nTF: {tf[x]} \\nSF: {sf[x]}', fontsize=14, va='center')  # Adjust the position and font size as desired\n",
        "\n",
        "fig.suptitle(f'Scatter Plots and Fitted lines of Green Mean, Green Std, and Red against Pupil Size ({cell_name})', fontsize=16)\n",
        "# plt.tight_layout()  # Adjust the spacing between subplots\n",
        "# using plt.tight_layout() will let fig.suptitle overlap the plots, so use constrained_layout=True in plt.subplots instead.\n",
        "filename = f'Scatter_Plots_Green_Mean_Std_Red_against_Pupil_in_groups_{cell_name}.pdf'\n",
        "plt.savefig(filename)\n",
        "# filename = f'Scatter_Plots_Green_Mean_Std_Red_against_Pupil_in_groups_{cell_name}.png'\n",
        "# plt.savefig(filename)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4DlSabhe8oy"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuteAjtu2ybj"
      },
      "source": [
        "### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yio94tIGNM63"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, r2_score\n",
        "from scipy.optimize import curve_fit\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ96loavNNme"
      },
      "outputs": [],
      "source": [
        "def plot_comparison(y_test, y_pred, subtitle = ''):\n",
        "    # Sort y_pred and y_test based on y_test\n",
        "    sorted_indices = np.argsort(y_test)\n",
        "    sorted_y_pred = y_pred[sorted_indices]\n",
        "    sorted_y_test = y_test[sorted_indices]\n",
        "    # Plot sorted_y_pred and sorted_y_test\n",
        "    plt.plot(sorted_y_pred, label='Sorted Predictions')\n",
        "    plt.plot(sorted_y_test, label='Sorted Ground Truth')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title(f'Comparison of Sorted Predictions and Sorted Ground Truth \\n ({subtitle})')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'Comparison ({subtitle}).png', bbox_inches='tight')\n",
        "    # bbox_inches='tight' will adjust the figure's bounding box to fit all the content, ensuring that\n",
        "    # the complete words are visible in the saved figure. Otherwise, the saved figure may not show the\n",
        "    # complete words, e.g., for x-label, or for the long title.\n",
        "    # The default setting can sometimes result in cutoff or clipped text. It tries to include the entire\n",
        "    # figure within the saved image, but there may be cases where the default behavior is not sufficient\n",
        "    # to capture all the content. The default behavior assumes the figure content fits within the predefined\n",
        "    # margins and padding.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UoS046BmEkL"
      },
      "source": [
        "#### Divide train and val datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5jidRV6xo48",
        "outputId": "8a563e5e-10cf-4f76-c7a3-683502dfb046"
      },
      "outputs": [],
      "source": [
        "# independent data\n",
        "x = group_satcked_green\n",
        "\n",
        "class_num = 480\n",
        "\n",
        "# dependent data (labels/targets)\n",
        "y = np.squeeze(stacked_red)\n",
        "# print(np.max(y), np.min(y))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
        "# Calculate the minimum and maximum values\n",
        "infinitesimal = np.finfo(float).eps\n",
        "min_val = np.min(y) - infinitesimal # to guarantee to include min\n",
        "max_val = np.max(y) + infinitesimal # to guarantee to include max\n",
        "# Generate class_num+1 evenly spaced intervals\n",
        "intervals = np.linspace(min_val, max_val, num=class_num+1) # num = class num + 1\n",
        "# print(intervals)\n",
        "# Digitize the array to get the indices of the intervals\n",
        "y_train = np.digitize(y_train, intervals) - 1\n",
        "y_test = np.digitize(y_test, intervals) - 1\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# to see unique elements (see if we have all 0, 1,..., class_num-1 classes, better close to all)\n",
        "unique_elements = np.unique(y_train)\n",
        "print(\"Unique elements:\", unique_elements)\n",
        "print(\"Number of unique elements:\", len(unique_elements))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBH_I4MDmaRi"
      },
      "source": [
        "#### Fit and predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlS4Tx4mCfre"
      },
      "source": [
        "'multinomial' (default option for multi-calss) achieves better performance than 'ovr'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_0j-BRqmadJ",
        "outputId": "ce1767ca-0d02-4a35-9b08-ba9e53a1923f"
      },
      "outputs": [],
      "source": [
        "# fit\n",
        "model = linear_model.LogisticRegression(fit_intercept=True, max_iter=1000, multi_class='multinomial')\n",
        "# model = linear_model.LogisticRegression(fit_intercept=True, max_iter=1000, multi_class='ovr')\n",
        "\n",
        "fit_result = model.fit(x_train, y_train)\n",
        "print(fit_result.intercept_.shape, fit_result.coef_.shape)\n",
        "# print(\"Coefficients:\", model.coef_[0,:])\n",
        "# print(\"Intercept:\", model.intercept_[0])\n",
        "print('--- --- ---')\n",
        "\n",
        "# predict\n",
        "# Use the trained model to make predictions\n",
        "y_pred = model.predict(x_test)\n",
        "# Alternatively, you can get the predicted probabilities for each class\n",
        "y_prob = model.predict_proba(x_test)\n",
        "\n",
        "print('y_prob.shape:', y_prob.shape)\n",
        "print(np.sum(y_prob, axis = 1))\n",
        "# print(y_prob[0,:])\n",
        "\n",
        "# Print the predicted class labels\n",
        "print('y_pred:', y_pred)\n",
        "print('y_test:', y_test)\n",
        "print('y_pred shape:', y_pred.shape, 'y_test shape:', y_test.shape)\n",
        "# Print the predicted probabilities\n",
        "# print(y_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NkqiuKXmTHh"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdYWV-LfNno1"
      },
      "source": [
        "##### Evaluate (normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikh3nJOfNx28"
      },
      "source": [
        "In previous data division, I classify data into class_num (e.g., class_num = 160) intervals (histogram, by np.digitize). Here, evaluate the results with the same number of classes (e.g., class_num = 160)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "FZHacjIU_H_Q",
        "outputId": "1ab68464-f34c-459e-d822-f002f4fe6bcf"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# cm = confusion_matrix(y_test, y_pred)\n",
        "# print(\"Confusion Matrix:\") # y_test doesn't include all classes, so confusion matrix is less than num_class by num_class\n",
        "# print(cm)\n",
        "# the columns represent the predicted labels (predictions)\n",
        "# the rows represent the true labels (ground truth)\n",
        "#                Predicted Class\n",
        "#           |   Class 1   |   Class 2   |   Class 3   |\n",
        "# -----------------------------------------------------\n",
        "# True Class   |     TP1     |     FN1     |     FN1     |\n",
        "# -----------------------------------------------------\n",
        "# True Class   |     FP2     |     TP2     |     FN2     |\n",
        "# -----------------------------------------------------\n",
        "# True Class   |     FN3     |     FP3     |     TP3     |\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "correlation = np.corrcoef(y_pred, y_test)[0, 1]\n",
        "print(\"Correlation coefficient:\", correlation)\n",
        "\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"Coefficient of determination (R-squared score, R2 score):\", r_squared)\n",
        "\n",
        "\n",
        "# Sort y_pred and y_test based on y_test\n",
        "plot_comparison(y_test, y_pred, 'Logistic Linear Regression, Test Set')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "60Dz_1v7nShb",
        "outputId": "644ba20b-fc53-4cde-ae80-5d3cd55eda79"
      },
      "outputs": [],
      "source": [
        "# predict on train\n",
        "# Use the trained model to make predictions\n",
        "y_pred_ = model.predict(x_train)\n",
        "# Alternatively, you can get the predicted probabilities for each class\n",
        "y_prob_ = model.predict_proba(x_train)\n",
        "\n",
        "accuracy = accuracy_score(y_train, y_pred_)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# cm = confusion_matrix(y_train, y_pred_)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "# the columns represent the predicted labels (predictions)\n",
        "# the rows represent the true labels (ground truth)\n",
        "#                Predicted Class\n",
        "#              |   Class 1   |   Class 2   |   Class 3   |\n",
        "# -----------------------------------------------------\n",
        "# True Class   |     TP1     |     FN1     |     FN1     |\n",
        "# -----------------------------------------------------\n",
        "# True Class   |     FP2     |     TP2     |     FN2     |\n",
        "# -----------------------------------------------------\n",
        "# True Class   |     FN3     |     FP3     |     TP3     |\n",
        "\n",
        "mse = mean_squared_error(y_train, y_pred_)\n",
        "print(\"Mean squared error:\", mse)\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "correlation = np.corrcoef(y_pred_, y_train)[0, 1]\n",
        "print(\"Correlation coefficient:\", correlation)\n",
        "\n",
        "r_squared = r2_score(y_train, y_pred_)\n",
        "print(\"Coefficient of determination (R-squared score, R2 score):\", r_squared)\n",
        "\n",
        "plot_comparison(y_train, y_pred_, 'Logistic Linear Regression, Train Set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnyjUpDwOb-0"
      },
      "source": [
        "##### Evaluate (reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UPsnEQmOcQ8"
      },
      "source": [
        "The model is based on classifying data into class_num (e.g., class_num = 160) intervals (histogram, by np.digitize). Here, evaluate the results a smaller number of classes (e.g., reduced_class_num = 16), that is, for the example of class_num = 160 and reduced_class_num = 16, classes 0, 1, ..., 15 become one class, i.e., 0; ...; classes 144, 145, ..., 159 become one class, i.e., 15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhQDUvS5PCWN",
        "outputId": "867b90bc-0f15-4438-fbe5-0f82cb04c30c"
      },
      "outputs": [],
      "source": [
        "print(\"---- ---- ----\")\n",
        "# Define the boundaries for digitization\n",
        "reduced_class_num = 16\n",
        "intervals = np.arange(0, class_num + 1, class_num / reduced_class_num)\n",
        "print(intervals)\n",
        "\n",
        "y_train_digital = np.digitize(y_train, intervals) - 1\n",
        "y_test_digital = np.digitize(y_test, intervals) - 1\n",
        "y_pred_train_digital = np.digitize(y_pred_, intervals) - 1\n",
        "y_pred_test_digital = np.digitize(y_pred, intervals) - 1\n",
        "print(y_pred)\n",
        "print(y_pred_test_digital)\n",
        "mse = mean_squared_error(y_test_digital, y_pred_test_digital)\n",
        "print(\"Mean squared error:\", mse)\n",
        "correlation = np.corrcoef(y_pred_test_digital, y_test_digital)[0, 1]\n",
        "print(\"Correlation coefficient:\", correlation)\n",
        "r_squared = r2_score(y_test_digital, y_pred_test_digital)\n",
        "print(\"Coefficient of determination (R-squared score, R2 score):\", r_squared)\n",
        "mse = mean_squared_error(y_train_digital, y_pred_train_digital)\n",
        "print(\"Mean squared error:\", mse)\n",
        "correlation = np.corrcoef(y_pred_train_digital, y_train_digital)[0, 1]\n",
        "print(\"Correlation coefficient:\", correlation)\n",
        "r_squared = r2_score(y_train_digital, y_pred_train_digital)\n",
        "print(\"Coefficient of determination (R-squared score, R2 score):\", r_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhqcfpFAdJIP"
      },
      "source": [
        "Fixing the reduced class number, I enumerate the original class number to see what a original class number is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LZunnrq2dKJ-",
        "outputId": "41e2baec-1870-451f-93c0-50ee9e1835e4"
      },
      "outputs": [],
      "source": [
        "# independent data\n",
        "x = group_satcked_green\n",
        "# dependent data (labels/targets)\n",
        "y = np.squeeze(stacked_red)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
        "# Calculate the minimum and maximum values\n",
        "infinitesimal = np.finfo(float).eps\n",
        "min_val = np.min(y) - infinitesimal # to guarantee to include min\n",
        "max_val = np.max(y) + infinitesimal # to guarantee to include max\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "reduced_class_num = 16\n",
        "class_num_array = np.arange(reduced_class_num, reduced_class_num * 100 + 1, reduced_class_num)\n",
        "# class_num_array = np.arange(reduced_class_num * 5, reduced_class_num * 40 + 1, reduced_class_num)\n",
        "mse_test_list = []\n",
        "correlation_test_list = []\n",
        "r_squared_test_list = []\n",
        "mse_train_list = []\n",
        "correlation_train_list = []\n",
        "r_squared_train_list = []\n",
        "\n",
        "for class_num in class_num_array:\n",
        "    print('---- ---- ----')\n",
        "    print(f'class_num = {class_num}')\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
        "\n",
        "    # Generate class_num+1 evenly spaced intervals\n",
        "    intervals = np.linspace(min_val, max_val, num=class_num+1) # num = class num + 1\n",
        "    # print(intervals)\n",
        "    # Digitize the array to get the indices of the intervals\n",
        "    y_train = np.digitize(y_train, intervals) - 1\n",
        "    y_test = np.digitize(y_test, intervals) - 1\n",
        "\n",
        "    # to see unique elements (see if we have all 0, 1,..., class_num-1 classes, better close to all)\n",
        "    unique_elements = np.unique(y_train)\n",
        "    # print(\"Unique elements:\", unique_elements)\n",
        "    print(\"Number of unique elements:\", len(unique_elements))\n",
        "\n",
        "    model = linear_model.LogisticRegression(fit_intercept=True, max_iter=1000, multi_class='multinomial')\n",
        "    fit_result = model.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_ = model.predict(x_train)\n",
        "\n",
        "    # Define the boundaries for digitization\n",
        "    intervals = np.arange(0, class_num+1, class_num/16)\n",
        "    print(intervals)\n",
        "\n",
        "    y_train_digital = np.digitize(y_train, intervals) - 1\n",
        "    y_test_digital = np.digitize(y_test, intervals) - 1\n",
        "    y_pred_train_digital = np.digitize(y_pred_, intervals) - 1\n",
        "    y_pred_test_digital = np.digitize(y_pred, intervals) - 1\n",
        "    # print(y_pred)\n",
        "    # print(y_pred_test_digital)\n",
        "    print(\"test eval:\")\n",
        "    mse = mean_squared_error(y_test_digital, y_pred_test_digital)\n",
        "    print(\"Mean squared error:\", mse)\n",
        "    correlation = np.corrcoef(y_pred_test_digital, y_test_digital)[0, 1]\n",
        "    print(\"Correlation coefficient:\", correlation)\n",
        "    r_squared = r2_score(y_test_digital, y_pred_test_digital)\n",
        "    print(\"Coefficient of determination (R-squared score, R2 score):\", r_squared)\n",
        "    mse_test_list.append(mse)\n",
        "    correlation_test_list.append(correlation)\n",
        "    r_squared_test_list.append(r_squared)\n",
        "\n",
        "    print(\"train eval:\")\n",
        "    mse = mean_squared_error(y_train_digital, y_pred_train_digital)\n",
        "    print(\"Mean squared error:\", mse)\n",
        "    correlation = np.corrcoef(y_pred_train_digital, y_train_digital)[0, 1]\n",
        "    print(\"Correlation coefficient:\", correlation)\n",
        "    r_squared = r2_score(y_train_digital, y_pred_train_digital)\n",
        "    print(\"Coefficient of determination (R-squared score, R2 score):\", r_squared)\n",
        "    mse_train_list.append(mse)\n",
        "    correlation_train_list.append(correlation)\n",
        "    r_squared_train_list.append(r_squared)\n",
        "\n",
        "    plot_comparison(y_test, y_pred, f'Logistic Linear Regression Reduced Evaluation {class_num} to {reduced_class_num}, Test Set')\n",
        "    plot_comparison(y_train, y_pred_, f'Logistic Linear Regression Reduced Evaluation {class_num} to {reduced_class_num}, Train Set')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vWYrKtzhkteY",
        "outputId": "9f06e6d9-4980-4d60-9319-18c192a382be"
      },
      "outputs": [],
      "source": [
        "# plot the trend figures of mse, correlation, and r2\n",
        "\n",
        "# Create a figure and subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 12))\n",
        "\n",
        "# Plot MSE\n",
        "ax1.plot(class_num_array, mse_test_list, label='MSE (Test)')\n",
        "ax1.plot(class_num_array, mse_train_list, label='MSE (Train)')\n",
        "ax1.set_ylabel('MSE')\n",
        "ax1.set_xlabel('Original Class Number')\n",
        "ax1.set_title(f'MSE Curve (reduced class num is {reduced_class_num})')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot Correlation\n",
        "ax2.plot(class_num_array, correlation_test_list, label='Correlation (Test)')\n",
        "ax2.plot(class_num_array, correlation_train_list, label='Correlation (Train)')\n",
        "ax2.set_ylabel('Correlation')\n",
        "ax2.set_xlabel('Original Class Number')\n",
        "ax2.set_title(f'Correlation Curve (reduced class num is {reduced_class_num})')\n",
        "ax2.legend()\n",
        "\n",
        "# Plot R-squared\n",
        "ax3.plot(class_num_array, r_squared_test_list, label='R-squared (Test)')\n",
        "ax3.plot(class_num_array, r_squared_train_list, label='R-squared (Train)')\n",
        "ax3.set_ylabel('R-squared')\n",
        "ax3.set_xlabel('Original Class Number')\n",
        "ax3.set_title(f'R-squared Curve (reduced class num is {reduced_class_num})')\n",
        "ax3.legend()\n",
        "\n",
        "# Increase the vertical spacing between subplots\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Adjust tick, label, title, and legend font sizes\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "ax1.tick_params(labelsize=10)\n",
        "ax2.tick_params(labelsize=10)\n",
        "ax3.tick_params(labelsize=10)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(f'mse_correlation_r2_trend_curve_reduced_eval_reduced_class_num_{reduced_class_num}.png', bbox_inches='tight')\n",
        "\n",
        "# Show the figure\n",
        "plt.show()\n",
        "\n",
        "# Convert r_squared_test_list to a NumPy array\n",
        "r_squared_test_array = np.array(r_squared_test_list)\n",
        "# Find the index of the maximum value\n",
        "max_index = np.argmax(r_squared_test_array)\n",
        "# Get the corresponding class_num value\n",
        "max_class_num = class_num_array[max_index]\n",
        "\n",
        "# Print the index and corresponding class_num\n",
        "print(\"Max Index:\", max_index)\n",
        "print(\"Max Original Class Num:\", max_class_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7LmKeYpxR8S",
        "outputId": "5971fa31-fe98-40bc-ff53-1247f981b004"
      },
      "outputs": [],
      "source": [
        "# calculate the mean value for the metrics in all situations\n",
        "mse_test_array = np.array(mse_test_list)\n",
        "r_squared_test_array = np.array(r_squared_test_list)\n",
        "mse_train_array = np.array(mse_train_list)\n",
        "r_squared_train_array = np.array(r_squared_train_list)\n",
        "mse_test_mean = np.mean(mse_test_array)\n",
        "r_squared_test_mean = np.mean(r_squared_test_array)\n",
        "mse_train_mean = np.mean(mse_train_array)\n",
        "r_squared_train_mean = np.mean(r_squared_train_array)\n",
        "\n",
        "# Print the index and corresponding class_num\n",
        "print(\"mean of mse test:\", mse_test_mean)\n",
        "print(\"mean of r2 score test:\", r_squared_test_mean)\n",
        "print(\"mean of mse train:\", mse_train_mean)\n",
        "print(\"mean of r2 score train:\", r_squared_train_mean)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
